1
00:00:00,000 --> 00:00:00,120

2
00:00:00,120 --> 00:00:01,670
GARRICK EVANS: My name's
Garrick Evans.

3
00:00:01,670 --> 00:00:04,480
I'm a software architect on
the Google Cloud Platform

4
00:00:04,480 --> 00:00:05,830
solutions team.

5
00:00:05,830 --> 00:00:08,270
And I want to tell you one of
the coolest parts about my job

6
00:00:08,270 --> 00:00:10,300
is the opportunity that comes
up once in a while to get

7
00:00:10,300 --> 00:00:13,910
involved with some projects
that partners are doing

8
00:00:13,910 --> 00:00:16,910
working on some very, very hard
problems and asking some

9
00:00:16,910 --> 00:00:18,820
of the most intriguing and
important questions of the

10
00:00:18,820 --> 00:00:22,190
world, in this particular
case, in the universe.

11
00:00:22,190 --> 00:00:24,030
So today, I'm actually pleased
to share with you

12
00:00:24,030 --> 00:00:24,990
one of these projects.

13
00:00:24,990 --> 00:00:27,580
It's the ATLAS experiment on
Google Compute Engine.

14
00:00:27,580 --> 00:00:30,240
And with this project, what we
wanted to demonstrate was

15
00:00:30,240 --> 00:00:33,470
actually tangible acceleration
of scientific research by

16
00:00:33,470 --> 00:00:35,560
leveraging and taking advantage
of Google's Cloud in

17
00:00:35,560 --> 00:00:38,010
a particular Compute Engine.

18
00:00:38,010 --> 00:00:40,060
The computational and data
demands of the ATLAS

19
00:00:40,060 --> 00:00:41,830
experiment are pretty
substantial.

20
00:00:41,830 --> 00:00:44,120
There's thousands of
collaborators running as a

21
00:00:44,120 --> 00:00:49,590
baseline hundreds of thousands
of jobs per day, peaking at

22
00:00:49,590 --> 00:00:51,210
over 10 times that amount.

23
00:00:51,210 --> 00:00:53,840
The experiment generates tens
of petabytes of data a year,

24
00:00:53,840 --> 00:00:55,950
and there's currently well over
100 petabytes of data

25
00:00:55,950 --> 00:00:58,150
under management coordination.

26
00:00:58,150 --> 00:01:00,580
So with me today are Dr.
Sergey Panitkin of the

27
00:01:00,580 --> 00:01:03,895
Brookhaven National Lab in New
York and Andrew Hanushevsy of

28
00:01:03,895 --> 00:01:06,600
the SLAC National Accelerator
Lab at Stanford.

29
00:01:06,600 --> 00:01:09,710
Sergey leads research and
development of cloud computing

30
00:01:09,710 --> 00:01:12,420
at the ATLAS experiment at
a Large Hadron Collider.

31
00:01:12,420 --> 00:01:14,790
And we'll talk about the project
itself, the compute

32
00:01:14,790 --> 00:01:16,800
clusters this team has
assembled, and share his

33
00:01:16,800 --> 00:01:18,570
results with you.

34
00:01:18,570 --> 00:01:21,870
Andy's an information systems
specialist who's designed and

35
00:01:21,870 --> 00:01:25,980
co-developed a data clustering
technology, XRootD, which was

36
00:01:25,980 --> 00:01:28,820
used to federate data between
ATLAS and the Cloud and will

37
00:01:28,820 --> 00:01:30,860
provide us an overview
of the technology.

38
00:01:30,860 --> 00:01:33,165
So with that, I'd like to
introduce you to Dr. Sergey

39
00:01:33,165 --> 00:01:35,975
Panitkin of Brookhaven
National Lab.

40
00:01:35,975 --> 00:01:43,750
[APPLAUSE]

41
00:01:43,750 --> 00:01:45,093
DR. SERGEY PANITKIN: Today
I will talk-- oh.

42
00:01:45,093 --> 00:01:46,020
[LAUGHTER]

43
00:01:46,020 --> 00:01:48,190
DR. SERGEY PANITKIN: Today I
will talk a little bit about

44
00:01:48,190 --> 00:01:51,940
ATLAS experiment, Large Hadron
Collider, ATLAS experiment

45
00:01:51,940 --> 00:01:55,730
computing the challenges of
a big data experiment, our

46
00:01:55,730 --> 00:01:59,090
interesting Clouds, and our
recent project Google Compute

47
00:01:59,090 --> 00:02:03,510
Engine, some conditional
clusters that we run on the

48
00:02:03,510 --> 00:02:08,500
grid and now we're running in
the Cloud, and we'll also

49
00:02:08,500 --> 00:02:10,419
describe the XRoot technology.

50
00:02:10,419 --> 00:02:12,980
Andy will go into more
details about that.

51
00:02:12,980 --> 00:02:15,140
We're saying that there are
products developed in the high

52
00:02:15,140 --> 00:02:16,190
energy community.

53
00:02:16,190 --> 00:02:19,080
Remember, worldwide web.

54
00:02:19,080 --> 00:02:21,630
There is a new generation of
technology developed that

55
00:02:21,630 --> 00:02:24,210
community, they're open source,
and we're saying

56
00:02:24,210 --> 00:02:26,960
they're ready to be shared with
people who are interested

57
00:02:26,960 --> 00:02:29,270
in Clouds and large
computational clustering,

58
00:02:29,270 --> 00:02:31,070
bridging virtual and real

59
00:02:31,070 --> 00:02:33,470
infrastructure, things like that.

60
00:02:33,470 --> 00:02:37,310

61
00:02:37,310 --> 00:02:39,670
ATLAS is a multipurpose detector
and Large Hadron

62
00:02:39,670 --> 00:02:41,090
Collider at CERN.

63
00:02:41,090 --> 00:02:44,620
The ATLAS experiment itself
is a large international

64
00:02:44,620 --> 00:02:48,420
collaboration of about 3,000
scientists and engineers from

65
00:02:48,420 --> 00:02:51,920
many universities and labs
around the globe.

66
00:02:51,920 --> 00:02:54,480
It took almost 20 years to
design and build the

67
00:02:54,480 --> 00:02:58,760
apparatus, but nevertheless,
it's very young collaboration

68
00:02:58,760 --> 00:03:01,980
with more than 1,200 graduate
students working ATLAS and

69
00:03:01,980 --> 00:03:05,200
driving the analyses.

70
00:03:05,200 --> 00:03:11,050
This photo shows the outline of
the LHC tunnel on an aerial

71
00:03:11,050 --> 00:03:14,880
view of countryside near
Geneva, Switzerland.

72
00:03:14,880 --> 00:03:17,620
The LHC is one of the largest
scientific instruments ever

73
00:03:17,620 --> 00:03:21,790
built and certainly only one
of the most complex.

74
00:03:21,790 --> 00:03:25,060
And everything about the LHC
is extreme, its size, its

75
00:03:25,060 --> 00:03:27,140
energy, its detectors.

76
00:03:27,140 --> 00:03:32,220
It's the coldest and emptiest
place in the solar system.

77
00:03:32,220 --> 00:03:34,190
It's the hottest place in the
universe, with temperatures of

78
00:03:34,190 --> 00:03:37,700
particle created under collision
exceeding trillions

79
00:03:37,700 --> 00:03:38,950
degrees of Celsius.

80
00:03:38,950 --> 00:03:42,660

81
00:03:42,660 --> 00:03:46,420
LHC tunnel is about 27
kilometers long.

82
00:03:46,420 --> 00:03:50,210
Thousands of superconducting
magnets working at near

83
00:03:50,210 --> 00:03:53,250
absolute zero temperature are
needed in order to accelerate

84
00:03:53,250 --> 00:03:56,970
and collide protons and heavy
ions at the highest

85
00:03:56,970 --> 00:03:59,890
temperatures ever achieved
in the lab.

86
00:03:59,890 --> 00:04:02,685
Such energies are needed in
order to explore the high

87
00:04:02,685 --> 00:04:05,580
energy frontier of modern
particle physics to discover

88
00:04:05,580 --> 00:04:08,600
things like Higgs boson, the
missing piece of the standard

89
00:04:08,600 --> 00:04:11,420
model, a particle that is
responsible for electric

90
00:04:11,420 --> 00:04:13,020
symmetry breaking
that generates

91
00:04:13,020 --> 00:04:15,310
massive elementary particles.

92
00:04:15,310 --> 00:04:18,930
LHC's aim to explore physics
beyond the standard model,

93
00:04:18,930 --> 00:04:21,690
things like supersymmetry,
possible existence of

94
00:04:21,690 --> 00:04:24,970
extradimensional, possible
candidates of dark matter,

95
00:04:24,970 --> 00:04:30,000
dark energy, and whatever
can be at that frontier.

96
00:04:30,000 --> 00:04:32,680

97
00:04:32,680 --> 00:04:35,550
The ATLAS detector is a
multipurpose apparatus

98
00:04:35,550 --> 00:04:40,000
designed to detect particles
created in the collision of

99
00:04:40,000 --> 00:04:41,750
the LHC beams.

100
00:04:41,750 --> 00:04:44,830
It's the largest detector of its
kind, and also one of the

101
00:04:44,830 --> 00:04:47,150
most complex.

102
00:04:47,150 --> 00:04:50,630
It's built like a Russian doll
with detectors inside other

103
00:04:50,630 --> 00:04:54,320
detectors with several giant
superconducting magnets.

104
00:04:54,320 --> 00:04:59,080
All of this is needed to
accurately register particle

105
00:04:59,080 --> 00:05:01,080
and identify them.

106
00:05:01,080 --> 00:05:04,150
It has very high granularity of
sensing elements with about

107
00:05:04,150 --> 00:05:08,120
150 millions of sensors
of various kinds.

108
00:05:08,120 --> 00:05:10,520
And it's capable of taking
snapshots of the collision

109
00:05:10,520 --> 00:05:13,950
events at the rate of 40
megahertz, or every 25

110
00:05:13,950 --> 00:05:16,010
nanoseconds.

111
00:05:16,010 --> 00:05:19,250
And it weighs about 7,000
tons, and it's

112
00:05:19,250 --> 00:05:22,780
quite large in size.

113
00:05:22,780 --> 00:05:24,940
This slide gives you a
feeling of the size

114
00:05:24,940 --> 00:05:26,240
of the ATLAS detector.

115
00:05:26,240 --> 00:05:29,830
Here, outlines of ATLAS and our
sister detector, CMS, are

116
00:05:29,830 --> 00:05:33,470
superimposed with the image of
the ATLAS and CMS six-story

117
00:05:33,470 --> 00:05:35,400
office building at CERN.

118
00:05:35,400 --> 00:05:37,870
And ATLAS is taller than
that building.

119
00:05:37,870 --> 00:05:39,120
It's a huge apparatus.

120
00:05:39,120 --> 00:05:41,860

121
00:05:41,860 --> 00:05:45,400
The detector itself sits in
the LHC tunnel about 100

122
00:05:45,400 --> 00:05:48,720
meters below the surface, and
it was assembled there piece

123
00:05:48,720 --> 00:05:51,390
by piece carefully like
a ship in the bottle.

124
00:05:51,390 --> 00:05:54,470
Quite extraordinary engineering
achievement.

125
00:05:54,470 --> 00:05:57,210
Especially taking into account
that the detector should be

126
00:05:57,210 --> 00:05:59,990
able to measure particle
positions with accuracy

127
00:05:59,990 --> 00:06:02,040
exceeding 100 microns.

128
00:06:02,040 --> 00:06:04,330
So it should be assembled with
a very high precision.

129
00:06:04,330 --> 00:06:06,950

130
00:06:06,950 --> 00:06:09,650
And that's how it looks fully
assembled in the cover.

131
00:06:09,650 --> 00:06:15,850

132
00:06:15,850 --> 00:06:18,810
As I mentioned before, LHC and
ATLAS were built to explore

133
00:06:18,810 --> 00:06:22,030
high energy frontier of modern
particle physics to search for

134
00:06:22,030 --> 00:06:25,300
new phenomena that may occur at
these energies to probe the

135
00:06:25,300 --> 00:06:27,960
very fundamental
laws of nature.

136
00:06:27,960 --> 00:06:32,020
In particular, to search for
Higgs boson, this missing

137
00:06:32,020 --> 00:06:34,940
piece of standard model
elementary particles

138
00:06:34,940 --> 00:06:37,440
responsible for electroweak
symmetry break and again

139
00:06:37,440 --> 00:06:40,570
generation of masses of other
elementary particles.

140
00:06:40,570 --> 00:06:43,240
And Higgs' mechanism was
suggested about 40 years ago,

141
00:06:43,240 --> 00:06:45,720
and since then physicists
around the world were

142
00:06:45,720 --> 00:06:48,530
searching for proof
of its existence.

143
00:06:48,530 --> 00:06:52,160
And recent discovery of a new
product at the LHC which looks

144
00:06:52,160 --> 00:06:54,240
like Higgs boson culminated
this search.

145
00:06:54,240 --> 00:07:02,580

146
00:07:02,580 --> 00:07:05,180
Less than a year ago, two
experiments at LHC ATLAS and

147
00:07:05,180 --> 00:07:08,580
CMS announced discovery
of the new particle.

148
00:07:08,580 --> 00:07:11,800
It was called a giant leap for
science, the most important

149
00:07:11,800 --> 00:07:15,100
discovery of the last decade
in the particle physics.

150
00:07:15,100 --> 00:07:17,580
It generated a lot of public
interest and a lot of media

151
00:07:17,580 --> 00:07:20,525
attention, with thousands
and thousands of print

152
00:07:20,525 --> 00:07:22,090
stories and TV spots.

153
00:07:22,090 --> 00:07:26,130
And most likely you have heard
about this discovery already.

154
00:07:26,130 --> 00:07:30,010
But you probably haven't
seen my next slide.

155
00:07:30,010 --> 00:07:36,640
I must note here that typically
we do not detect

156
00:07:36,640 --> 00:07:40,050
elementary particles like
Higgs directly.

157
00:07:40,050 --> 00:07:43,740
Like many other subatomic
particles, Higgs is too heavy,

158
00:07:43,740 --> 00:07:47,570
too unstable, and
too short lived.

159
00:07:47,570 --> 00:07:49,870
When produced, it immediately
decays into wider, more

160
00:07:49,870 --> 00:07:52,310
stable, more long-lived
particles that are actually

161
00:07:52,310 --> 00:07:55,400
registered by our detectors,
like ATLAS.

162
00:07:55,400 --> 00:07:58,060
And by the way, such decays
may be multi-staged, and

163
00:07:58,060 --> 00:08:01,170
[INAUDIBLE] stable object
decay [INAUDIBLE].

164
00:08:01,170 --> 00:08:02,820
After several of such
decays, you're

165
00:08:02,820 --> 00:08:05,410
getting final state particles.

166
00:08:05,410 --> 00:08:09,740
By using collected information
about decayed products of

167
00:08:09,740 --> 00:08:13,190
Higgs, what we call the final
state particles, we can

168
00:08:13,190 --> 00:08:15,720
nevertheless prove the existence
of parent particles,

169
00:08:15,720 --> 00:08:17,550
like Higgs.

170
00:08:17,550 --> 00:08:21,080
Indirectly, yes, but without
any doubt, just by using

171
00:08:21,080 --> 00:08:24,090
simple laws of conservation of
energy and momentum familiar

172
00:08:24,090 --> 00:08:27,380
from freshman physics 101.

173
00:08:27,380 --> 00:08:30,080
Whatever energy existed before
the particle decayed should be

174
00:08:30,080 --> 00:08:32,900
conserved after the decay and
mass equivalent to energy, as

175
00:08:32,900 --> 00:08:37,010
was pointed out by Einstein,
and mass of the decaying

176
00:08:37,010 --> 00:08:38,410
parent particle doesn't
disappear.

177
00:08:38,410 --> 00:08:40,360
It transformed to masses
[INAUDIBLE]

178
00:08:40,360 --> 00:08:42,500
the decay product to
other particles.

179
00:08:42,500 --> 00:08:45,470
So again, by using the
information collected about

180
00:08:45,470 --> 00:08:47,480
final state particles,
we can reconstruct

181
00:08:47,480 --> 00:08:49,200
the mass of the particle.

182
00:08:49,200 --> 00:08:51,180
And we measure many events.

183
00:08:51,180 --> 00:08:53,860
For each event, take all final
state particles that belong to

184
00:08:53,860 --> 00:08:55,040
a particular decay channel.

185
00:08:55,040 --> 00:08:58,640
We calculate effective mass of
such combination, and plug in

186
00:08:58,640 --> 00:09:02,400
a histogram of effective mass.

187
00:09:02,400 --> 00:09:05,610
And eventually we expect to see
a peak in that histogram

188
00:09:05,610 --> 00:09:08,350
that corresponds to the mass
of the parent particle.

189
00:09:08,350 --> 00:09:13,360
Of course, there may be other
particles that decay into the

190
00:09:13,360 --> 00:09:15,390
same final state, and you
will see a spectrum in

191
00:09:15,390 --> 00:09:17,370
distribution.

192
00:09:17,370 --> 00:09:19,840
But they should be at peak
corresponding to the mass of

193
00:09:19,840 --> 00:09:23,310
the particle you search for.

194
00:09:23,310 --> 00:09:25,710
There will be noise, but if
particle exists, there will be

195
00:09:25,710 --> 00:09:28,900
a signal simply because
of conservation

196
00:09:28,900 --> 00:09:30,150
of energy and momentum.

197
00:09:30,150 --> 00:09:33,160

198
00:09:33,160 --> 00:09:35,810
And on this slide, you can see
how the discovery of the Higgs

199
00:09:35,810 --> 00:09:38,500
boson unfolded.

200
00:09:38,500 --> 00:09:40,910
Here, looking at the spectrum
of effective mass of

201
00:09:40,910 --> 00:09:42,510
[INAUDIBLE]

202
00:09:42,510 --> 00:09:43,850
detected by ATLAS.

203
00:09:43,850 --> 00:09:44,800
Decaying to [INAUDIBLE]

204
00:09:44,800 --> 00:09:47,860
is one of the predicted decay
channels for the Higgs boson.

205
00:09:47,860 --> 00:09:50,940
And as we were collecting
more and more data,

206
00:09:50,940 --> 00:09:53,800
the peak at 125 GV--

207
00:09:53,800 --> 00:09:56,600
and GV is a unit of mass used in
high energy physics-- just

208
00:09:56,600 --> 00:10:00,530
kept growing, indicating that
there is a new particle, very

209
00:10:00,530 --> 00:10:05,088
heavy, never seen before, clear
and beautiful peak.

210
00:10:05,088 --> 00:10:07,960

211
00:10:07,960 --> 00:10:09,680
And of course, you
do search in many

212
00:10:09,680 --> 00:10:11,550
different decay channels.

213
00:10:11,550 --> 00:10:14,280
Higgs can decay into anything
that is not forbidden by

214
00:10:14,280 --> 00:10:15,700
conservation laws.

215
00:10:15,700 --> 00:10:17,950
In one event, it can decay into
gamma [INAUDIBLE], in

216
00:10:17,950 --> 00:10:21,420
another into [INAUDIBLE], and so
forth, and some events may

217
00:10:21,420 --> 00:10:24,060
have no Higgs in there.

218
00:10:24,060 --> 00:10:27,990
But it's the same Higgs, so in
every decay channel, you

219
00:10:27,990 --> 00:10:32,180
should expect a peak at the same
place, at the same mass

220
00:10:32,180 --> 00:10:35,050
in these various
decay channels.

221
00:10:35,050 --> 00:10:37,270
And you see the clear indication
that there is

222
00:10:37,270 --> 00:10:41,920
something at 125 GV.

223
00:10:41,920 --> 00:10:46,860
Now, let's talk about data
challenges of such analysis.

224
00:10:46,860 --> 00:10:49,130
Particles that we want to
discover and study are rare.

225
00:10:49,130 --> 00:10:51,580
That's why LHC runs at such
high energy and with such

226
00:10:51,580 --> 00:10:53,400
intense beams, almost a billion

227
00:10:53,400 --> 00:10:55,590
interactions per second.

228
00:10:55,590 --> 00:10:58,065
Even in this condition, the
probability to create Higgs

229
00:10:58,065 --> 00:10:59,320
boson is tiny.

230
00:10:59,320 --> 00:11:01,550
You would need to search for
one Higgs in more than a

231
00:11:01,550 --> 00:11:03,850
trillion events.

232
00:11:03,850 --> 00:11:06,720
And at high beam densities,
multiple collisions can occur

233
00:11:06,720 --> 00:11:10,360
in one beam crossing.

234
00:11:10,360 --> 00:11:14,660
This plot shows how such
event looks like.

235
00:11:14,660 --> 00:11:16,800
It's quite messy.

236
00:11:16,800 --> 00:11:19,270
That explains why you need such
a big detector, such high

237
00:11:19,270 --> 00:11:22,500
granularity, such strong
magnetic fields, so many

238
00:11:22,500 --> 00:11:26,160
channels of sensors, so many
channels of electronics,

239
00:11:26,160 --> 00:11:30,960
because you are searching
for something very rare.

240
00:11:30,960 --> 00:11:36,710
And that's probably for Google
Developers, at a familiar

241
00:11:36,710 --> 00:11:39,950
problem, your selectivity should
be very, very high,

242
00:11:39,950 --> 00:11:42,020
like one in several trillions.

243
00:11:42,020 --> 00:11:44,290
It's like looking for one
person's cells in the world

244
00:11:44,290 --> 00:11:50,140
population, of one needle
in 20 million haystacks.

245
00:11:50,140 --> 00:11:54,360
ATLAS is the quintessential
big data experiment.

246
00:11:54,360 --> 00:11:56,880
ATLAS detector generates
about one petabyte per

247
00:11:56,880 --> 00:11:59,580
second of raw data.

248
00:11:59,580 --> 00:12:01,220
No one can store it,
even Google.

249
00:12:01,220 --> 00:12:04,980
Most is filtered out in real
time by the trigger system.

250
00:12:04,980 --> 00:12:07,930
Interesting events are
recorded for further

251
00:12:07,930 --> 00:12:10,130
reconstruction analysis.

252
00:12:10,130 --> 00:12:13,870
And as of this year, we're
managing about 140 petabytes

253
00:12:13,870 --> 00:12:17,960
of data worldwide, and that's
distributed over

254
00:12:17,960 --> 00:12:19,450
100 computing centers.

255
00:12:19,450 --> 00:12:22,150
And that's actually not only
raw data, but the derived

256
00:12:22,150 --> 00:12:25,890
formats, the simulation,
about 50% of our data.

257
00:12:25,890 --> 00:12:29,970
The Monte Carlo simulation of
various properties, simulation

258
00:12:29,970 --> 00:12:31,760
of how the particle propagates
a detector

259
00:12:31,760 --> 00:12:34,830
iteration, things like that.

260
00:12:34,830 --> 00:12:38,030
And we expect that these data
rates will be only growing

261
00:12:38,030 --> 00:12:43,130
after LHC large shut down,
what's happening now in 2013,

262
00:12:43,130 --> 00:12:45,090
the data rate will be higher,
the energy of

263
00:12:45,090 --> 00:12:46,360
collision will be higher.

264
00:12:46,360 --> 00:12:49,610
We expect the influx of already
filtered data on the

265
00:12:49,610 --> 00:12:53,430
level of 40 petabytes
per year.

266
00:12:53,430 --> 00:12:55,760
And we have to deliver this
data to thousands of

267
00:12:55,760 --> 00:12:57,010
physicists worldwide.

268
00:12:57,010 --> 00:12:59,210

269
00:12:59,210 --> 00:13:01,830
Now, a little bit about
ATLAS computing.

270
00:13:01,830 --> 00:13:04,570
ATLAS uses grid computing
paradigm for organization of

271
00:13:04,570 --> 00:13:06,190
distributed resources.

272
00:13:06,190 --> 00:13:08,410
Job distribution is
managed by PanDA

273
00:13:08,410 --> 00:13:09,760
Workload Management System.

274
00:13:09,760 --> 00:13:11,830
PanDA stands for production
analysis.

275
00:13:11,830 --> 00:13:14,340
Think of it as grid
metascheduler.

276
00:13:14,340 --> 00:13:16,810
PanDA was developed by ATLAS,
and now it manages

277
00:13:16,810 --> 00:13:19,520
distribution of computing jobs
for hundreds of computing

278
00:13:19,520 --> 00:13:24,060
sites, about 100,000 cores, 100
million jobs per year, and

279
00:13:24,060 --> 00:13:27,630
serving thousands of users.

280
00:13:27,630 --> 00:13:30,690
Organizationally, it's a grid
set up in a tiered system,

281
00:13:30,690 --> 00:13:32,460
highly hierarchical
with the tier zero

282
00:13:32,460 --> 00:13:34,090
central located at CERN.

283
00:13:34,090 --> 00:13:39,330
Tier zero center receives the
raw data from the ATLAS

284
00:13:39,330 --> 00:13:42,530
detectors, performs first pass
analysis, and then distributes

285
00:13:42,530 --> 00:13:44,730
among other tiers.

286
00:13:44,730 --> 00:13:48,230
Tier one is typically national
based large centers.

287
00:13:48,230 --> 00:13:50,880
One of them is the
Brookhaven Lab.

288
00:13:50,880 --> 00:13:54,260
And each tier one facility then
distributors derived data

289
00:13:54,260 --> 00:13:56,820
to tier two computing facilities
that provides data

290
00:13:56,820 --> 00:13:59,720
storage and processing
capabilities for more in depth

291
00:13:59,720 --> 00:14:00,970
end user analysis.

292
00:14:00,970 --> 00:14:04,080

293
00:14:04,080 --> 00:14:06,530
This plot shows the distribution
of running jobs,

294
00:14:06,530 --> 00:14:07,890
both Monte Carlo simulation--

295
00:14:07,890 --> 00:14:09,560
we call them production jobs--

296
00:14:09,560 --> 00:14:12,100
on the ATLAS grid and in
analysis jobs on the ATLAS

297
00:14:12,100 --> 00:14:13,630
grid for the past year.

298
00:14:13,630 --> 00:14:18,310
We run about 100,000 cores
worldwide, processing 150,000

299
00:14:18,310 --> 00:14:19,860
jobs per day.

300
00:14:19,860 --> 00:14:23,520
It's clear that all available
computational resources are

301
00:14:23,520 --> 00:14:26,370
fully stressed and utilized.

302
00:14:26,370 --> 00:14:29,560
And on this plot, you see the
distribution of pending jobs,

303
00:14:29,560 --> 00:14:32,490
submitted but waiting for
execution on ATLAS grid for

304
00:14:32,490 --> 00:14:33,530
the past year.

305
00:14:33,530 --> 00:14:35,680
You can see that the job
submission pattern is very

306
00:14:35,680 --> 00:14:37,210
uneven in time.

307
00:14:37,210 --> 00:14:39,550
Spikes in demand usually happen
before major physics

308
00:14:39,550 --> 00:14:42,420
conference or during
data reprocessing.

309
00:14:42,420 --> 00:14:44,770
And demand can exceed available
computational

310
00:14:44,770 --> 00:14:47,000
resources by more than an
order of magnitude.

311
00:14:47,000 --> 00:14:50,480
Lack of resources slows down
the pace of scientific

312
00:14:50,480 --> 00:14:53,560
discovery, and that's why ATLAS
is interested in cloud

313
00:14:53,560 --> 00:14:58,760
computing, and, in particular,
in public cloud resources.

314
00:14:58,760 --> 00:15:01,750
A couple of years ago, ATLAS
set up cloud computing R&D

315
00:15:01,750 --> 00:15:04,020
project to explore
virtualization and cloud

316
00:15:04,020 --> 00:15:06,700
computing primarily as a
tool to cope with peak

317
00:15:06,700 --> 00:15:07,980
loads on the grid.

318
00:15:07,980 --> 00:15:10,910
We wanted to learn how to use
public and private clouds in

319
00:15:10,910 --> 00:15:13,810
typical ATLAS computational
scenarios.

320
00:15:13,810 --> 00:15:16,580
Since then, we gained experience
with many cloud

321
00:15:16,580 --> 00:15:19,420
platforms, like Amazon,
[INAUDIBLE]

322
00:15:19,420 --> 00:15:22,120
consortium of European cloud
providers, future grid

323
00:15:22,120 --> 00:15:25,740
[INAUDIBLE] academic clouds in
US and Canada and many others.

324
00:15:25,740 --> 00:15:29,510
We also explored private and
hybrid cloud configuration

325
00:15:29,510 --> 00:15:33,240
based on OpenStack, cloud stack
open nebula, and others.

326
00:15:33,240 --> 00:15:35,710
And now our latest project was
on Google Compute Engine, and

327
00:15:35,710 --> 00:15:38,510
I'll talk about it
now in detail.

328
00:15:38,510 --> 00:15:41,510
We were invited to participate
in Google Compute Engine trial

329
00:15:41,510 --> 00:15:43,610
period in August 2012.

330
00:15:43,610 --> 00:15:46,320
And we were immediately
attracted by modern hardware,

331
00:15:46,320 --> 00:15:49,880
powerful API, and competitive
pricing.

332
00:15:49,880 --> 00:15:53,310
And this is Google, after all.

333
00:15:53,310 --> 00:15:55,300
At the beginning, we were
frustrated that none of the

334
00:15:55,300 --> 00:15:57,695
tools that we had used before
supported Google Compute

335
00:15:57,695 --> 00:16:00,130
Engine, so initial a lot of
manual labor and image

336
00:16:00,130 --> 00:16:03,670
building cluster management was
needed since we couldn't

337
00:16:03,670 --> 00:16:07,000
reuse our standard tools.

338
00:16:07,000 --> 00:16:09,520
We're glad to see here at Google
I/O that this situation

339
00:16:09,520 --> 00:16:12,590
is changing and many tools are
supporting Google Compute

340
00:16:12,590 --> 00:16:14,310
Engine now.

341
00:16:14,310 --> 00:16:18,480
But Google engineers were very
helpful in helping us with

342
00:16:18,480 --> 00:16:21,740
initial setup and debugging the
problems and explaining to

343
00:16:21,740 --> 00:16:24,990
us features of the Google Cloud
platform implementation.

344
00:16:24,990 --> 00:16:27,780
And also Google was very
gracious in providing more

345
00:16:27,780 --> 00:16:30,320
resources than the initial
trial quarter so we could

346
00:16:30,320 --> 00:16:34,650
start working on the larger
scale cluster.

347
00:16:34,650 --> 00:16:38,110
We wanted to try several ATLAS
computational scenarios, high

348
00:16:38,110 --> 00:16:40,990
performance analysis clusters
like PROOF.

349
00:16:40,990 --> 00:16:43,450
We wanted to learn about storage
and data management of

350
00:16:43,450 --> 00:16:45,970
the cloud, in particular
utilizing XRootD technology

351
00:16:45,970 --> 00:16:49,450
for storage aggregation,
Ephemeral Storage aggregation,

352
00:16:49,450 --> 00:16:51,480
and federation.

353
00:16:51,480 --> 00:16:54,640
We also wanted to try a life
scale Monte Carlo production

354
00:16:54,640 --> 00:16:57,350
simulation of the cloud using
PanDA Workload Management

355
00:16:57,350 --> 00:17:01,760
System, as well as some other
smaller projects.

356
00:17:01,760 --> 00:17:03,570
So let me talk about
PanDA [INAUDIBLE]

357
00:17:03,570 --> 00:17:05,520
on Google Compute Engine.

358
00:17:05,520 --> 00:17:08,730
Google agreed to allocate
additional resources for ATLAS

359
00:17:08,730 --> 00:17:12,280
at the tune of about five
million core hours.

360
00:17:12,280 --> 00:17:20,130
Resources were organized as
HTCondor PanDA queue, and that

361
00:17:20,130 --> 00:17:22,569
allows for transparent inclusion
of the cloud

362
00:17:22,569 --> 00:17:25,000
resource into ATLAS
computational grid.

363
00:17:25,000 --> 00:17:27,720
Google Compute Engine looks as a
part of the ATLAS grid, just

364
00:17:27,720 --> 00:17:29,180
like another grid site.

365
00:17:29,180 --> 00:17:30,770
Very transparent.

366
00:17:30,770 --> 00:17:32,970
It was intended to run CPU
intensive Monte Carlo

367
00:17:32,970 --> 00:17:36,280
simulation, and the idea was
to try to have a production

368
00:17:36,280 --> 00:17:38,360
type of run on Google
Compute Engine.

369
00:17:38,360 --> 00:17:40,730
And the system was delivered
to ATLAS as a production

370
00:17:40,730 --> 00:17:44,340
resource, not as R&D platform.

371
00:17:44,340 --> 00:17:46,690
We ran for about eight weeks.

372
00:17:46,690 --> 00:17:48,650
Two weeks were planned
for start up.

373
00:17:48,650 --> 00:17:53,130
And we had very stable
running.

374
00:17:53,130 --> 00:17:55,020
The Google Compute Engine
was rock solid.

375
00:17:55,020 --> 00:17:57,240
We had a few problems, and most
of them were on the ATLAS

376
00:17:57,240 --> 00:18:00,320
side, were on computational
intensive job, not much I/O,

377
00:18:00,320 --> 00:18:02,670
for this particular workload.

378
00:18:02,670 --> 00:18:04,430
These were physics [INAUDIBLE]
generators [INAUDIBLE]

379
00:18:04,430 --> 00:18:07,110
fast detector simulation, full
detector simulation.

380
00:18:07,110 --> 00:18:09,230
Produced data was automatically
shipped to ATLAS

381
00:18:09,230 --> 00:18:11,760
grid storage for further
processing and analysis.

382
00:18:11,760 --> 00:18:14,400
That was really usable data.

383
00:18:14,400 --> 00:18:20,340
We completed about 450,000 jobs
generated and processed

384
00:18:20,340 --> 00:18:23,510
about more than 200
million events.

385
00:18:23,510 --> 00:18:25,970
Very good performance, very
comparable to performance of

386
00:18:25,970 --> 00:18:28,280
ATLAS grid.

387
00:18:28,280 --> 00:18:30,680
This plot shows the job
failure rate as

388
00:18:30,680 --> 00:18:31,910
a function of time.

389
00:18:31,910 --> 00:18:34,580
Most failures occurred during
start up and scale up period

390
00:18:34,580 --> 00:18:35,990
as we expected.

391
00:18:35,990 --> 00:18:38,130
Most problems were actually
on the ATLAS side.

392
00:18:38,130 --> 00:18:40,680
No failures were due to the
Google Compute Engine.

393
00:18:40,680 --> 00:18:44,000
Very stable performance
of the platform.

394
00:18:44,000 --> 00:18:46,960
This plot shows distribution of
finished and failed jobs.

395
00:18:46,960 --> 00:18:49,420
Green histogram is for finished
jobs, the pink one

396
00:18:49,420 --> 00:18:50,080
for failed ones.

397
00:18:50,080 --> 00:18:51,170
Again, very good performance.

398
00:18:51,170 --> 00:18:53,000
We reached high rates
of production,

399
00:18:53,000 --> 00:18:55,220
50,000 jobs per day.

400
00:18:55,220 --> 00:18:56,470
Good number.

401
00:18:56,470 --> 00:18:58,230

402
00:18:58,230 --> 00:19:01,590
We also tried PROOF clusters.

403
00:19:01,590 --> 00:19:04,310
And PROOF is implementation of
MapReduce paradigm based on

404
00:19:04,310 --> 00:19:05,605
the ROOT framework.

405
00:19:05,605 --> 00:19:09,880
ROOT framework for data analysis
was developed by high

406
00:19:09,880 --> 00:19:12,630
energy nuclear physics
community, developed and

407
00:19:12,630 --> 00:19:14,846
supported by the ROOT
team at CERN.

408
00:19:14,846 --> 00:19:17,370
It's written in C++, free,
open source, very high

409
00:19:17,370 --> 00:19:18,510
performance.

410
00:19:18,510 --> 00:19:21,840
And we'll have a slide with the
pointers to the system.

411
00:19:21,840 --> 00:19:24,330
And PROOF allows for efficient
aggregation and use of

412
00:19:24,330 --> 00:19:27,700
distributed computing resources
for data intensive

413
00:19:27,700 --> 00:19:29,940
event based analyses.

414
00:19:29,940 --> 00:19:32,040
It uses XRootD for clustering,
storage

415
00:19:32,040 --> 00:19:34,200
aggregation, data discovery.

416
00:19:34,200 --> 00:19:37,600
Xroot is well suited for
ephemeral storage aggregation

417
00:19:37,600 --> 00:19:40,390
into one name space.

418
00:19:40,390 --> 00:19:42,910
And PROOF cluster also
can be federated.

419
00:19:42,910 --> 00:19:46,430
So on this slide, you can see
typical architecture of the

420
00:19:46,430 --> 00:19:50,750
PROOF clusters with the super
master, which serves as users'

421
00:19:50,750 --> 00:19:51,840
single point of entry.

422
00:19:51,840 --> 00:19:55,680
System complex is completely
hidden from users.

423
00:19:55,680 --> 00:19:59,500
And it allows, for example,
for interactive analysis,

424
00:19:59,500 --> 00:20:02,380
where you can send the query and
in real time see how the

425
00:20:02,380 --> 00:20:05,350
particular histogram just
grows and changes.

426
00:20:05,350 --> 00:20:07,250
One of the distinctive
feature.

427
00:20:07,250 --> 00:20:09,820
But it also allows batch
analysis, and you can connect

428
00:20:09,820 --> 00:20:12,170
to the system, look at the
histogram, and disconnect,

429
00:20:12,170 --> 00:20:14,610
then connect again, look again,
see how it's going.

430
00:20:14,610 --> 00:20:17,380

431
00:20:17,380 --> 00:20:19,305
And here's another view of
the typical structure

432
00:20:19,305 --> 00:20:20,420
of the PROOF cluster.

433
00:20:20,420 --> 00:20:22,750
PROOF, as I mentioned,
works very well with

434
00:20:22,750 --> 00:20:24,380
XRootD based storage.

435
00:20:24,380 --> 00:20:28,260
XRootD provides data discovery
and hails PROOF exploits this

436
00:20:28,260 --> 00:20:29,510
data locality [INAUDIBLE].

437
00:20:29,510 --> 00:20:32,140

438
00:20:32,140 --> 00:20:34,530
Access to Google Compute Engine
allowed us to build and

439
00:20:34,530 --> 00:20:38,520
test large PROOF clusters, up
to 1,000 workers, something

440
00:20:38,520 --> 00:20:40,410
that is very difficult to
do in the real domain.

441
00:20:40,410 --> 00:20:44,360
We just don't have resources
to do this kind of test and

442
00:20:44,360 --> 00:20:45,610
see how it all scales.

443
00:20:45,610 --> 00:20:48,700
And the figure here shows
scalability test for 500

444
00:20:48,700 --> 00:20:50,690
worker cluster.

445
00:20:50,690 --> 00:20:52,700
And it shows very good
performance and pretty good

446
00:20:52,700 --> 00:20:53,950
scalability.

447
00:20:53,950 --> 00:20:57,670

448
00:20:57,670 --> 00:21:01,220
We also look at the storage
performance of

449
00:21:01,220 --> 00:21:02,470
Google Compute Engine.

450
00:21:02,470 --> 00:21:09,210
And this plot shows the
performance in the typical

451
00:21:09,210 --> 00:21:14,140
ATLAS analysis scenario of
ephemeral store, persistent

452
00:21:14,140 --> 00:21:17,890
store, and here compare it to
what happened if you have all

453
00:21:17,890 --> 00:21:19,970
the data in the memory.

454
00:21:19,970 --> 00:21:22,340
And know that ephemeral disk
has better single worker

455
00:21:22,340 --> 00:21:27,430
performance, but the persistent
storage shows

456
00:21:27,430 --> 00:21:29,750
better scaling and better
peak performance.

457
00:21:29,750 --> 00:21:32,920
And of course, in this situation
it's clear that the

458
00:21:32,920 --> 00:21:34,520
RAID is needed for better
performance.

459
00:21:34,520 --> 00:21:37,780

460
00:21:37,780 --> 00:21:43,030
We also look at the data
transfer capabilities.

461
00:21:43,030 --> 00:21:47,180
And this plot shows the data
transfer from our own

462
00:21:47,180 --> 00:21:51,570
Federated ATLAS Xroot to Google
Compute Engine in

463
00:21:51,570 --> 00:21:54,480
extreme copy mode, which is sort
of similar to bitTorrent.

464
00:21:54,480 --> 00:21:57,180
If you have several copies,
you can copy them in

465
00:21:57,180 --> 00:21:59,530
multisource, multistream mode.

466
00:21:59,530 --> 00:22:03,140
And Google Compute Engine Xroot
cluster using ephemeral

467
00:22:03,140 --> 00:22:07,120
storage was used for this test,
and average transfer

468
00:22:07,120 --> 00:22:11,050
rate was about 60 megabyte
per second.

469
00:22:11,050 --> 00:22:14,190
And this is very good taking
into account that this is over

470
00:22:14,190 --> 00:22:16,315
completely unmanaged
public network.

471
00:22:16,315 --> 00:22:18,090
We have no control there.

472
00:22:18,090 --> 00:22:20,950
But still, this is single
client performance.

473
00:22:20,950 --> 00:22:24,540
Many of the plots that I'm
showing, they're single client

474
00:22:24,540 --> 00:22:26,810
because the system that we're
running, they scale very well

475
00:22:26,810 --> 00:22:28,140
when you add resources.

476
00:22:28,140 --> 00:22:31,360
So you expect clustering and
scaling up, so what you are

477
00:22:31,360 --> 00:22:32,390
really interested in is how the

478
00:22:32,390 --> 00:22:34,710
building blocks are running.

479
00:22:34,710 --> 00:22:37,660
And here is single client, but
you can run on multiple VMs,

480
00:22:37,660 --> 00:22:38,875
multiple clients simultaneously

481
00:22:38,875 --> 00:22:41,720
and stream them over.

482
00:22:41,720 --> 00:22:44,590
And we're also thinking about
dedicated network peering

483
00:22:44,590 --> 00:22:46,680
between ATLAS network

484
00:22:46,680 --> 00:22:48,860
infrastructure and Google storage.

485
00:22:48,860 --> 00:22:52,400
So that will give us much
higher, 100 gigabyte per

486
00:22:52,400 --> 00:22:55,750
second performance, than we
would be able to control it

487
00:22:55,750 --> 00:22:57,660
and do manageable transfers.

488
00:22:57,660 --> 00:23:00,770
But this is if you just want to
run and bring the data in.

489
00:23:00,770 --> 00:23:02,020
It's doable.

490
00:23:02,020 --> 00:23:04,340

491
00:23:04,340 --> 00:23:08,230
And now we'll talk about Xroot,
this clustering and

492
00:23:08,230 --> 00:23:12,130
storage clustering technology
that Andy was a creator and a

493
00:23:12,130 --> 00:23:15,550
driving force behind this
project, and still is.

494
00:23:15,550 --> 00:23:16,760
And Andy, please.

495
00:23:16,760 --> 00:23:20,826
[APPLAUSE]

496
00:23:20,826 --> 00:23:23,630
ANDREW HANUSHEVSKY:
[INAUDIBLE].

497
00:23:23,630 --> 00:23:27,780
So I want to take a quick trip
through a bit of technology we

498
00:23:27,780 --> 00:23:32,280
developed a while back but wound
up being absolutely an

499
00:23:32,280 --> 00:23:34,600
ideal match with the Google
Compute Engine.

500
00:23:34,600 --> 00:23:35,930
And that's XRootD.

501
00:23:35,930 --> 00:23:37,815
Now you'll say, never
heard of it.

502
00:23:37,815 --> 00:23:40,710
Well, it's a system for scalable
cluster data access.

503
00:23:40,710 --> 00:23:41,850
And you say, that's nice.

504
00:23:41,850 --> 00:23:43,080
What is it really?

505
00:23:43,080 --> 00:23:45,500
Well, what we have is an
implementation of two

506
00:23:45,500 --> 00:23:49,780
services, one an XRoot service
that provides access to data.

507
00:23:49,780 --> 00:23:52,400
So you would take one of these
demons and drop it on every

508
00:23:52,400 --> 00:23:55,510
node where you have data that
you need to access.

509
00:23:55,510 --> 00:23:57,380
Now, there's a companion
service.

510
00:23:57,380 --> 00:23:58,760
It's called CMSD.

511
00:23:58,760 --> 00:24:01,050
Stands for Cluster Management
Services.

512
00:24:01,050 --> 00:24:04,540
And that's used for data
discovery as well as routing

513
00:24:04,540 --> 00:24:08,150
clients to where the data is
and server clustering.

514
00:24:08,150 --> 00:24:11,880
So these are separate, but we
normally use them together.

515
00:24:11,880 --> 00:24:14,710
And so for the purposes of this
talk, we'll always talk

516
00:24:14,710 --> 00:24:16,700
about this particular pair.

517
00:24:16,700 --> 00:24:19,120
Now I want to emphasize
that this system

518
00:24:19,120 --> 00:24:21,030
is not a file system.

519
00:24:21,030 --> 00:24:24,300
People are actually using the
system to cluster existing

520
00:24:24,300 --> 00:24:25,030
file systems.

521
00:24:25,030 --> 00:24:30,570
So we have people basically
taking HDFS, GPFS, Lustre, and

522
00:24:30,570 --> 00:24:34,580
building one big giant cluster
out of that and having uniform

523
00:24:34,580 --> 00:24:36,530
data access.

524
00:24:36,530 --> 00:24:40,300
So while it's not a file system,
it's also not just for

525
00:24:40,300 --> 00:24:41,890
file systems.

526
00:24:41,890 --> 00:24:45,780
We have an experiment that's
using this as a framework to

527
00:24:45,780 --> 00:24:51,160
cluster MySQL tables across
hundreds of MySQL servers so

528
00:24:51,160 --> 00:24:55,400
they could do massively
parallel queries.

529
00:24:55,400 --> 00:24:59,220
So the idea is that if we don't
have a plug-in for your

530
00:24:59,220 --> 00:25:02,630
data, and you can write a
plug-in for your data, then

531
00:25:02,630 --> 00:25:03,850
you can cluster it.

532
00:25:03,850 --> 00:25:06,560
And so I'd like to show you what
that plug-in architecture

533
00:25:06,560 --> 00:25:07,620
looks like.

534
00:25:07,620 --> 00:25:11,150
First, we start off with
a protocol driver.

535
00:25:11,150 --> 00:25:14,160
You can plug-in any number of
protocols into that driver.

536
00:25:14,160 --> 00:25:16,580
In our particular case,
we want to do XRoot.

537
00:25:16,580 --> 00:25:19,410
So let's take a look as we
plug stuff together.

538
00:25:19,410 --> 00:25:21,650
So plug in your protocol.

539
00:25:21,650 --> 00:25:24,140
Plug in your authentication
framework.

540
00:25:24,140 --> 00:25:27,130
Plug in your logical file
system, your authorization

541
00:25:27,130 --> 00:25:30,190
framework, your storage
system, and then your

542
00:25:30,190 --> 00:25:31,220
clustering.

543
00:25:31,220 --> 00:25:34,550
And all of a sudden, you've
built up a clustering system

544
00:25:34,550 --> 00:25:38,050
for a particular kind
of application.

545
00:25:38,050 --> 00:25:42,150
So let's get back to what the
data access problem is.

546
00:25:42,150 --> 00:25:44,140
It's the High Energy
Physics regime.

547
00:25:44,140 --> 00:25:48,680
Yeah, they do nasty things,
like start up thousands of

548
00:25:48,680 --> 00:25:50,150
parallel jobs.

549
00:25:50,150 --> 00:25:53,480
And they all start up pretty
much at the same time.

550
00:25:53,480 --> 00:25:55,490
And if that weren't bad enough,
each one of those

551
00:25:55,490 --> 00:25:58,100
opens 10 or more files.

552
00:25:58,100 --> 00:26:00,200
Pretty much a profile
[INAUDIBLE] denial service

553
00:26:00,200 --> 00:26:02,320
attack, if you ask me.

554
00:26:02,320 --> 00:26:05,820
But basically, you have
to handle that.

555
00:26:05,820 --> 00:26:09,250
To make matters worse, the
particular framework that they

556
00:26:09,250 --> 00:26:13,320
use is small block sparse random
I/O. What do we mean by

557
00:26:13,320 --> 00:26:14,430
small block?

558
00:26:14,430 --> 00:26:16,900
Average read size about 4K.

559
00:26:16,900 --> 00:26:18,530
What do we mean by sparse?

560
00:26:18,530 --> 00:26:22,110
Well, they have like a
10-gigabyte file, and you'll

561
00:26:22,110 --> 00:26:25,780
be lucky if they read
half of it randomly.

562
00:26:25,780 --> 00:26:27,670
So pretty challenging.

563
00:26:27,670 --> 00:26:31,100
So we adopted a synergistic
solution to try

564
00:26:31,100 --> 00:26:33,740
to attack this problem.

565
00:26:33,740 --> 00:26:37,220
And you'll see what we
mean by synergy here.

566
00:26:37,220 --> 00:26:39,820
First, we wanted to minimize
the latency.

567
00:26:39,820 --> 00:26:42,600
And the key elements there
were using a paralyzable

568
00:26:42,600 --> 00:26:46,700
protocol, file sessions, a
sticky thread model, and

569
00:26:46,700 --> 00:26:50,970
lockless I/O. Next, we wanted
to minimize hardware

570
00:26:50,970 --> 00:26:54,360
requirements, so short code
paths, compact data

571
00:26:54,360 --> 00:26:58,900
structures, members that are
cognizant of what the memory

572
00:26:58,900 --> 00:27:00,835
cache is so it's friendly
to the memory cache.

573
00:27:00,835 --> 00:27:03,340

574
00:27:03,340 --> 00:27:06,300
We don't actually move data
around in the server, and we

575
00:27:06,300 --> 00:27:08,810
don't do crossthread
data sharing.

576
00:27:08,810 --> 00:27:12,280
So in the end, we wind up with
less than seven microseconds

577
00:27:12,280 --> 00:27:16,450
overhead on a two gigahertz CPU
per I/O request and less

578
00:27:16,450 --> 00:27:18,870
than a 100-megabyte
memory footprint.

579
00:27:18,870 --> 00:27:20,730
So pretty compact.

580
00:27:20,730 --> 00:27:23,610
Now those two are synergistic
in the sense that if you

581
00:27:23,610 --> 00:27:27,000
minimize latency, you'll see
opportunities to minimize

582
00:27:27,000 --> 00:27:28,590
hardware requirements.

583
00:27:28,590 --> 00:27:32,500
And if you start minimizing
hardware requirements, you see

584
00:27:32,500 --> 00:27:35,570
opportunities to minimize
latency.

585
00:27:35,570 --> 00:27:39,570
The next thing we wanted to do
was minimize human cost.

586
00:27:39,570 --> 00:27:40,980
So what does that mean?

587
00:27:40,980 --> 00:27:45,110
Well for us that meant a single
configuration file, no

588
00:27:45,110 --> 00:27:47,430
database requirement.

589
00:27:47,430 --> 00:27:49,620
You can add and delete
nodes at will.

590
00:27:49,620 --> 00:27:50,200
We don't care.

591
00:27:50,200 --> 00:27:51,550
You don't have to restart
anything.

592
00:27:51,550 --> 00:27:54,220
You just add stuff
and delete stuff.

593
00:27:54,220 --> 00:27:58,530
And you use your natural file
system administration tools to

594
00:27:58,530 --> 00:28:02,140
administer this thing because
that's what you know.

595
00:28:02,140 --> 00:28:06,750
Now, that together, we wanted
to maximize scaling.

596
00:28:06,750 --> 00:28:09,510
And those two are actually
synergistic.

597
00:28:09,510 --> 00:28:11,640
You can't maximize scaling
unless you

598
00:28:11,640 --> 00:28:13,300
minimize the human cost.

599
00:28:13,300 --> 00:28:15,700
And you see immediately
opportunities between those

600
00:28:15,700 --> 00:28:18,820
two as you attack both
of those problems.

601
00:28:18,820 --> 00:28:21,570
So let's talk about scaling.

602
00:28:21,570 --> 00:28:24,340
We used B64 trees for scaling.

603
00:28:24,340 --> 00:28:29,060
And we'll basically scale this
using this pair, XRootD CMS.

604
00:28:29,060 --> 00:28:32,920
So let's start out with
a single node.

605
00:28:32,920 --> 00:28:35,770
And then what we'll do
is we'll add 64 data

606
00:28:35,770 --> 00:28:39,120
servers to that node.

607
00:28:39,120 --> 00:28:42,440
Gee, looks like a really dinky
cluster, doesn't it?

608
00:28:42,440 --> 00:28:46,310
So what we do, it's a B64 tree,
well, we'll add 64 data

609
00:28:46,310 --> 00:28:48,940
servers to each one
of those 64 nodes.

610
00:28:48,940 --> 00:28:52,170
Well now we have a
cluster of 4,096.

611
00:28:52,170 --> 00:28:54,340
Decent, but not very big.

612
00:28:54,340 --> 00:28:56,350
We can just repeat this step.

613
00:28:56,350 --> 00:29:02,010
And now we've accomplished a
cluster of 262,000 servers.

614
00:29:02,010 --> 00:29:06,640
Well, that looks big, but what
if we iterate one more time?

615
00:29:06,640 --> 00:29:09,200
Now we've constructed
a cluster of 16

616
00:29:09,200 --> 00:29:12,240
million data servers.

617
00:29:12,240 --> 00:29:15,340
And you look at that and
say, hm, that looks

618
00:29:15,340 --> 00:29:17,860
like a house of cards.

619
00:29:17,860 --> 00:29:21,130
Well, not really because we can
replicate the head node,

620
00:29:21,130 --> 00:29:24,530
geographically distribute it,
and now we have quite a bit of

621
00:29:24,530 --> 00:29:26,660
redundancy in the system.

622
00:29:26,660 --> 00:29:29,220
So let's add some names
to these things.

623
00:29:29,220 --> 00:29:30,770
The head node is the manager.

624
00:29:30,770 --> 00:29:32,800
Intermediate nodes
are supervisors.

625
00:29:32,800 --> 00:29:35,550
And data servers always
are at the leaf nodes.

626
00:29:35,550 --> 00:29:38,150
So remember that.

627
00:29:38,150 --> 00:29:39,850
Now, this is a B tree.

628
00:29:39,850 --> 00:29:42,300
We can split it up
any way we want.

629
00:29:42,300 --> 00:29:44,910
And this works great for
basically doing cloud

630
00:29:44,910 --> 00:29:48,650
deployment because in fact
part of that three can be

631
00:29:48,650 --> 00:29:52,920
inside the GCE, another part
can be in a private cloud,

632
00:29:52,920 --> 00:29:55,990
another part in a private
cluster, and we can piece that

633
00:29:55,990 --> 00:30:00,070
all together to make it look
like one big cluster.

634
00:30:00,070 --> 00:30:02,140
Now, you look at that
and say great.

635
00:30:02,140 --> 00:30:04,360
But I have 16 million nodes.

636
00:30:04,360 --> 00:30:05,660
How do I get to the data?

637
00:30:05,660 --> 00:30:08,440
I can't keep track of
16 million things.

638
00:30:08,440 --> 00:30:13,580
Well, in fact, let's take
a look at how we do it.

639
00:30:13,580 --> 00:30:17,660
So when you've got a big cluster
like this, you really

640
00:30:17,660 --> 00:30:20,430
have to adopt a brand
new strategy.

641
00:30:20,430 --> 00:30:22,780
So we have a client.

642
00:30:22,780 --> 00:30:25,670
He gets an open or doesn't
open to the head node.

643
00:30:25,670 --> 00:30:29,440
And here we're going to assume
that the head node knows

644
00:30:29,440 --> 00:30:32,610
nothing about the file
the client needs.

645
00:30:32,610 --> 00:30:34,160
So what does it have to do?

646
00:30:34,160 --> 00:30:37,270
It has to find the route
to the file.

647
00:30:37,270 --> 00:30:39,430
And it'll accomplish that
by just doing a directed

648
00:30:39,430 --> 00:30:41,370
broadcast parallel query.

649
00:30:41,370 --> 00:30:44,970
And that'll set up the routing
tables in this tree.

650
00:30:44,970 --> 00:30:48,880
After that, the head node can
then redirect the client to

651
00:30:48,880 --> 00:30:50,340
the next subtree.

652
00:30:50,340 --> 00:30:52,250
That subtree in turns
redirects the

653
00:30:52,250 --> 00:30:54,970
client to the leaf node.

654
00:30:54,970 --> 00:30:58,280
Now, this is the only scalable
way of doing it because you

655
00:30:58,280 --> 00:30:59,940
don't have to keep track
of anything but

656
00:30:59,940 --> 00:31:01,160
the immediate route.

657
00:31:01,160 --> 00:31:04,820
Pretty much in how the
internet works.

658
00:31:04,820 --> 00:31:08,030
So the bottom line here, this
is a simple, flexible, and

659
00:31:08,030 --> 00:31:09,730
effective system.

660
00:31:09,730 --> 00:31:13,170
I want to say it's simple, but
the devil's in the details.

661
00:31:13,170 --> 00:31:16,890
We have a paper we presented
in IPDS that will give you

662
00:31:16,890 --> 00:31:20,760
some of the algorithms
we have to use.

663
00:31:20,760 --> 00:31:24,820
And you can actually get the
paper at XRootD.org.

664
00:31:24,820 --> 00:31:26,440
It's LGPL open-source.

665
00:31:26,440 --> 00:31:28,680
You can download it, run it.

666
00:31:28,680 --> 00:31:30,420
It's managed by the XRootD
collaboration.

667
00:31:30,420 --> 00:31:33,750
That collaboration is
open to new members.

668
00:31:33,750 --> 00:31:39,030
And I do encourage you
to go to XRootD.org.

669
00:31:39,030 --> 00:31:41,230
So now Sergey, finish it up.

670
00:31:41,230 --> 00:31:45,640

671
00:31:45,640 --> 00:31:48,205
DR. SERGEY PANITKIN: And
let me summarize.

672
00:31:48,205 --> 00:31:50,820

673
00:31:50,820 --> 00:31:52,840
All in all, we had great
experience with

674
00:31:52,840 --> 00:31:54,450
Google Compute Engine.

675
00:31:54,450 --> 00:31:57,180
We tested several computational
scenarios on

676
00:31:57,180 --> 00:32:01,960
that platform, PROOF, XRootD
clusters, PanDA batch

677
00:32:01,960 --> 00:32:06,990
clusters, and ran large scale
Monte Carlo production.

678
00:32:06,990 --> 00:32:10,180
We think that Google Compute
Engine is modern cloud

679
00:32:10,180 --> 00:32:13,480
infrastructure that can serve
as a stable high performance

680
00:32:13,480 --> 00:32:16,020
platform for scientific
computing.

681
00:32:16,020 --> 00:32:20,110
And tools developed by the LHC
community may be of some

682
00:32:20,110 --> 00:32:22,200
interest to the broader
community of developers

683
00:32:22,200 --> 00:32:24,530
working on Google Compute
Engine and

684
00:32:24,530 --> 00:32:27,780
other compute engine.

685
00:32:27,780 --> 00:32:30,122
And thank you very much.

686
00:32:30,122 --> 00:32:37,840
[APPLAUSE]

687
00:32:37,840 --> 00:32:39,940
GARRICK EVANS: Thanks a lot.

688
00:32:39,940 --> 00:32:43,080
So we're done, and we'd be happy
to take any questions

689
00:32:43,080 --> 00:32:45,470
that you guys have
at the time.

690
00:32:45,470 --> 00:32:46,870
Please come up to the mics.

691
00:32:46,870 --> 00:32:52,040

692
00:32:52,040 --> 00:32:53,930
No?

693
00:32:53,930 --> 00:32:54,950
OK, well, thanks.

694
00:32:54,950 --> 00:32:56,790
Hope you had a great I/O.

695
00:32:56,790 --> 00:33:03,517

