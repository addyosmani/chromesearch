1
00:00:00,000 --> 00:00:02,300

2
00:00:02,300 --> 00:00:03,610
DAVID SYMONDS: Thank you
very much for coming.

3
00:00:03,610 --> 00:00:04,940
How about we give our
gopher dancers a

4
00:00:04,940 --> 00:00:05,860
big round of applause.

5
00:00:05,860 --> 00:00:13,800
[APPLAUSE]

6
00:00:13,800 --> 00:00:15,834
DAVID SYMONDS: This is High
Performance Apps with Go on

7
00:00:15,834 --> 00:00:16,390
App Engine.

8
00:00:16,390 --> 00:00:17,920
Thank you very much
for coming.

9
00:00:17,920 --> 00:00:19,670
My name is David Symonds.

10
00:00:19,670 --> 00:00:22,920
I am an engineer on the Go team
at Google, and I lead the

11
00:00:22,920 --> 00:00:26,930
Technical Development for the
Go runtime for App Engine.

12
00:00:26,930 --> 00:00:28,290
As you might tell for
my accent, I'm

13
00:00:28,290 --> 00:00:29,050
not from around here.

14
00:00:29,050 --> 00:00:31,650
I'm from the Sydney office.

15
00:00:31,650 --> 00:00:33,580
So here's what we're going
to cover today.

16
00:00:33,580 --> 00:00:34,870
I'm just going to give
you a brief overview

17
00:00:34,870 --> 00:00:36,100
of Go on App Engine.

18
00:00:36,100 --> 00:00:38,770
Hopefully many of you here
have tried it out at one

19
00:00:38,770 --> 00:00:43,130
stage, just to give you a feel
for where we're at and give

20
00:00:43,130 --> 00:00:45,410
you a bit of an overview of
a couple of relatively

21
00:00:45,410 --> 00:00:48,110
high-profile apps that
are using it.

22
00:00:48,110 --> 00:00:50,490
Then I'm going to introduce a
motivating example for what

23
00:00:50,490 --> 00:00:53,520
we're going to use to explore
some performance techniques

24
00:00:53,520 --> 00:00:54,340
for your app.

25
00:00:54,340 --> 00:00:55,955
And then the rest of the
session, which will be most of

26
00:00:55,955 --> 00:00:58,550
the time, I'll be talking
about those performance

27
00:00:58,550 --> 00:01:01,400
techniques in some detail.

28
00:01:01,400 --> 00:01:02,720
So why Go on App engine?

29
00:01:02,720 --> 00:01:05,080
First of all, it's got the
peanut butter and the

30
00:01:05,080 --> 00:01:06,715
chocolate in a great
combination.

31
00:01:06,715 --> 00:01:08,900
Go compiles to native code.

32
00:01:08,900 --> 00:01:10,300
It runs really fast.

33
00:01:10,300 --> 00:01:13,020
It's a fantastic programming
environment to work in.

34
00:01:13,020 --> 00:01:15,820
At the same time, App Engine is
a fantastic platform as a

35
00:01:15,820 --> 00:01:19,190
service you can use to
host your web app.

36
00:01:19,190 --> 00:01:20,540
It auto scales.

37
00:01:20,540 --> 00:01:23,220
It's very, very low
maintenance.

38
00:01:23,220 --> 00:01:26,610
I don't like getting woken up
at 3:00 AM to fix things.

39
00:01:26,610 --> 00:01:29,190
I imagine neither do you.

40
00:01:29,190 --> 00:01:31,620
So if you're hosting on App
Engine, we have people at

41
00:01:31,620 --> 00:01:33,750
Google who will wake up at 3:00
AM to fix problems when

42
00:01:33,750 --> 00:01:35,690
they happen.

43
00:01:35,690 --> 00:01:38,080
Combining these two together, we
get the fastest runtime for

44
00:01:38,080 --> 00:01:39,060
App Engine.

45
00:01:39,060 --> 00:01:40,140
It starts really fast.

46
00:01:40,140 --> 00:01:42,102
It runs really fast.

47
00:01:42,102 --> 00:01:44,700
It keeps your instance
hours down low.

48
00:01:44,700 --> 00:01:47,730
It's cheap for what it does.

49
00:01:47,730 --> 00:01:52,060
And you've got code running at
full machine speed so you can

50
00:01:52,060 --> 00:01:54,950
do sophisticated, actual
processing, not

51
00:01:54,950 --> 00:01:57,450
just serving text.

52
00:01:57,450 --> 00:01:59,700
So we first unveiled the Go
runtime for App Engine here at

53
00:01:59,700 --> 00:02:01,660
Google I/O two years ago.

54
00:02:01,660 --> 00:02:03,870
And since then, we've seen
a lot of people adopt it.

55
00:02:03,870 --> 00:02:04,810
It's grown over time.

56
00:02:04,810 --> 00:02:07,100
It's supporting more of
the App Engine APIs.

57
00:02:07,100 --> 00:02:08,800
It's got better tooling.

58
00:02:08,800 --> 00:02:10,650
And we're seeing steady
growth and several

59
00:02:10,650 --> 00:02:11,940
high-profile users.

60
00:02:11,940 --> 00:02:14,860
Now, I wanted to talk about two
that Google has done to

61
00:02:14,860 --> 00:02:16,630
give you a feel for
what's possible.

62
00:02:16,630 --> 00:02:20,110
These are by no means hugely
sophisticated apps.

63
00:02:20,110 --> 00:02:22,700
But they're kind of
representative of things that

64
00:02:22,700 --> 00:02:25,230
are easy and natural to do in Go
that might be a little bit

65
00:02:25,230 --> 00:02:26,670
harder in other languages.

66
00:02:26,670 --> 00:02:30,570
But they really show off the
power and speed you can get.

67
00:02:30,570 --> 00:02:35,530
The first one is a front page of
google.com, a Turkey Doodle

68
00:02:35,530 --> 00:02:38,020
from Thanksgiving
two years ago.

69
00:02:38,020 --> 00:02:39,220
This is an interesting
one because it was

70
00:02:39,220 --> 00:02:40,540
written by a Go newcomer.

71
00:02:40,540 --> 00:02:43,540
He'd never written any Go code
before, but he picked it up,

72
00:02:43,540 --> 00:02:45,710
and he got this high-profile
app to

73
00:02:45,710 --> 00:02:47,740
launch in under 24 hours.

74
00:02:47,740 --> 00:02:51,020
So if you're thinking to
yourself, I love App Engine,

75
00:02:51,020 --> 00:02:53,710
but I'm a Python, or Java
programmer, or maybe a PHP

76
00:02:53,710 --> 00:02:55,290
programmer now.

77
00:02:55,290 --> 00:02:56,460
What's Go got in for me?

78
00:02:56,460 --> 00:02:57,840
Maybe it's too hard to learn.

79
00:02:57,840 --> 00:03:02,040
This is great evidence that
you can get from 0 to 100

80
00:03:02,040 --> 00:03:05,090
miles per hour with not
too much difficulty.

81
00:03:05,090 --> 00:03:07,920
Go's really easy to pick up.

82
00:03:07,920 --> 00:03:10,930
When this person built it, this
app is doing some image

83
00:03:10,930 --> 00:03:12,000
compositing.

84
00:03:12,000 --> 00:03:16,450
So you can see it's rendering
a graphic of a turkey with

85
00:03:16,450 --> 00:03:17,330
different feathers.

86
00:03:17,330 --> 00:03:21,180
You can see it animated from
that link down below.

87
00:03:21,180 --> 00:03:22,760
But it's doing image
work here.

88
00:03:22,760 --> 00:03:24,650
It's not just serving
static images.

89
00:03:24,650 --> 00:03:26,600
It's doing sophisticated
stuff.

90
00:03:26,600 --> 00:03:29,020
And this Go newcomer found that
he could build this app

91
00:03:29,020 --> 00:03:32,550
relatively easily and get half
the latency of an equivalent

92
00:03:32,550 --> 00:03:34,860
Python 2.7 version.

93
00:03:34,860 --> 00:03:37,880
As you can imagine, watching it
on google.com, the homepage

94
00:03:37,880 --> 00:03:40,140
produces a fair bit of
traffic, and this

95
00:03:40,140 --> 00:03:41,420
didn't miss a beat.

96
00:03:41,420 --> 00:03:42,650
And there's some more details
of how well it

97
00:03:42,650 --> 00:03:46,320
performed at that link.

98
00:03:46,320 --> 00:03:49,030
Santa Tracker for last year
was written in Go.

99
00:03:49,030 --> 00:03:52,130
And this was the component that
kept track of where Santa

100
00:03:52,130 --> 00:03:56,080
was and informed the client-side
JavaScript where

101
00:03:56,080 --> 00:03:57,580
to render Santa and how
many presents he had

102
00:03:57,580 --> 00:03:59,865
delivered, and so forth.

103
00:03:59,865 --> 00:04:03,190
It served a peak of nearly 5,000
QPS, so 5,000 queries

104
00:04:03,190 --> 00:04:04,230
per second.

105
00:04:04,230 --> 00:04:06,400
This is also a very
high-pressure situation.

106
00:04:06,400 --> 00:04:08,645
If things aren't going right, he
can't just tell people come

107
00:04:08,645 --> 00:04:11,115
back tomorrow for something
like this.

108
00:04:11,115 --> 00:04:12,970
It's too late by then.

109
00:04:12,970 --> 00:04:14,910
We didn't make any
children cry.

110
00:04:14,910 --> 00:04:17,765
And you can see down at the
bottom, we did this with under

111
00:04:17,765 --> 00:04:19,170
80 instances.

112
00:04:19,170 --> 00:04:21,390
So this is a lot fewer than you
might expect for this kind

113
00:04:21,390 --> 00:04:22,640
of query volume.

114
00:04:22,640 --> 00:04:24,970

115
00:04:24,970 --> 00:04:28,500
So let me introduce to
you Gopher Mart.

116
00:04:28,500 --> 00:04:31,310
Gopher Mart is a little sample
app, and I'll have a link at

117
00:04:31,310 --> 00:04:36,750
the end of this presentation
for source code for it.

118
00:04:36,750 --> 00:04:38,940
It's a bit of a silly app.

119
00:04:38,940 --> 00:04:41,000
But it kind of demonstrates
enough of the features for us

120
00:04:41,000 --> 00:04:43,620
to discuss building a really
high-performance app.

121
00:04:43,620 --> 00:04:47,230

122
00:04:47,230 --> 00:04:49,450
So here's the situation.

123
00:04:49,450 --> 00:04:53,805
Imagine you are running an app
to manage or to run, indeed,

124
00:04:53,805 --> 00:04:56,010
your Gopher Mart, which is your
one-stop shop for all

125
00:04:56,010 --> 00:04:57,700
your gopher needs.

126
00:04:57,700 --> 00:04:59,680
You gophers can come in, get
their gopher bran, their

127
00:04:59,680 --> 00:05:02,990
gopher flakes, their gopher
liter, load them up in a cart,

128
00:05:02,990 --> 00:05:08,850
take them to a checkout, and
then take them home obviously.

129
00:05:08,850 --> 00:05:12,450
And the correspondence to an app
that we're going to use is

130
00:05:12,450 --> 00:05:15,030
that the gophers that are doing
the shopping are your

131
00:05:15,030 --> 00:05:17,210
user requests.

132
00:05:17,210 --> 00:05:18,340
Each of your checkout
gophers--

133
00:05:18,340 --> 00:05:21,050
the gophers processing each
of those requests--

134
00:05:21,050 --> 00:05:22,320
is an app instance.

135
00:05:22,320 --> 00:05:25,000
And then there's the scheduler
gopher that's got a direct

136
00:05:25,000 --> 00:05:28,316
user request to app instances.

137
00:05:28,316 --> 00:05:30,410
A checkout gopher can
only deal with one

138
00:05:30,410 --> 00:05:33,340
request at a time.

139
00:05:33,340 --> 00:05:37,180
And checkout gophers can be
hired or fired in our Gopher

140
00:05:37,180 --> 00:05:39,250
Mart, but there is a certain
overhead to this.

141
00:05:39,250 --> 00:05:43,460
This models how we deal with
App Engine instances.

142
00:05:43,460 --> 00:05:45,110
They take a little bit
of time to start up.

143
00:05:45,110 --> 00:05:47,070
They're very quick to start up
and go, but it's still a

144
00:05:47,070 --> 00:05:48,500
nonzero amount of time.

145
00:05:48,500 --> 00:05:50,410
And they can't be fired
straight away.

146
00:05:50,410 --> 00:05:54,260
Our Gopher Mart has serious
industrial relations laws.

147
00:05:54,260 --> 00:05:57,000
You need to keep your checkout
gophers around for at least 15

148
00:05:57,000 --> 00:05:58,780
minutes after they've
been hired.

149
00:05:58,780 --> 00:06:01,340

150
00:06:01,340 --> 00:06:05,150
But as in our Gopher Mart, our
gophers don't want to be

151
00:06:05,150 --> 00:06:06,660
standing in queue with
their gopher bran.

152
00:06:06,660 --> 00:06:08,640
They want to get home and eat
their gopher bran-- pour

153
00:06:08,640 --> 00:06:11,250
gopher milk all over it, scoop
it out of their gopher bowls.

154
00:06:11,250 --> 00:06:12,560
They don't want to be standing
in queues waiting

155
00:06:12,560 --> 00:06:13,490
for all this to happen.

156
00:06:13,490 --> 00:06:15,640
They'll get kind of grumpy
if we take too long.

157
00:06:15,640 --> 00:06:17,770
So we're going to look at
different ways of making our

158
00:06:17,770 --> 00:06:19,590
app really fly.

159
00:06:19,590 --> 00:06:20,840
I'm going to be looking at five
different techniques.

160
00:06:20,840 --> 00:06:23,410

161
00:06:23,410 --> 00:06:25,180
But before we look at any
techniques, we have to

162
00:06:25,180 --> 00:06:27,680
understand the way
that we approach

163
00:06:27,680 --> 00:06:29,280
high-performance apps.

164
00:06:29,280 --> 00:06:31,240
And it's very easy to fool
ourselves into thinking that

165
00:06:31,240 --> 00:06:34,500
we know where the
slow bits are.

166
00:06:34,500 --> 00:06:35,880
Whenever you're doing
performance work, you need to

167
00:06:35,880 --> 00:06:38,400
first measure to understand
where the slow bits are.

168
00:06:38,400 --> 00:06:41,910
There's no point of spending
months of your time tuning,

169
00:06:41,910 --> 00:06:46,070
and tweaking, and caching, and
doing all kinds of things to

170
00:06:46,070 --> 00:06:48,750
one bit of code if that bit of
code isn't your slow bit.

171
00:06:48,750 --> 00:06:51,890
So you need to measure first.

172
00:06:51,890 --> 00:06:54,310
It's also important to
measure periodically.

173
00:06:54,310 --> 00:06:57,390
Your app will behave differently
based on different

174
00:06:57,390 --> 00:07:00,120
request loads, different types
of request processing, what

175
00:07:00,120 --> 00:07:01,960
you're trying to do
in each request.

176
00:07:01,960 --> 00:07:06,160
So it's important to come back
to measuring how fast it is

177
00:07:06,160 --> 00:07:08,900
over time because it will change
as you change your app.

178
00:07:08,900 --> 00:07:10,960
Even if you don't change your
app, the infrastructure

179
00:07:10,960 --> 00:07:13,760
underneath will change over time
as well and almost always

180
00:07:13,760 --> 00:07:15,010
for the better.

181
00:07:15,010 --> 00:07:17,340
So App Engine gets better
at scheduling

182
00:07:17,340 --> 00:07:19,070
instances to app servers.

183
00:07:19,070 --> 00:07:21,610
It'll get better at getting
files to the app servers,

184
00:07:21,610 --> 00:07:23,920
starting them up quickly,
and so on.

185
00:07:23,920 --> 00:07:26,240
Go got a lot faster.

186
00:07:26,240 --> 00:07:29,030
Just a few days ago when we
announced Go 1.1, that was

187
00:07:29,030 --> 00:07:33,650
major milestone, our second
major stable release, that

188
00:07:33,650 --> 00:07:36,030
brought a lot of performance
improvements.

189
00:07:36,030 --> 00:07:40,170
We got emails to the mailing
list about people seeing their

190
00:07:40,170 --> 00:07:42,570
programs written in Go dropping
from like 24 hours

191
00:07:42,570 --> 00:07:44,640
runtime to 9 hours runtime.

192
00:07:44,640 --> 00:07:47,210
So there's startling differences
in every release

193
00:07:47,210 --> 00:07:49,780
of things, so you need to
periodically come back to

194
00:07:49,780 --> 00:07:51,620
remeasure your app to understand
what bits are slow

195
00:07:51,620 --> 00:07:52,870
and what bits aren't.

196
00:07:52,870 --> 00:07:56,250

197
00:07:56,250 --> 00:08:00,420
The tool that we're going to be
using to get a feel for how

198
00:08:00,420 --> 00:08:03,100
our app performs is
called Appstats.

199
00:08:03,100 --> 00:08:06,560
And what Appstats does is it
traces API RPCs over the

200
00:08:06,560 --> 00:08:07,580
course of requests.

201
00:08:07,580 --> 00:08:10,960
It measures the start time and
the stop time, what these

202
00:08:10,960 --> 00:08:13,040
requests are doing, and also
records the stack trace so you

203
00:08:13,040 --> 00:08:16,570
can dig in through a web
interface to be able to see

204
00:08:16,570 --> 00:08:20,130
where this RPC came from, how
long it took, any errors that

205
00:08:20,130 --> 00:08:22,040
happened, and so on.

206
00:08:22,040 --> 00:08:25,730
And as you can see here, here's
a -- timeline, with

207
00:08:25,730 --> 00:08:27,430
time running along
the x-axis--

208
00:08:27,430 --> 00:08:29,910
of my first version
of this app.

209
00:08:29,910 --> 00:08:33,480
Now, I wrote it as simply as
possible with no thoughts to

210
00:08:33,480 --> 00:08:35,340
performance, just simplicity.

211
00:08:35,340 --> 00:08:36,480
And you can see it's
pretty slow.

212
00:08:36,480 --> 00:08:39,210
It's taking nearly
400 milliseconds.

213
00:08:39,210 --> 00:08:41,530
And what it's doing
is, the gopher

214
00:08:41,530 --> 00:08:43,750
comes up to the checkout.

215
00:08:43,750 --> 00:08:48,150
The checkout gopher has to scan
each item that was in

216
00:08:48,150 --> 00:08:50,720
their cart, and then emails
them a receipt afterwards.

217
00:08:50,720 --> 00:08:53,515
This is a fairly modern
Gopher Mart.

218
00:08:53,515 --> 00:08:55,750
So instead of just giving them
a printed receipt, they'll

219
00:08:55,750 --> 00:08:59,440
just get it in their email
later on that day.

220
00:08:59,440 --> 00:09:00,570
So we're going to look
at different ways of

221
00:09:00,570 --> 00:09:03,170
speeding this up.

222
00:09:03,170 --> 00:09:04,190
So I'm going to be
covering five

223
00:09:04,190 --> 00:09:05,780
performance techniques today.

224
00:09:05,780 --> 00:09:07,290
These aren't comprehensive.

225
00:09:07,290 --> 00:09:10,120
There's plenty more that you
can think of, and each one,

226
00:09:10,120 --> 00:09:13,940
I'll be only giving a bit of a
flavor for the depth that you

227
00:09:13,940 --> 00:09:17,270
can go into to fully realize
these techniques.

228
00:09:17,270 --> 00:09:19,090
But these should be five
fairly representative

229
00:09:19,090 --> 00:09:20,972
techniques of how to
tackle these kind

230
00:09:20,972 --> 00:09:26,610
of performance problems.

231
00:09:26,610 --> 00:09:30,080
First one to do is
deferring work.

232
00:09:30,080 --> 00:09:32,790
Not all work needs to be done
during the request.

233
00:09:32,790 --> 00:09:36,270
In our Gopher Mart situation
when the gophers come up to

234
00:09:36,270 --> 00:09:38,555
the checkout and are checking
out their items, they don't

235
00:09:38,555 --> 00:09:42,416
need the email of the receipt
right then and there.

236
00:09:42,416 --> 00:09:44,790
They're going to take their cart
out to their gopher car,

237
00:09:44,790 --> 00:09:47,030
load up the gopher boot with
their gopher bran, drive to

238
00:09:47,030 --> 00:09:47,820
the gopher home.

239
00:09:47,820 --> 00:09:49,055
It's going to take them
a little while.

240
00:09:49,055 --> 00:09:52,250
So the importance of getting the
mail out to them straight

241
00:09:52,250 --> 00:09:54,800
away is not so important.

242
00:09:54,800 --> 00:09:57,900
And in general, a lot of web
apps need to do things as a

243
00:09:57,900 --> 00:10:02,390
consequence of a request, but
don't need to do them during

244
00:10:02,390 --> 00:10:03,720
the request.

245
00:10:03,720 --> 00:10:07,010
So this may seem like a pretty
simplistic technique, and

246
00:10:07,010 --> 00:10:10,110
we'll get on to slightly more
sophisticated ones shortly.

247
00:10:10,110 --> 00:10:11,895
But it's an important
one to remember.

248
00:10:11,895 --> 00:10:13,370
Lots of the time, you can
forget that you're doing

249
00:10:13,370 --> 00:10:16,030
things that you don't
need to be doing.

250
00:10:16,030 --> 00:10:18,220
The main ways that we're going
to going do this with the Go

251
00:10:18,220 --> 00:10:22,340
App Engine API is with the Task
Queue API or the delay

252
00:10:22,340 --> 00:10:25,150
package that is built over
the Task Queue API.

253
00:10:25,150 --> 00:10:28,210
And we're going to use this to
shift work that it needs to do

254
00:10:28,210 --> 00:10:29,790
outside the request scope.

255
00:10:29,790 --> 00:10:32,870
In our Gopher Mart situation,
we're going to shift that slow

256
00:10:32,870 --> 00:10:36,720
mail lots end call that took a
large chunk of our timeline.

257
00:10:36,720 --> 00:10:39,920
And we're going to do that in a
task that's going to happen

258
00:10:39,920 --> 00:10:41,750
usually very soon afterwards.

259
00:10:41,750 --> 00:10:43,840
But we're going to replace
it with a very quick

260
00:10:43,840 --> 00:10:44,890
taskqueue.add.

261
00:10:44,890 --> 00:10:47,340
And we're going to use
the delay package.

262
00:10:47,340 --> 00:10:49,250
And here's how the delay package
works in case you

263
00:10:49,250 --> 00:10:50,230
haven't seen it before.

264
00:10:50,230 --> 00:10:53,810
What it does is it
wraps a function.

265
00:10:53,810 --> 00:10:56,170
You just go to try and
invoke that function.

266
00:10:56,170 --> 00:10:59,740
And what it'll do behind the
scenes is it will marshall the

267
00:10:59,740 --> 00:11:04,350
function arguments, store them
in a Task Queue task, add it

268
00:11:04,350 --> 00:11:07,100
to the queue, and then when it
comes back, it'll invoke your

269
00:11:07,100 --> 00:11:08,480
function for you.

270
00:11:08,480 --> 00:11:12,080
So it's a very, very simple way
of dealing with Task Queue

271
00:11:12,080 --> 00:11:14,760
when you're doing it
to defer work.

272
00:11:14,760 --> 00:11:19,750
So you can see at the top, two
lines of my code for my simple

273
00:11:19,750 --> 00:11:22,660
version there's a sendReceipt
function invocation.

274
00:11:22,660 --> 00:11:24,520
And then you can see just the
function signature of

275
00:11:24,520 --> 00:11:25,760
sendReceipt.

276
00:11:25,760 --> 00:11:27,440
It's fairly simple.

277
00:11:27,440 --> 00:11:30,740
All we have to do to use the
delay package is import

278
00:11:30,740 --> 00:11:31,990
appengine/delay.

279
00:11:31,990 --> 00:11:33,420

280
00:11:33,420 --> 00:11:36,170
And then we turn our sendReceipt
invocation to

281
00:11:36,170 --> 00:11:37,750
stick a dot Call in there.

282
00:11:37,750 --> 00:11:40,090
Nothing else has to
change there.

283
00:11:40,090 --> 00:11:43,430
Then we turn our function
definition into a variable

284
00:11:43,430 --> 00:11:45,040
definition.

285
00:11:45,040 --> 00:11:49,040
And we just use delay.func, give
it a name, and then we

286
00:11:49,040 --> 00:11:51,340
have our function body
exactly as is.

287
00:11:51,340 --> 00:11:53,840
So there's no difference from
what we were doing before.

288
00:11:53,840 --> 00:11:55,170
And you can see the effect
that it has.

289
00:11:55,170 --> 00:11:57,140
It shrunk that mail.send call.

290
00:11:57,140 --> 00:11:59,820
It be a bit slow, and it might
be waiting on the mail

291
00:11:59,820 --> 00:12:02,910
protocol to start so it can
verify that it can send mail,

292
00:12:02,910 --> 00:12:05,270
spam filters, whatever it is.

293
00:12:05,270 --> 00:12:07,810
And it's turned into a very
quick Task Queue call.

294
00:12:07,810 --> 00:12:10,470
So this is a lot quicker, and
then the mail will happen in

295
00:12:10,470 --> 00:12:12,100
its own Task Queue request.

296
00:12:12,100 --> 00:12:14,890

297
00:12:14,890 --> 00:12:18,370
Second technique I want to
talk about is batching.

298
00:12:18,370 --> 00:12:22,380
In our Gopher Mart scenario,
it kind of makes sense that

299
00:12:22,380 --> 00:12:24,480
you don't want a family of
10 gophers all going in

300
00:12:24,480 --> 00:12:27,210
individually, picking up their
own box of gopher flakes,

301
00:12:27,210 --> 00:12:29,700
walking up to the counter, each
going through the process

302
00:12:29,700 --> 00:12:31,930
of checking them out, paying
for them, and then walking

303
00:12:31,930 --> 00:12:32,670
home with them.

304
00:12:32,670 --> 00:12:36,450
It makes a lot more sense for
one of the gophers to go in by

305
00:12:36,450 --> 00:12:39,910
themselves, load up their cart
with 10 boxes of gopher

306
00:12:39,910 --> 00:12:42,800
flakes, and then check
them out all at once.

307
00:12:42,800 --> 00:12:44,400
And again, this is a relatively
simple technique,

308
00:12:44,400 --> 00:12:45,650
but it's often overlooked.

309
00:12:45,650 --> 00:12:48,610

310
00:12:48,610 --> 00:12:51,440
This would be using API calls
like GetMulti instead of Get,

311
00:12:51,440 --> 00:12:54,910
or PutMulti instead of Put.

312
00:12:54,910 --> 00:12:56,800
Often they're a little bit
more overhead than just a

313
00:12:56,800 --> 00:12:58,720
single call if you've only
got a single object.

314
00:12:58,720 --> 00:13:01,530
But a lot time, you're dealing
with multiple entities.

315
00:13:01,530 --> 00:13:05,540
And so they can be a lot quicker
because you only have

316
00:13:05,540 --> 00:13:09,370
one lot of the RPC overhead
instead of in.

317
00:13:09,370 --> 00:13:11,490
So here you can see the
code that I wrote

318
00:13:11,490 --> 00:13:13,352
for the simple version.

319
00:13:13,352 --> 00:13:15,640
You can see that it's just
sitting in a loop.

320
00:13:15,640 --> 00:13:19,350
And for each key it goes and
fetches the item corresponding

321
00:13:19,350 --> 00:13:21,066
to that key from
the data store.

322
00:13:21,066 --> 00:13:23,840
It deals with any error, and
then just builds up a slice of

323
00:13:23,840 --> 00:13:25,160
those items.

324
00:13:25,160 --> 00:13:27,140
That's obviously very simple
to understand and to reason

325
00:13:27,140 --> 00:13:30,760
about, but it's slow.

326
00:13:30,760 --> 00:13:32,740
And here's what we do
when we batch it.

327
00:13:32,740 --> 00:13:35,620

328
00:13:35,620 --> 00:13:37,760
Instead of a loop where we're
doing each data store call, we

329
00:13:37,760 --> 00:13:40,040
use datastore.GetMulti.

330
00:13:40,040 --> 00:13:42,630
We tell it all the keys to go
and get, and then it returns

331
00:13:42,630 --> 00:13:44,600
all the items for us.

332
00:13:44,600 --> 00:13:48,140
And you can see, in this case,
the task you call was very

333
00:13:48,140 --> 00:13:49,380
luckily short.

334
00:13:49,380 --> 00:13:52,790
But more importantly the
datastore.Get call turned from

335
00:13:52,790 --> 00:13:58,170
five sequential calls into
just one short one.

336
00:13:58,170 --> 00:14:01,560
So we've already cut down our
program's request processing

337
00:14:01,560 --> 00:14:02,810
time dramatically.

338
00:14:02,810 --> 00:14:05,330

339
00:14:05,330 --> 00:14:08,380
Third technique I want to
talk about is caching.

340
00:14:08,380 --> 00:14:11,170
And I'm sure many of you have
cached things in the past.

341
00:14:11,170 --> 00:14:15,840
And in our Gopher Mart scenario,
this would be like a

342
00:14:15,840 --> 00:14:19,280
checkout gopher remembering
that for a certain fresh

343
00:14:19,280 --> 00:14:21,170
fruit-- which might not have bar
codes, and they normally

344
00:14:21,170 --> 00:14:22,830
have to look up a
booklet for--

345
00:14:22,830 --> 00:14:26,380
they just remember gopher
berries, 361.

346
00:14:26,380 --> 00:14:28,620
Gopher bananas, 492.

347
00:14:28,620 --> 00:14:30,700
They can just remember these
things once they've been

348
00:14:30,700 --> 00:14:34,880
running for a while, and they
can be a lot quicker.

349
00:14:34,880 --> 00:14:36,290
But the first layer
that I want to

350
00:14:36,290 --> 00:14:37,150
talk about is Datastore.

351
00:14:37,150 --> 00:14:40,120
And Datastore is a great
storage system.

352
00:14:40,120 --> 00:14:40,910
It's very fast.

353
00:14:40,910 --> 00:14:41,560
It's very reliable.

354
00:14:41,560 --> 00:14:43,080
It's highly scalable.

355
00:14:43,080 --> 00:14:45,180
But it also can be
used as a cache.

356
00:14:45,180 --> 00:14:48,380
So your program might be doing
expensive computation work.

357
00:14:48,380 --> 00:14:50,090
It might be compositing
images.

358
00:14:50,090 --> 00:14:55,880
It might be fetching a request
via URL fetch from a remote

359
00:14:55,880 --> 00:14:57,200
server that's slow.

360
00:14:57,200 --> 00:14:59,565
It could be doing any number of
things outside itself that

361
00:14:59,565 --> 00:15:02,840
it has very little control over
that can be quite slow.

362
00:15:02,840 --> 00:15:05,055
We can stick those in the
Datastore, and then all our

363
00:15:05,055 --> 00:15:07,370
app instances can check the
Datastore before trying to do

364
00:15:07,370 --> 00:15:11,680
that expensive computation
or remote fetch.

365
00:15:11,680 --> 00:15:15,560
One thing even faster than
Datastore is Memcache.

366
00:15:15,560 --> 00:15:19,350
And as opposed to Datastore
where a small Get might take

367
00:15:19,350 --> 00:15:20,100
20 milliseconds--

368
00:15:20,100 --> 00:15:21,720
thereabouts--

369
00:15:21,720 --> 00:15:23,750
Memcache can do it in
about 1 millisecond.

370
00:15:23,750 --> 00:15:28,020
So it can often be 20 times
faster for small workloads.

371
00:15:28,020 --> 00:15:28,710
That's great.

372
00:15:28,710 --> 00:15:29,980
It's shared across all
your instances.

373
00:15:29,980 --> 00:15:34,040
You know, one of your checkout
gophers needs to look up the

374
00:15:34,040 --> 00:15:35,095
code for gopher berries.

375
00:15:35,095 --> 00:15:37,910
It stores it in your Memcache,
and every other checkout

376
00:15:37,910 --> 00:15:41,620
gopher suddenly gets the benefit
of that caching.

377
00:15:41,620 --> 00:15:43,750
You need to remember that,
unlike Datastore, Memcache is

378
00:15:43,750 --> 00:15:45,500
not guaranteed to hang around.

379
00:15:45,500 --> 00:15:49,460
It's not a persistent storage
system so it could get flushed

380
00:15:49,460 --> 00:15:54,240
any time your app has been using
Memcache too much or

381
00:15:54,240 --> 00:15:56,950
your App Engine infrastructure
moves your app to

382
00:15:56,950 --> 00:15:57,720
another data center.

383
00:15:57,720 --> 00:15:59,030
So you can't depend
on it being there.

384
00:15:59,030 --> 00:16:04,110
But it's a great way of keeping
hot used data at

385
00:16:04,110 --> 00:16:05,770
immediate access.

386
00:16:05,770 --> 00:16:09,400
Third thing to use, and
something that might be quite

387
00:16:09,400 --> 00:16:11,170
foreign to particularly
Python programmers,

388
00:16:11,170 --> 00:16:13,440
is using local memory.

389
00:16:13,440 --> 00:16:15,510
Your app is running as a program
on a real machine.

390
00:16:15,510 --> 00:16:18,390
And we can easily forget about
that with our abstract notions

391
00:16:18,390 --> 00:16:22,910
of the App Engine set up, and
instances, and requests, and

392
00:16:22,910 --> 00:16:24,260
it all feels very abstract.

393
00:16:24,260 --> 00:16:27,490
And we forget that they're
programs running on a machine

394
00:16:27,490 --> 00:16:29,930
and that machine has memory.

395
00:16:29,930 --> 00:16:32,300
So we can use local memory
in your app instance.

396
00:16:32,300 --> 00:16:34,270
It would just be sticking things
in a global variable,

397
00:16:34,270 --> 00:16:35,730
for example.

398
00:16:35,730 --> 00:16:36,710
And that's even faster.

399
00:16:36,710 --> 00:16:38,850
That's three orders of magnitude
faster than Memcache

400
00:16:38,850 --> 00:16:42,400
will usually be for
small amounts.

401
00:16:42,400 --> 00:16:44,810
It's even more fragile than
Memcache, though.

402
00:16:44,810 --> 00:16:46,610
It's not shared across
instances.

403
00:16:46,610 --> 00:16:49,610
So if one instance gets shut
down because it hasn't been

404
00:16:49,610 --> 00:16:52,270
receiving enough requests or
it's not needed, that memory

405
00:16:52,270 --> 00:16:53,802
will disappear.

406
00:16:53,802 --> 00:16:56,470
But it's an important tool in
your tool belt when your

407
00:16:56,470 --> 00:16:58,970
caching things to use
local memory.

408
00:16:58,970 --> 00:17:03,850
It can be heaps faster than
Memcache, heaps more reliable,

409
00:17:03,850 --> 00:17:05,329
with the proviso that
it will disappear.

410
00:17:05,329 --> 00:17:09,009

411
00:17:09,009 --> 00:17:10,899
The third technique I want to
talk about it concurrency.

412
00:17:10,899 --> 00:17:13,190
And this is where Go
really shines.

413
00:17:13,190 --> 00:17:17,470
And this is where it
really excels as

414
00:17:17,470 --> 00:17:19,270
compared to other runtimes.

415
00:17:19,270 --> 00:17:22,890
Concurrency is where you can
decompose what your app is

416
00:17:22,890 --> 00:17:26,579
doing into independently
executing things.

417
00:17:26,579 --> 00:17:30,370
So even though Go on App Engine
is, at the moment,

418
00:17:30,370 --> 00:17:34,630
bound to a single CPU thread,
concurrency allows you to

419
00:17:34,630 --> 00:17:37,640
decompose your program so that
you can write very simple

420
00:17:37,640 --> 00:17:41,550
chunks of code that are
run concurrently.

421
00:17:41,550 --> 00:17:42,640
They might not run
in parallel.

422
00:17:42,640 --> 00:17:45,250
But while one is blocked on an
API request, another one will

423
00:17:45,250 --> 00:17:47,310
immediately start running.

424
00:17:47,310 --> 00:17:49,990
And in our Gopher Mart scenario,
this would be as if

425
00:17:49,990 --> 00:17:53,250
a checkout gopher needs to get
a price check on an item and

426
00:17:53,250 --> 00:17:56,420
then can immediately start
scanning the other items.

427
00:17:56,420 --> 00:17:58,260
If you walk into a regular
supermarket, you don't see

428
00:17:58,260 --> 00:18:00,380
somebody waiting for a price
check, and then just standing

429
00:18:00,380 --> 00:18:03,480
there waiting, getting the
price check back and then

430
00:18:03,480 --> 00:18:04,180
doing the other items.

431
00:18:04,180 --> 00:18:05,850
They go on and do more things.

432
00:18:05,850 --> 00:18:07,140
And so that's concurrent work.

433
00:18:07,140 --> 00:18:10,190

434
00:18:10,190 --> 00:18:13,160
So let me take a bit of our code
from our example app that

435
00:18:13,160 --> 00:18:17,660
was poorly written by me, and
let's make it better and

436
00:18:17,660 --> 00:18:19,620
faster by using concurrency.

437
00:18:19,620 --> 00:18:22,490
So here might be a little chunk
of code that needs to

438
00:18:22,490 --> 00:18:25,880
look up all the entities
of two Datastore kinds.

439
00:18:25,880 --> 00:18:28,990
In this case, we have a list and
an item kind, and we want

440
00:18:28,990 --> 00:18:32,570
to get all the items of each.

441
00:18:32,570 --> 00:18:35,330
This is a straight line code,
and it's very slow.

442
00:18:35,330 --> 00:18:38,380
If each query takes a certain
amount of time, the total time

443
00:18:38,380 --> 00:18:39,750
to run this bit of code
is going to be

444
00:18:39,750 --> 00:18:41,130
the sum of the times.

445
00:18:41,130 --> 00:18:43,560
It'd be much, much quicker if we
could run them in parallel.

446
00:18:43,560 --> 00:18:47,140

447
00:18:47,140 --> 00:18:51,690
And since most of your web apps,
most of all web apps,

448
00:18:51,690 --> 00:18:54,790
tend to be bound waiting on API
calls to come back from

449
00:18:54,790 --> 00:18:57,650
various other services, this
is an easy way to speed up

450
00:18:57,650 --> 00:19:00,210
most requests by doing
things concurrently.

451
00:19:00,210 --> 00:19:03,290
So here all we've had to do is
kick off two Go routines to

452
00:19:03,290 --> 00:19:07,930
run these queries, and they'll
be independently waiting.

453
00:19:07,930 --> 00:19:10,370
Then we're using an error
channel to signal

454
00:19:10,370 --> 00:19:11,100
when they're done.

455
00:19:11,100 --> 00:19:13,950
Hopefully it'll send back
nil if there's no error.

456
00:19:13,950 --> 00:19:15,360
And then, down at the bottom,
we just wait for

457
00:19:15,360 --> 00:19:18,230
both of them to return.

458
00:19:18,230 --> 00:19:22,140
This means that instead of two
requests summing together, we

459
00:19:22,140 --> 00:19:26,130
now end up taking time roughly
equivalent of the maximum

460
00:19:26,130 --> 00:19:27,800
length of either of them.

461
00:19:27,800 --> 00:19:31,650
So that's going to be strictly
less than the sum in any

462
00:19:31,650 --> 00:19:32,800
reasonable scenario.

463
00:19:32,800 --> 00:19:36,460
So this is a very simple flavor
of concurrency in Go.

464
00:19:36,460 --> 00:19:38,360
This is far from
sophisticated.

465
00:19:38,360 --> 00:19:41,140
So I'll commend you, my
colleague, Sameer Ajmani, and

466
00:19:41,140 --> 00:19:43,710
his talk this afternoon where
he'll be talking about some

467
00:19:43,710 --> 00:19:45,910
advanced Go concurrency
patterns.

468
00:19:45,910 --> 00:19:48,465
So if this kind of thing sounds
interesting to you, and

469
00:19:48,465 --> 00:19:50,600
it does to me, get along
to that talk.

470
00:19:50,600 --> 00:19:58,940

471
00:19:58,940 --> 00:20:00,320
The next technique I
want to talk about

472
00:20:00,320 --> 00:20:02,980
is controlling variance.

473
00:20:02,980 --> 00:20:06,150
And in our Gopher Mart,
we have a 10

474
00:20:06,150 --> 00:20:07,930
items or fewer queue.

475
00:20:07,930 --> 00:20:10,810
But occasionally a gopher comes
by when they've got a

476
00:20:10,810 --> 00:20:13,660
few more than 10 items, if
you know what I mean.

477
00:20:13,660 --> 00:20:17,740
And while most gophers might
take a millisecond to process,

478
00:20:17,740 --> 00:20:20,000
one in 100 might take
10 times that.

479
00:20:20,000 --> 00:20:22,560
It might take 100 milliseconds,
for example.

480
00:20:22,560 --> 00:20:24,990
And this is a very
common thing.

481
00:20:24,990 --> 00:20:26,980
It could be due to
the request.

482
00:20:26,980 --> 00:20:29,330
So in our Gopher Mart situation,
it might just be

483
00:20:29,330 --> 00:20:31,920
the occasional gopher with a
very, very big cart full of

484
00:20:31,920 --> 00:20:36,650
items, whereas most carts
only have a few items.

485
00:20:36,650 --> 00:20:38,460
But it can also be due
to the infrastructure

486
00:20:38,460 --> 00:20:39,710
underneath your app.

487
00:20:39,710 --> 00:20:42,310

488
00:20:42,310 --> 00:20:44,950
And the App Engine
infrastructure is great.

489
00:20:44,950 --> 00:20:46,080
It's highly scalable.

490
00:20:46,080 --> 00:20:46,590
It's fast.

491
00:20:46,590 --> 00:20:47,615
It's dependable.

492
00:20:47,615 --> 00:20:48,770
It's very reliable.

493
00:20:48,770 --> 00:20:50,820
But it's not perfect.

494
00:20:50,820 --> 00:20:52,465
Occasionally you'll be doing
Datastore requests, and

495
00:20:52,465 --> 00:20:54,820
they'll be going 20
milliseconds, 25 milliseconds,

496
00:20:54,820 --> 00:20:57,800
22 milliseconds, be really
reliably quick.

497
00:20:57,800 --> 00:21:02,260
But then a small fraction of it
might take half a second.

498
00:21:02,260 --> 00:21:08,490

499
00:21:08,490 --> 00:21:12,280
And the consequences of this
kind of variance can cascade

500
00:21:12,280 --> 00:21:15,850
through our system if
it's sophisticated.

501
00:21:15,850 --> 00:21:18,670
If your app needs to do several
things in a sequence,

502
00:21:18,670 --> 00:21:20,830
where each one depends on the
results of the previous one

503
00:21:20,830 --> 00:21:24,050
such that these things can't be
done concurrently, then a

504
00:21:24,050 --> 00:21:26,670
variance in one affects
the whole pipeline.

505
00:21:26,670 --> 00:21:30,660
And the perfect storm of
variance affecting all your

506
00:21:30,660 --> 00:21:33,990
items in this sequence
will blow out your

507
00:21:33,990 --> 00:21:36,360
request time hugely.

508
00:21:36,360 --> 00:21:39,930
Variable requests also make
scheduling a lot harder.

509
00:21:39,930 --> 00:21:41,955
And this something that I
haven't really touched on

510
00:21:41,955 --> 00:21:43,560
much, and I won't go into
too much detail.

511
00:21:43,560 --> 00:21:48,330
But the App Engine scheduler
needs to take a guess as to

512
00:21:48,330 --> 00:21:52,420
which App Engine instance
each request will

513
00:21:52,420 --> 00:21:53,660
best be served by.

514
00:21:53,660 --> 00:21:55,130
It's got a balance
across them.

515
00:21:55,130 --> 00:21:56,980
If it's getting too much
traffic, it'll start up new

516
00:21:56,980 --> 00:21:59,170
instances and so on
automatically.

517
00:21:59,170 --> 00:22:01,920
But it needs to usually, for the
most part, take a guess as

518
00:22:01,920 --> 00:22:05,780
to which instance will be idle
the first so it can add that

519
00:22:05,780 --> 00:22:10,510
request to that App Engine
instance's queue.

520
00:22:10,510 --> 00:22:13,640
The less predictable your
request handling is, though,

521
00:22:13,640 --> 00:22:16,270
the hard it is to do that
kind of prediction.

522
00:22:16,270 --> 00:22:20,840
So if there's variance in your
request handling, the

523
00:22:20,840 --> 00:22:23,740
scheduler, though it is great
and highly sophisticated, will

524
00:22:23,740 --> 00:22:25,600
start to make mistakes
more often.

525
00:22:25,600 --> 00:22:28,570
And so you might get your
instances being a bit

526
00:22:28,570 --> 00:22:28,860
overbalanced.

527
00:22:28,860 --> 00:22:32,880
You know, one handling 100
requests for only two being

528
00:22:32,880 --> 00:22:34,150
handled by another, and so on.

529
00:22:34,150 --> 00:22:35,530
So it might make
mistakes there.

530
00:22:35,530 --> 00:22:39,920
And this will lead to you
requiring more app instances,

531
00:22:39,920 --> 00:22:43,040
more instance hours, which will
lead to a higher bill if

532
00:22:43,040 --> 00:22:45,120
you've got billing enabled.

533
00:22:45,120 --> 00:22:47,590
So all we want to do
is control the

534
00:22:47,590 --> 00:22:48,630
variance of our requests.

535
00:22:48,630 --> 00:22:52,205
We want to put an upper limit
on how long we're going to

536
00:22:52,205 --> 00:22:54,280
spend doing certain
operations.

537
00:22:54,280 --> 00:23:00,909
I'll point you at this paper
publishing the ACM by two guys

538
00:23:00,909 --> 00:23:01,490
from Google.

539
00:23:01,490 --> 00:23:04,810
This goes into a lot of detail
about some techniques for

540
00:23:04,810 --> 00:23:09,010
controlling this kind of
variance, in particular, long

541
00:23:09,010 --> 00:23:10,810
tail latency.

542
00:23:10,810 --> 00:23:14,740
And it's geared around machines
in a data center

543
00:23:14,740 --> 00:23:15,990
doing server processing.

544
00:23:15,990 --> 00:23:19,060
But the same techniques apply
to web apps that are

545
00:23:19,060 --> 00:23:21,980
processing requests as well.

546
00:23:21,980 --> 00:23:27,280
So I'm going to discuss just
one way of doing this.

547
00:23:27,280 --> 00:23:31,500
So I'll point you to that paper
to go into more detail.

548
00:23:31,500 --> 00:23:33,620
Storing in Memcache is usually
an optimization.

549
00:23:33,620 --> 00:23:37,350
So as I mentioned before,
Memcache is not perfect, and

550
00:23:37,350 --> 00:23:39,360
it won't necessarily guarantee
to hold your data.

551
00:23:39,360 --> 00:23:42,037
If you need guaranteed data
storage, you need to store it

552
00:23:42,037 --> 00:23:43,770
in something like Datastore.

553
00:23:43,770 --> 00:23:48,740
But Memcache, as a result, is
usually an optimization.

554
00:23:48,740 --> 00:23:52,910
And optimizations by their
nature are usually optional.

555
00:23:52,910 --> 00:23:57,112
So what we want to do is, in our
request, we don't want to

556
00:23:57,112 --> 00:23:59,780
spend huge amounts of time,
unlimited amounts of time, on

557
00:23:59,780 --> 00:24:02,990
these things that aren't
strictly required.

558
00:24:02,990 --> 00:24:06,526
So in our Gopher Mart scenario,
we might be doing a

559
00:24:06,526 --> 00:24:09,190
bunch of request processing, and
then we might not want to

560
00:24:09,190 --> 00:24:13,110
store those fresh fruit items
that we've looked up out of

561
00:24:13,110 --> 00:24:14,700
the booklet in Memcache.

562
00:24:14,700 --> 00:24:16,170
But we don't want to spend
an infinite amount

563
00:24:16,170 --> 00:24:17,150
of time doing that.

564
00:24:17,150 --> 00:24:20,210
You know, Memcache might be
briefly down, or it might be

565
00:24:20,210 --> 00:24:23,330
running a bit slowly,
or whatever it is.

566
00:24:23,330 --> 00:24:25,460
So we want to cap the amount
of time that we'll spend

567
00:24:25,460 --> 00:24:27,260
waiting on it.

568
00:24:27,260 --> 00:24:29,570
And here's the first approach
that does not work.

569
00:24:29,570 --> 00:24:33,340

570
00:24:33,340 --> 00:24:35,600
In our request handler, we
might just kick off a

571
00:24:35,600 --> 00:24:40,000
memcache.Set operation at the
bottom in its own Go routine.

572
00:24:40,000 --> 00:24:41,120
We think, that's great.

573
00:24:41,120 --> 00:24:43,890
My request will immediately
return and Memcache will do

574
00:24:43,890 --> 00:24:45,620
its thing behind the scenes.

575
00:24:45,620 --> 00:24:49,450
But the problem here is that,
in the App Engine model, the

576
00:24:49,450 --> 00:24:52,770
scope of your context objects
are bound to the

577
00:24:52,770 --> 00:24:53,890
scope of the request.

578
00:24:53,890 --> 00:24:56,700
So any API calls have to be
done while the request is

579
00:24:56,700 --> 00:24:58,160
still going.

580
00:24:58,160 --> 00:25:03,850
And as soon as the HTTP handler
returns here, your

581
00:25:03,850 --> 00:25:07,110
context will be invalidated, and
any outstanding API calls

582
00:25:07,110 --> 00:25:09,730
will be canceled.

583
00:25:09,730 --> 00:25:11,810
So we can't do this.

584
00:25:11,810 --> 00:25:14,750
Here's what we can do.

585
00:25:14,750 --> 00:25:17,090
We can again kick off the
Memcache operation

586
00:25:17,090 --> 00:25:18,860
concurrently.

587
00:25:18,860 --> 00:25:20,590
And then we use a
done channel.

588
00:25:20,590 --> 00:25:24,520
And this done channel is going
to be signaled on as soon as

589
00:25:24,520 --> 00:25:27,420
that Memcache operation
returns.

590
00:25:27,420 --> 00:25:30,260
And then down at the bottom,
we use Go's concurrency

591
00:25:30,260 --> 00:25:33,350
primitive called a Select block
to wait for one of two

592
00:25:33,350 --> 00:25:34,940
channel operations to happen.

593
00:25:34,940 --> 00:25:36,880
The first one, if the
Memcache operation

594
00:25:36,880 --> 00:25:38,910
finishes, we go through.

595
00:25:38,910 --> 00:25:40,470
There's nothing in the select
body, so nothing's going to

596
00:25:40,470 --> 00:25:43,770
happen but will just immediately
return from the

597
00:25:43,770 --> 00:25:44,830
HTTP handler.

598
00:25:44,830 --> 00:25:48,760
So in the likely event that
Memcache is its usual quick

599
00:25:48,760 --> 00:25:53,160
and speedy self, this
has no overhead.

600
00:25:53,160 --> 00:25:55,080
The second case, though,
is if we end up

601
00:25:55,080 --> 00:25:56,320
waiting a bit too long.

602
00:25:56,320 --> 00:26:00,250
And Memcache will usually be 1
millisecond, 2 milliseconds,

603
00:26:00,250 --> 00:26:02,770
depending on what you're doing
with it and how much data is

604
00:26:02,770 --> 00:26:04,340
being sloshed around.

605
00:26:04,340 --> 00:26:06,050
If it takes more than 3
milliseconds that we've

606
00:26:06,050 --> 00:26:10,370
decided we want to wait for,
time.After will signal on the

607
00:26:10,370 --> 00:26:12,920
channel the returns after
3 milliseconds.

608
00:26:12,920 --> 00:26:16,340
So at that point, if Memcache
takes too long, that second

609
00:26:16,340 --> 00:26:19,730
channel will be signal on we
drop as like we've and return

610
00:26:19,730 --> 00:26:22,330
and that point the Memcache
operation will be canceled.

611
00:26:22,330 --> 00:26:23,170
It will fail.

612
00:26:23,170 --> 00:26:24,360
It will go away.

613
00:26:24,360 --> 00:26:26,540
And we've returned
to the user.

614
00:26:26,540 --> 00:26:30,840
So using these kind of
primitives to timeout waiting

615
00:26:30,840 --> 00:26:35,230
for things that are optional
is a useful technique to

616
00:26:35,230 --> 00:26:37,020
ensure high-performance
consistently.

617
00:26:37,020 --> 00:26:41,550

618
00:26:41,550 --> 00:26:45,800
So here's an overview
of where we went.

619
00:26:45,800 --> 00:26:48,070
We started off with a very
poorly written app on my

620
00:26:48,070 --> 00:26:50,810
behalf that did very
simplicity code.

621
00:26:50,810 --> 00:26:53,100
It just did a sequence of
Datastore operations, and then

622
00:26:53,100 --> 00:26:56,470
a mail.send and took nearly
400 milliseconds.

623
00:26:56,470 --> 00:26:58,210
Just by applying two of the
techniques that we did today,

624
00:26:58,210 --> 00:27:00,700
we got it down to only
70 milliseconds.

625
00:27:00,700 --> 00:27:04,140
So a dramatic speedup with
very little work.

626
00:27:04,140 --> 00:27:05,720
Once you've got experience
with it, these kinds of

627
00:27:05,720 --> 00:27:07,790
transformations will take
5 to 10 minutes.

628
00:27:07,790 --> 00:27:11,250
So it can very quickly done.

629
00:27:11,250 --> 00:27:13,245
So let me sum up what
we covered today.

630
00:27:13,245 --> 00:27:16,210
The first thing is to
find the performance

631
00:27:16,210 --> 00:27:17,330
bottlenecks in your code.

632
00:27:17,330 --> 00:27:19,840
Don't rely on your own
intuition, because your

633
00:27:19,840 --> 00:27:23,220
intuition will fool
you reliably.

634
00:27:23,220 --> 00:27:26,770
And, even if your intuition is
correct one day, it might be

635
00:27:26,770 --> 00:27:27,880
incorrect down the road.

636
00:27:27,880 --> 00:27:31,340
So the important thing to note
is to measure, and measure,

637
00:27:31,340 --> 00:27:33,100
and measure, and keep coming
back to measuring to

638
00:27:33,100 --> 00:27:35,620
understand your app's
performance.

639
00:27:35,620 --> 00:27:37,190
Then the five techniques
that I covered--

640
00:27:37,190 --> 00:27:38,210
deferring work--

641
00:27:38,210 --> 00:27:41,300
moving work that doesn't have
to happen during the request

642
00:27:41,300 --> 00:27:43,650
out of the scope of the request,
probably by using

643
00:27:43,650 --> 00:27:46,190
Task Queue or the
delay package.

644
00:27:46,190 --> 00:27:46,790
Batching--

645
00:27:46,790 --> 00:27:50,540
so doing multiple operations
in one shot.

646
00:27:50,540 --> 00:27:51,270
Caching--

647
00:27:51,270 --> 00:27:53,720
so memoizing data.

648
00:27:53,720 --> 00:27:55,170
It might be in the Datastore.

649
00:27:55,170 --> 00:27:56,130
It might be in Memcache.

650
00:27:56,130 --> 00:27:58,510
It might be in local memory.
it might be even in other

651
00:27:58,510 --> 00:28:02,690
places I haven't mentioned, so
that you can check them later

652
00:28:02,690 --> 00:28:05,970
and speed up those parts
of your app.

653
00:28:05,970 --> 00:28:06,880
Concurrency--

654
00:28:06,880 --> 00:28:09,970
so decomposing the work that
you're doing into independent

655
00:28:09,970 --> 00:28:12,940
pieces that can be combined.

656
00:28:12,940 --> 00:28:16,440
And finally controlling variance
to cut off the long

657
00:28:16,440 --> 00:28:19,230
tail of latency so that your
app requests can be

658
00:28:19,230 --> 00:28:22,750
consistently quick rather than
very, very occasionally taking

659
00:28:22,750 --> 00:28:25,890
a bit too much time.

660
00:28:25,890 --> 00:28:28,550
So finally here's a bunch
of links to the

661
00:28:28,550 --> 00:28:29,690
Go project in general.

662
00:28:29,690 --> 00:28:31,210
There's a heap of documentation
on there.

663
00:28:31,210 --> 00:28:34,120
You can download Go,
try it out online.

664
00:28:34,120 --> 00:28:39,680
The second link is to the
Go on App Engine docs.

665
00:28:39,680 --> 00:28:43,670
The third one links to these
talk's slides, plus my really

666
00:28:43,670 --> 00:28:47,640
bad source code for the Gopher
Mart app and its revisions,

667
00:28:47,640 --> 00:28:50,800
and then to the Appstats package
that was written by

668
00:28:50,800 --> 00:28:52,375
Matt Jibson, who
might be here.

669
00:28:52,375 --> 00:28:54,380
He did really good
work on that.

670
00:28:54,380 --> 00:28:56,780
There's more Go sessions coming
up later today and

671
00:28:56,780 --> 00:28:58,430
tomorrow, so get
along to that.

672
00:28:58,430 --> 00:29:04,120
In particular, I'll be at the
Office Hours this level at the

673
00:29:04,120 --> 00:29:06,970
Cloud Sandbox if you have any
particular questions about Go

674
00:29:06,970 --> 00:29:09,240
on App Engine, whether
performance or otherwise.

675
00:29:09,240 --> 00:29:12,160
So come along and have a chat.

676
00:29:12,160 --> 00:29:15,160
So with that, I'll thank you for
coming again, and we can

677
00:29:15,160 --> 00:29:16,858
take questions.

678
00:29:16,858 --> 00:29:24,995
[APPLAUSE]

679
00:29:24,995 --> 00:29:25,730
AUDIENCE: Can you hear me?

680
00:29:25,730 --> 00:29:26,480
OK.

681
00:29:26,480 --> 00:29:29,150
So I've been using Go for a
while now, but I haven't used

682
00:29:29,150 --> 00:29:29,880
App Engine yet.

683
00:29:29,880 --> 00:29:32,210
So quick question on the
control variance.

684
00:29:32,210 --> 00:29:32,990
DAVID SYMONDS: Yep.

685
00:29:32,990 --> 00:29:35,770
AUDIENCE: Instead of trying to
set it in Memcache right then

686
00:29:35,770 --> 00:29:39,610
and there, could you not put it
in a global channel and do

687
00:29:39,610 --> 00:29:42,865
a select on it with a timeout,
and then handle it outside?

688
00:29:42,865 --> 00:29:45,670

689
00:29:45,670 --> 00:29:46,475
DAVID SYMONDS: In a
global channel?

690
00:29:46,475 --> 00:29:48,100
You mean like in a
global variable?

691
00:29:48,100 --> 00:29:49,965
AUDIENCE: Right, right.

692
00:29:49,965 --> 00:29:52,520
DAVID SYMONDS: Yeah, you can
store there right away.

693
00:29:52,520 --> 00:29:55,390
The downside of that is that it
will only be used for that

694
00:29:55,390 --> 00:29:56,420
app instance.

695
00:29:56,420 --> 00:29:58,610
So the memory for each
app instance is only

696
00:29:58,610 --> 00:29:59,630
accessible to the app.

697
00:29:59,630 --> 00:30:00,810
AUDIENCE: That wasn't
the question, sir.

698
00:30:00,810 --> 00:30:03,780
So the operation of setting it
in Memcache, instead of doing

699
00:30:03,780 --> 00:30:07,180
it in that handle, could you
not put it in a channel and

700
00:30:07,180 --> 00:30:09,370
then pick it up from there
with a worker?

701
00:30:09,370 --> 00:30:12,590
Just set it in.

702
00:30:12,590 --> 00:30:15,390
DAVID SYMONDS: You could.

703
00:30:15,390 --> 00:30:18,410
The only way to make API calls
is during a request.

704
00:30:18,410 --> 00:30:20,940
So you can't kick off a Go
routine that does its own

705
00:30:20,940 --> 00:30:23,070
thing independent of requests.

706
00:30:23,070 --> 00:30:25,160
You could put it in a channel
and in a later request pull

707
00:30:25,160 --> 00:30:28,100
things off the channel and do
it at that point, or use a

708
00:30:28,100 --> 00:30:30,550
Task Queue or use backends
to do that kind of thing.

709
00:30:30,550 --> 00:30:31,760
Yeah, that would work.

710
00:30:31,760 --> 00:30:34,220
AUDIENCE: Thanks.

711
00:30:34,220 --> 00:30:34,960
DAVID SYMONDS: One
from this side.

712
00:30:34,960 --> 00:30:35,420
AUDIENCE: Hi.

713
00:30:35,420 --> 00:30:36,800
Nelson from [INAUDIBLE].

714
00:30:36,800 --> 00:30:39,350
Can you comment on a couple
techniques people are using to

715
00:30:39,350 --> 00:30:44,810
make Go applications more
efficient in pricing compared

716
00:30:44,810 --> 00:30:47,735
to Java and Python?

717
00:30:47,735 --> 00:30:51,270
DAVID SYMONDS: So I mentioned
that Go apps get compiled to

718
00:30:51,270 --> 00:30:52,690
native code.

719
00:30:52,690 --> 00:30:55,230
They're binaries.

720
00:30:55,230 --> 00:30:57,640
That results in them starting
up really quickly.

721
00:30:57,640 --> 00:31:00,840
So every time a new request
starts coming in and the

722
00:31:00,840 --> 00:31:04,760
infrastructure decides to spin
up new requests, it'll be a

723
00:31:04,760 --> 00:31:05,160
lot faster load.

724
00:31:05,160 --> 00:31:06,530
It's just got to run
a single program.

725
00:31:06,530 --> 00:31:08,520
It doesn't have to start a
virtual machine, page in a

726
00:31:08,520 --> 00:31:11,730
bunch of modules, or
whatever it is.

727
00:31:11,730 --> 00:31:13,700
So it'll be really
fast to start.

728
00:31:13,700 --> 00:31:17,190
That means that that single
instance starts up quickly and

729
00:31:17,190 --> 00:31:18,880
immediately can start
doing work.

730
00:31:18,880 --> 00:31:21,305
If you suddenly get an onslaught
of traffic, in other

731
00:31:21,305 --> 00:31:23,540
runtimes, you might have to
start up a bunch of instance.

732
00:31:23,540 --> 00:31:25,410
That'll take a little while,
but then they can deal with

733
00:31:25,410 --> 00:31:27,710
all the traffic that's
coming in.

734
00:31:27,710 --> 00:31:29,760
The second main way is that you
have a lot more control

735
00:31:29,760 --> 00:31:33,470
over the memory that
you are using.

736
00:31:33,470 --> 00:31:36,670
Go is a very hands-on
language.

737
00:31:36,670 --> 00:31:38,760
It gives you a lot more control
over things like

738
00:31:38,760 --> 00:31:39,730
memory lag and so on.

739
00:31:39,730 --> 00:31:43,200
So you can allocate a slab of
bytes, and you've got that

740
00:31:43,200 --> 00:31:43,820
slab of bytes.

741
00:31:43,820 --> 00:31:46,890
You don't have any overhead
for that.

742
00:31:46,890 --> 00:31:48,480
So using those kinds of things,
if you're intensively

743
00:31:48,480 --> 00:31:52,370
using your application
instance's memory, you can

744
00:31:52,370 --> 00:31:56,350
feed a lot more in, say an F1
instance, than you can in,

745
00:31:56,350 --> 00:31:58,630
say, other runtime.

746
00:31:58,630 --> 00:32:00,520
AUDIENCE: Why did you use
a buffered channel

747
00:32:00,520 --> 00:32:02,330
in that last example?

748
00:32:02,330 --> 00:32:03,580
DAVID SYMONDS: Good question.

749
00:32:03,580 --> 00:32:05,480

750
00:32:05,480 --> 00:32:07,450
AUDIENCE: Is that so the
Go routine dies?

751
00:32:07,450 --> 00:32:09,330
DAVID SYMONDS: Yeah, I use a
buffered channel in case the

752
00:32:09,330 --> 00:32:10,730
timeout happens.

753
00:32:10,730 --> 00:32:13,330
So if the timeout happens,
nothing's going to be reading

754
00:32:13,330 --> 00:32:15,520
from that done channel.

755
00:32:15,520 --> 00:32:17,040
So if the timeout happens,
nothing will be reading from

756
00:32:17,040 --> 00:32:19,580
that buffered channel, which
means when the Memcache call

757
00:32:19,580 --> 00:32:23,280
does return, it will block
unless I buffer it.

758
00:32:23,280 --> 00:32:26,810
So I buffer it so that if that
timeout happens, the Memcache

759
00:32:26,810 --> 00:32:29,300
can return and still do a
channel operation to a channel

760
00:32:29,300 --> 00:32:30,500
that will just disappear.

761
00:32:30,500 --> 00:32:31,955
But then that Go routine
will exit and

762
00:32:31,955 --> 00:32:33,080
won't be hanging around.

763
00:32:33,080 --> 00:32:34,540
AUDIENCE: So there could
be a memory leak if

764
00:32:34,540 --> 00:32:36,000
you don't do that?

765
00:32:36,000 --> 00:32:37,350
DAVID SYMONDS: You have
a Go routine leak.

766
00:32:37,350 --> 00:32:39,000
Yeah, that would be a memory
leak in that case if you

767
00:32:39,000 --> 00:32:39,590
didn't have a buffered
channel.

768
00:32:39,590 --> 00:32:41,120
AUDIENCE: It's not tied
to the parent channel,

769
00:32:41,120 --> 00:32:42,750
or the parent request?

770
00:32:42,750 --> 00:32:44,830
DAVID SYMONDS: No.

771
00:32:44,830 --> 00:32:46,170
AUDIENCE: I have a few
short questions.

772
00:32:46,170 --> 00:32:47,170
Thank you.

773
00:32:47,170 --> 00:32:50,040
NDB has caching built into it.

774
00:32:50,040 --> 00:32:53,980
Is there any plan to
do that for Go?

775
00:32:53,980 --> 00:32:55,760
DAVID SYMONDS: Yeah, there's a
couple of external packages

776
00:32:55,760 --> 00:32:59,880
that have been written that
provide an NBD-type layer.

777
00:32:59,880 --> 00:33:02,800
I can't remember them off the
top of my head, but they're

778
00:33:02,800 --> 00:33:04,400
out there, and I can look them
up if you want to ask me at

779
00:33:04,400 --> 00:33:05,280
Office Hours.

780
00:33:05,280 --> 00:33:05,630
AUDIENCE: Thank you.

781
00:33:05,630 --> 00:33:08,050
And also you seem to imply--

782
00:33:08,050 --> 00:33:09,610
it's only tied to
one CPU, right?

783
00:33:09,610 --> 00:33:12,145
But Go is really powerful
with multiple CPUs.

784
00:33:12,145 --> 00:33:16,550
You seem to imply that in the
future that won't be the case?

785
00:33:16,550 --> 00:33:20,260
DAVID SYMONDS: So each of your
app instances runs CPU bound

786
00:33:20,260 --> 00:33:21,180
on a single thread.

787
00:33:21,180 --> 00:33:25,470
So while it's doing an RPC to
do a Datastore fetch or

788
00:33:25,470 --> 00:33:28,170
whatever, it will switch
to other things.

789
00:33:28,170 --> 00:33:30,100
So it's got concurrency
in that respect.

790
00:33:30,100 --> 00:33:32,330
But we only have one CPU
thread running at any

791
00:33:32,330 --> 00:33:34,555
one time for now.

792
00:33:34,555 --> 00:33:36,210
AUDIENCE: It could change
in the future?

793
00:33:36,210 --> 00:33:36,970
DAVID SYMONDS: Yes, yes.

794
00:33:36,970 --> 00:33:37,760
AUDIENCE: Thank you.

795
00:33:37,760 --> 00:33:38,653
Thank you.

796
00:33:38,653 --> 00:33:40,770
DAVID SYMONDS: Yep.

797
00:33:40,770 --> 00:33:45,030
AUDIENCE: Python and Java
Runtime have a unit testing

798
00:33:45,030 --> 00:33:49,810
package with App Engine APIs.

799
00:33:49,810 --> 00:33:57,940
Now, Go has no new test package
for App Engine APIs.

800
00:33:57,940 --> 00:33:59,900
Do you have any plan to?

801
00:33:59,900 --> 00:34:02,300
DAVID SYMONDS: Yes, we realize
the testing story for Go on

802
00:34:02,300 --> 00:34:05,413
App Engine is weak, and we have
plans to improve that in

803
00:34:05,413 --> 00:34:06,880
the near future.

804
00:34:06,880 --> 00:34:09,040
AUDIENCE: Thank you.

805
00:34:09,040 --> 00:34:14,530
Just a question again on the
multi-threading side.

806
00:34:14,530 --> 00:34:18,440
If I take an instance with two
cores, what's happening there?

807
00:34:18,440 --> 00:34:21,969

808
00:34:21,969 --> 00:34:25,949
One of them sits idle
all the time?

809
00:34:25,949 --> 00:34:30,310
DAVID SYMONDS: The instance
classes are based on

810
00:34:30,310 --> 00:34:34,894
virtualized CPU gigahertz rather
than number of cores.

811
00:34:34,894 --> 00:34:37,889

812
00:34:37,889 --> 00:34:39,520
Sorry, I'm not sure off the top
of my head what it would

813
00:34:39,520 --> 00:34:42,170
do if you were on two cores
in that instance.

814
00:34:42,170 --> 00:34:44,260
It's possible that you would run
multiple instances on that

815
00:34:44,260 --> 00:34:46,540
machine instead.

816
00:34:46,540 --> 00:34:47,790
AUDIENCE: Thanks.

817
00:34:47,790 --> 00:34:50,429

818
00:34:50,429 --> 00:34:51,679
DAVID SYMONDS: Any
more questions?

819
00:34:51,679 --> 00:34:54,810

820
00:34:54,810 --> 00:34:55,310
Yup.

821
00:34:55,310 --> 00:34:55,630
OK.

822
00:34:55,630 --> 00:34:56,880
Thank you very much for coming.

823
00:34:56,880 --> 00:34:59,177

