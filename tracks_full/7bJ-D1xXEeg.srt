1
00:00:00,000 --> 00:00:05,040

2
00:00:05,040 --> 00:00:06,450
Thank you, everyone,
for coming today.

3
00:00:06,450 --> 00:00:07,880
My name is Colt McAnlis.

4
00:00:07,880 --> 00:00:11,090
I'm a Developer Advocate at
Google focusing on gaming and

5
00:00:11,090 --> 00:00:13,030
web technologies in general.

6
00:00:13,030 --> 00:00:16,079
You see my email address here,
colton@google.com.

7
00:00:16,079 --> 00:00:17,800
And we are here today to
talk about a very,

8
00:00:17,800 --> 00:00:19,560
very important topic.

9
00:00:19,560 --> 00:00:21,370
We're here to talk about
texture compression.

10
00:00:21,370 --> 00:00:24,070
Now before we get too started,
I'm going to need to see a

11
00:00:24,070 --> 00:00:24,700
show of hands.

12
00:00:24,700 --> 00:00:27,570
How many of you in here have
actually shipped a product, a

13
00:00:27,570 --> 00:00:29,290
video game, on a console?

14
00:00:29,290 --> 00:00:33,680
So Xbox 360, PS3, Nintendo
DS for that matter?

15
00:00:33,680 --> 00:00:34,180
OK, good.

16
00:00:34,180 --> 00:00:35,670
So out of those hands--

17
00:00:35,670 --> 00:00:37,110
you can put them down and
we'll get a new set--

18
00:00:37,110 --> 00:00:39,890
out of those hands, how many of
you have ever fought with

19
00:00:39,890 --> 00:00:41,976
texture footprint in memory?

20
00:00:41,976 --> 00:00:42,890
OK, Good.

21
00:00:42,890 --> 00:00:43,510
Good, good, good.

22
00:00:43,510 --> 00:00:47,100
So obviously, you know that
compressing your textures is a

23
00:00:47,100 --> 00:00:49,700
big problem and that, most
importantly, you spend a lot

24
00:00:49,700 --> 00:00:53,100
of time trying to fight your
memory constraints against

25
00:00:53,100 --> 00:00:54,150
what your artists
are producing.

26
00:00:54,150 --> 00:00:55,860
Well today, we're here to talk
about that specifically.

27
00:00:55,860 --> 00:00:58,440
And what we want to talk about
is DXT is not enough.

28
00:00:58,440 --> 00:01:00,820
Now I know there's hardware
architecture differences with

29
00:01:00,820 --> 00:01:03,540
compressed textures and
PVR and ECT and

30
00:01:03,540 --> 00:01:04,190
all these other things.

31
00:01:04,190 --> 00:01:06,950
So I hope you take what we're
talking about today and how

32
00:01:06,950 --> 00:01:08,940
it's being applied
to DXT texturing.

33
00:01:08,940 --> 00:01:10,660
And I hope you apply it to some
of these other things

34
00:01:10,660 --> 00:01:12,260
that may be platform-specific.

35
00:01:12,260 --> 00:01:14,350
So all right, let's
get started then.

36
00:01:14,350 --> 00:01:17,590
So texture footprints matters
for games, as we've already

37
00:01:17,590 --> 00:01:18,280
talked about this.

38
00:01:18,280 --> 00:01:21,130
How many times have you been
down to the wire or down to

39
00:01:21,130 --> 00:01:24,260
memory and needed an extra five
megabytes for animations

40
00:01:24,260 --> 00:01:25,760
or 10 megabytes for sound?

41
00:01:25,760 --> 00:01:28,810
And you go back and you do your
statistic gathering and

42
00:01:28,810 --> 00:01:32,110
find that your losing 50% or
60% of your entire memory

43
00:01:32,110 --> 00:01:33,880
footprint just for
texture assets.

44
00:01:33,880 --> 00:01:36,260
And there's tons of different
reasons for this.

45
00:01:36,260 --> 00:01:38,480
They could need to
be high-res.

46
00:01:38,480 --> 00:01:40,390
Or there could be too
many MIP chains.

47
00:01:40,390 --> 00:01:43,540
Or the artists author them at
2048's when they only occupy a

48
00:01:43,540 --> 00:01:46,200
64x64 pixel set on the screen.

49
00:01:46,200 --> 00:01:47,480
Whatever the reasons, right?

50
00:01:47,480 --> 00:01:51,010
For us, this is a big problem
because, as a console

51
00:01:51,010 --> 00:01:53,590
developer, you spend a lot
of time fighting this.

52
00:01:53,590 --> 00:01:56,070
Retail doesn't really care
a lot about this, though.

53
00:01:56,070 --> 00:01:57,110
There's a great article--

54
00:01:57,110 --> 00:01:59,580
I forget which site posted
it-- where John Carmack

55
00:01:59,580 --> 00:02:03,000
actually went to the head of
their publisher at the time

56
00:02:03,000 --> 00:02:05,670
and said, hey, I've got an
amazing new technology.

57
00:02:05,670 --> 00:02:08,160
It's called MegaTexture.

58
00:02:08,160 --> 00:02:09,030
And it's awesome.

59
00:02:09,030 --> 00:02:10,630
You can stream in
infinite detail.

60
00:02:10,630 --> 00:02:11,610
And it's fantastic.

61
00:02:11,610 --> 00:02:15,590
The problem is that we're now
looking at 17 gigabytes of

62
00:02:15,590 --> 00:02:17,560
data just for one game.

63
00:02:17,560 --> 00:02:20,070
And in the article, it actually
says that the head of

64
00:02:20,070 --> 00:02:21,990
the publisher said, hey,
that's fantastic.

65
00:02:21,990 --> 00:02:22,490
I love it.

66
00:02:22,490 --> 00:02:25,790
This will push our Blu-ray
sales for video games.

67
00:02:25,790 --> 00:02:30,170
The point of this is that, for
retail developers, that's

68
00:02:30,170 --> 00:02:31,180
good, right?

69
00:02:31,180 --> 00:02:33,540
They're giving out this physical
medium and pushing

70
00:02:33,540 --> 00:02:36,920
this physical medium to consoles
and to PC developers

71
00:02:36,920 --> 00:02:39,020
the 17-gig Blu-rays.

72
00:02:39,020 --> 00:02:40,210
This is great.

73
00:02:40,210 --> 00:02:45,790
But for us digital distribution
outlets, as well

74
00:02:45,790 --> 00:02:48,680
as consumers in general, this
isn't a good thing.

75
00:02:48,680 --> 00:02:51,440
Waiting for 17 to 40 gigabytes
worth of data to play your

76
00:02:51,440 --> 00:02:53,740
game is not good at all.

77
00:02:53,740 --> 00:02:54,630
It takes time.

78
00:02:54,630 --> 00:02:58,260
It creates a lag between what
the user's expectation is and

79
00:02:58,260 --> 00:03:00,130
pay rate is to when
they actually

80
00:03:00,130 --> 00:03:01,140
experience the content.

81
00:03:01,140 --> 00:03:04,550
And there's a large gap there
that is directly correlated to

82
00:03:04,550 --> 00:03:07,150
how many users buy your game,
how many users continue to

83
00:03:07,150 --> 00:03:09,480
play your game, and how many
users just get stopped at the

84
00:03:09,480 --> 00:03:10,040
front door.

85
00:03:10,040 --> 00:03:12,310
And as we all know, especially
the Indies that may be in the

86
00:03:12,310 --> 00:03:14,180
audience today, that's
huge, right?

87
00:03:14,180 --> 00:03:16,200
You want these people
giving you money.

88
00:03:16,200 --> 00:03:19,800
And creating barriers to their
experience is a bad thing.

89
00:03:19,800 --> 00:03:22,920
So we talk about texture
compression systems because we

90
00:03:22,920 --> 00:03:25,940
want to avoid 17 gigabytes,
right?

91
00:03:25,940 --> 00:03:27,500
We want to move down.

92
00:03:27,500 --> 00:03:29,550
You need to look at
it as a triangle.

93
00:03:29,550 --> 00:03:32,520
For instance, this is a nice
little understanding here.

94
00:03:32,520 --> 00:03:34,610
And again, we're talking about
texture decompression, so

95
00:03:34,610 --> 00:03:36,620
we're talking about the runtime

96
00:03:36,620 --> 00:03:38,400
decompressing of textures.

97
00:03:38,400 --> 00:03:41,150
It's kind of a performance
triangle where you need to

98
00:03:41,150 --> 00:03:44,400
optimize, based upon your game
and your title, between the

99
00:03:44,400 --> 00:03:47,380
quality of the decompressed
image, the size and the

100
00:03:47,380 --> 00:03:49,930
footprint of the in-memory
representation and then how

101
00:03:49,930 --> 00:03:52,980
fast you can actually decompress
that data.

102
00:03:52,980 --> 00:03:56,330
And depending on your game,
you'll find a different point

103
00:03:56,330 --> 00:03:58,080
on this triangle.

104
00:03:58,080 --> 00:04:03,160
So for some of Id stuff,
you may be more towards

105
00:04:03,160 --> 00:04:05,600
decompression speed and
size than quality.

106
00:04:05,600 --> 00:04:09,680
For FarmVille, obviously, you
would want quality and size as

107
00:04:09,680 --> 00:04:10,550
a better metric.

108
00:04:10,550 --> 00:04:12,300
Maybe speed is not as
big of an issue.

109
00:04:12,300 --> 00:04:13,200
Who knows?

110
00:04:13,200 --> 00:04:15,860
But you really need to take
this triangle into thought

111
00:04:15,860 --> 00:04:17,610
when you're talking about
decompression and algorithms

112
00:04:17,610 --> 00:04:18,070
in general.

113
00:04:18,070 --> 00:04:19,450
And hopefully, you'll think
about this while we're talking

114
00:04:19,450 --> 00:04:21,910
about our stuff today too.

115
00:04:21,910 --> 00:04:23,930
So the state-of-the-art for this
process, let's talk about

116
00:04:23,930 --> 00:04:25,330
what you all are
already doing.

117
00:04:25,330 --> 00:04:26,120
So you should all know this.

118
00:04:26,120 --> 00:04:29,070
You should all be familiar with
this as we start forward.

119
00:04:29,070 --> 00:04:32,230
So the way that people handle
textures today is the artist

120
00:04:32,230 --> 00:04:35,290
usually generates some PSD
file in Photoshop.

121
00:04:35,290 --> 00:04:39,300
This PSD file is converted to a
DXT texture, either through

122
00:04:39,300 --> 00:04:41,350
Microsoft's tool, or
NVIDIA tools, or

123
00:04:41,350 --> 00:04:44,460
Compressonator from AMD, ATI.

124
00:04:44,460 --> 00:04:47,550
And then that data is usually
just thrown, batch process

125
00:04:47,550 --> 00:04:50,170
ad-hoc, into a zip file or
some sort of compressed,

126
00:04:50,170 --> 00:04:52,210
binary archive.

127
00:04:52,210 --> 00:04:55,640
And then that archive is then
put on disk or put into

128
00:04:55,640 --> 00:04:57,420
memory, shipped out
with the product.

129
00:04:57,420 --> 00:05:00,270
And when a level load or the
game loads, usually what

130
00:05:00,270 --> 00:05:04,450
happens is the zip
file is opened up

131
00:05:04,450 --> 00:05:06,400
in memory at runtime.

132
00:05:06,400 --> 00:05:08,850
The entire zip archive is either
loaded into memory or

133
00:05:08,850 --> 00:05:10,710
kept on disk and then
streamed from.

134
00:05:10,710 --> 00:05:13,290
And that data is copied right
from the memory, or right from

135
00:05:13,290 --> 00:05:15,560
the compressed version,
right to the GPU.

136
00:05:15,560 --> 00:05:18,180
So the DXT data that's in the
archive and in memory just

137
00:05:18,180 --> 00:05:20,970
gets, effectively,
mem copied over.

138
00:05:20,970 --> 00:05:22,590
And this is usually fine.

139
00:05:22,590 --> 00:05:26,340
But the problem here is that you
don't want to always keep

140
00:05:26,340 --> 00:05:29,410
this process going because,
when you open your zip

141
00:05:29,410 --> 00:05:31,730
archive, you either keep the
entire thing in memory or you

142
00:05:31,730 --> 00:05:35,060
keep it all on disc and
stream from it.

143
00:05:35,060 --> 00:05:37,150
And the point here is that you
should not keep your entire

144
00:05:37,150 --> 00:05:37,970
zip in archive.

145
00:05:37,970 --> 00:05:41,920
I remember, in my previous life,
we were putting out a

146
00:05:41,920 --> 00:05:43,920
console title on the Xbox 360.

147
00:05:43,920 --> 00:05:47,060
And we had archives
around four gigs.

148
00:05:47,060 --> 00:05:49,900
Well, the problem was the
hardware we were working on

149
00:05:49,900 --> 00:05:52,550
only had 512 megs of RAM that
we could actually work with.

150
00:05:52,550 --> 00:05:55,160
So obviously, keeping the entire
thing in memory is not

151
00:05:55,160 --> 00:05:56,330
a good idea.

152
00:05:56,330 --> 00:05:59,260
On the other side of this,
actually hitting the optical

153
00:05:59,260 --> 00:06:03,450
disk to stream in your data is
not always available as well.

154
00:06:03,450 --> 00:06:06,130
In addition to that, some of the
data that exists in your

155
00:06:06,130 --> 00:06:08,680
archive is not actually
needed.

156
00:06:08,680 --> 00:06:11,100
So for instance, let's say
you're loading a level, right?

157
00:06:11,100 --> 00:06:13,680
When you load a level, there's
going to be lots of data.

158
00:06:13,680 --> 00:06:16,270
There's going to be bounding
volume hierarchies, quad

159
00:06:16,270 --> 00:06:17,740
trees, navigational meshes.

160
00:06:17,740 --> 00:06:20,000
There's going to be animations
for characters.

161
00:06:20,000 --> 00:06:22,015
Some of this stuff can actually
be loaded once and

162
00:06:22,015 --> 00:06:24,810
then thrown away, while
other items and other

163
00:06:24,810 --> 00:06:27,420
representations need to be
loaded and continuously used.

164
00:06:27,420 --> 00:06:31,010
And this is where you get the
divide between load and forget

165
00:06:31,010 --> 00:06:32,380
and then load and
continue using

166
00:06:32,380 --> 00:06:36,070
between data and textures.

167
00:06:36,070 --> 00:06:39,050
So you kind of go, OK, well, if
I have a zip archive that

168
00:06:39,050 --> 00:06:43,790
represents level seven and I
only need to use the first 25%

169
00:06:43,790 --> 00:06:46,470
during load time, and then the
rest of it I need at runtime

170
00:06:46,470 --> 00:06:49,220
for textures or whatever,
then it makes sense to

171
00:06:49,220 --> 00:06:50,240
segment that out.

172
00:06:50,240 --> 00:06:52,480
And say, OK, well, here's
the pre-load

173
00:06:52,480 --> 00:06:53,870
stuff for level seven.

174
00:06:53,870 --> 00:06:56,490
And then, here's the runtime
stuff for level seven.

175
00:06:56,490 --> 00:06:58,240
And then there's sort of a
rabbit hole that, once you

176
00:06:58,240 --> 00:07:00,060
start thinking about this, you
go down this rabbit hole.

177
00:07:00,060 --> 00:07:01,790
And you end up in
utter madness.

178
00:07:01,790 --> 00:07:04,130
But you don't get the free tea
or the cool hat or the rabbit

179
00:07:04,130 --> 00:07:04,590
running around.

180
00:07:04,590 --> 00:07:08,320
But you get the craziness of
the experience because what

181
00:07:08,320 --> 00:07:11,230
happens is you start to bin-sort
all of your content

182
00:07:11,230 --> 00:07:13,950
around and figure out these
optimal solutions.

183
00:07:13,950 --> 00:07:15,340
That's really just madness.

184
00:07:15,340 --> 00:07:18,590
Really, what you should be doing
is you should find a way

185
00:07:18,590 --> 00:07:21,530
to hyper compress your textures
so that they turn

186
00:07:21,530 --> 00:07:24,370
into a load-once solution.

187
00:07:24,370 --> 00:07:26,860
And the rest of this talk is
going to be focused explicitly

188
00:07:26,860 --> 00:07:27,830
on this sort of technique.

189
00:07:27,830 --> 00:07:30,290
The idea here is that you're
going to want to hyper

190
00:07:30,290 --> 00:07:32,860
compress your textures in some
way that you can load them

191
00:07:32,860 --> 00:07:35,910
into main memory and keep them
in main memory for the entire

192
00:07:35,910 --> 00:07:38,130
duration of the game, or the
level, or what you need.

193
00:07:38,130 --> 00:07:41,600
And then, on-demand, decompress
them from whatever

194
00:07:41,600 --> 00:07:44,170
format they're in
to a GPU format.

195
00:07:44,170 --> 00:07:46,020
And then throw them
away, if you need.

196
00:07:46,020 --> 00:07:49,240
And this is actually a very
powerful instance, right?

197
00:07:49,240 --> 00:07:52,500
IDtech5 has made waves
with this.

198
00:07:52,500 --> 00:07:55,210
I've been hearing about
MegaTexture for almost six

199
00:07:55,210 --> 00:07:57,490
years, I think, seven years
has been the big thing.

200
00:07:57,490 --> 00:08:00,130
And RAGE has finally shipped.

201
00:08:00,130 --> 00:08:05,090
And you can go play the game
and see the RAGE of IDtech5

202
00:08:05,090 --> 00:08:06,380
and what happens.

203
00:08:06,380 --> 00:08:08,220
Basically, they realized
that this was a

204
00:08:08,220 --> 00:08:09,220
problem early on, right?

205
00:08:09,220 --> 00:08:11,070
They had tons and tons
of texture data.

206
00:08:11,070 --> 00:08:14,110
And the way that they handled
this was they stored the

207
00:08:14,110 --> 00:08:16,240
textures in this hyper
compressed format.

208
00:08:16,240 --> 00:08:20,200
Basically, some of their early
papers by Van Waveren--

209
00:08:20,200 --> 00:08:22,570
you can find them on the
Intel Developer site--

210
00:08:22,570 --> 00:08:26,060
actually talk about creating a
variant of JPEG, which is a

211
00:08:26,060 --> 00:08:30,610
DCT block-based format, and
actually compressing them to

212
00:08:30,610 --> 00:08:32,380
DXT at runtime.

213
00:08:32,380 --> 00:08:35,120
So basically, offline, they
would compress them into this

214
00:08:35,120 --> 00:08:36,929
lossy-style format, DCT.

215
00:08:36,929 --> 00:08:39,330
And then, at runtime, they'd
take that DCT data and then

216
00:08:39,330 --> 00:08:41,970
convert it over to DXT, which
was then mem copied to the

217
00:08:41,970 --> 00:08:43,360
GPU, right?

218
00:08:43,360 --> 00:08:45,830
The cool thing about this is,
according to their research,

219
00:08:45,830 --> 00:08:48,860
they were getting about 112
megapixels or megatexles a

220
00:08:48,860 --> 00:08:50,670
second on dual core.

221
00:08:50,670 --> 00:08:53,730
That's a lot of megapixels,
megatexles a second.

222
00:08:53,730 --> 00:08:56,670
That's a lot of texture data to
be effectively streaming,

223
00:08:56,670 --> 00:08:59,210
converting and moving around.

224
00:08:59,210 --> 00:09:01,320
Some of the down-sides of this,
though, aren't really

225
00:09:01,320 --> 00:09:01,980
that published.

226
00:09:01,980 --> 00:09:04,030
But when you actually start
implementing that algorithm

227
00:09:04,030 --> 00:09:06,970
and realizing what's going on
here, or if you've spend a lot

228
00:09:06,970 --> 00:09:09,650
of time with texture
compression, you first realize

229
00:09:09,650 --> 00:09:11,620
that this is very processor
intensive.

230
00:09:11,620 --> 00:09:14,550
You're spending a lot of
CPU time redoing the

231
00:09:14,550 --> 00:09:15,820
same sort of work.

232
00:09:15,820 --> 00:09:19,440
You're taking a DCT texture,
converting it to DXT.

233
00:09:19,440 --> 00:09:21,730
In addition to that, this
actually converts and

234
00:09:21,730 --> 00:09:24,640
introduces 2 times the
amount of noise.

235
00:09:24,640 --> 00:09:28,630
DCT actually is a lossy codec.

236
00:09:28,630 --> 00:09:29,610
It's a lossy form.

237
00:09:29,610 --> 00:09:32,070
So what happens is, when you
compress your texture, it's

238
00:09:32,070 --> 00:09:34,590
going to introduce noise
into the texture.

239
00:09:34,590 --> 00:09:38,130
And then, when you decompress
the texture to compress it to

240
00:09:38,130 --> 00:09:41,240
DXT, DXT is also
a lossy format.

241
00:09:41,240 --> 00:09:44,170
So what's happening is you're
introducing error, reverting

242
00:09:44,170 --> 00:09:46,470
that error, but keeping it
around to some respect and

243
00:09:46,470 --> 00:09:48,050
then introducing new
error again.

244
00:09:48,050 --> 00:09:50,990
So you're actually getting 2
times the amount of error in

245
00:09:50,990 --> 00:09:53,350
your image during the
compression process.

246
00:09:53,350 --> 00:09:56,040
In addition to that, because
of the fact that the DXT

247
00:09:56,040 --> 00:09:59,440
conversion is happening at
runtime, it means that you

248
00:09:59,440 --> 00:10:02,840
can't spend a ton of time
optimizing for how your DXT

249
00:10:02,840 --> 00:10:04,440
blocks should be considered,
right?

250
00:10:04,440 --> 00:10:07,430
Basically, you have to resort
to a box fit or a best fit

251
00:10:07,430 --> 00:10:10,480
algorithm, which might not
actually produce the best

252
00:10:10,480 --> 00:10:12,200
color correlation ratio.

253
00:10:12,200 --> 00:10:13,820
There's tons of algorithms
out there.

254
00:10:13,820 --> 00:10:16,960
We're going to talk about some
today that allow you to get a

255
00:10:16,960 --> 00:10:20,030
better quality, if you're
working at it offline.

256
00:10:20,030 --> 00:10:20,730
So here's the idea.

257
00:10:20,730 --> 00:10:22,470
Here the whole point of this
talk that I'd like to talk

258
00:10:22,470 --> 00:10:23,210
about today.

259
00:10:23,210 --> 00:10:25,410
So what if we approach this
from a different angle?

260
00:10:25,410 --> 00:10:30,330
What if, instead of taking an
existing compressed form,

261
00:10:30,330 --> 00:10:33,330
decompressing it and translating
it into DXT at

262
00:10:33,330 --> 00:10:36,920
runtime, what if we started with
the DXT data initially?

263
00:10:36,920 --> 00:10:40,030
And we took that DXT data, and
we hyper compressed it?

264
00:10:40,030 --> 00:10:43,270
So we hyper compress the DXT
data more, such that, at

265
00:10:43,270 --> 00:10:47,140
runtime, all we're doing is a
decompression step to get us

266
00:10:47,140 --> 00:10:49,250
our original DXT input.

267
00:10:49,250 --> 00:10:50,840
So this is the idea.

268
00:10:50,840 --> 00:10:53,020
The idea is that we don't
introduce any new error data

269
00:10:53,020 --> 00:10:55,300
with this because it's just
going from something

270
00:10:55,300 --> 00:10:57,320
compressed back to DXT.

271
00:10:57,320 --> 00:10:59,610
And then we can actually store
this in memory, hyper

272
00:10:59,610 --> 00:11:00,160
compressed.

273
00:11:00,160 --> 00:11:03,090
So we load it from our zip file,
keep it in some section

274
00:11:03,090 --> 00:11:05,080
of memory that we're not
using all the time.

275
00:11:05,080 --> 00:11:07,500
And then we decompress directly
to DXT whenever our

276
00:11:07,500 --> 00:11:11,070
view frustum changes, or we have
a cache invalidation, or

277
00:11:11,070 --> 00:11:14,600
we do a flush, or something
along those lines.

278
00:11:14,600 --> 00:11:19,200
So in order to test if this is
the right direction to go, we

279
00:11:19,200 --> 00:11:23,010
need some sort of test images
to prove both decompression

280
00:11:23,010 --> 00:11:25,550
time and size and speeds
and whatnot like that.

281
00:11:25,550 --> 00:11:30,240
So anytime you're doing image
compression block tests, you

282
00:11:30,240 --> 00:11:32,980
have to produce some sort of
conical representation

283
00:11:32,980 --> 00:11:33,650
to take a look at.

284
00:11:33,650 --> 00:11:35,800
So what I've created is
actually a random

285
00:11:35,800 --> 00:11:36,730
collection of images.

286
00:11:36,730 --> 00:11:38,640
All images behave differently.

287
00:11:38,640 --> 00:11:42,150
I think it would be
irresponsible on my side to

288
00:11:42,150 --> 00:11:43,650
just use one type of image.

289
00:11:43,650 --> 00:11:46,900
So I've actually included a
bunch of images in my set from

290
00:11:46,900 --> 00:11:47,320
different things.

291
00:11:47,320 --> 00:11:48,220
Some are from games--

292
00:11:48,220 --> 00:11:51,200
the actual PSD, the source
images, the TGAs uncompressed.

293
00:11:51,200 --> 00:11:53,010
So there's no noise
being added there.

294
00:11:53,010 --> 00:11:54,370
Some are public.

295
00:11:54,370 --> 00:11:56,880
For example, the famous Lena
image, which, if you don't

296
00:11:56,880 --> 00:12:00,500
know who Lena is in respect to
compression, please go take a

297
00:12:00,500 --> 00:12:01,540
look at it.

298
00:12:01,540 --> 00:12:03,060
And some are from
image libraries.

299
00:12:03,060 --> 00:12:05,100
The Kodak Image Library
is actually fantastic.

300
00:12:05,100 --> 00:12:07,720
It's a lot of high-resolution
images that are great to test

301
00:12:07,720 --> 00:12:10,230
against your own compression
algorithms.

302
00:12:10,230 --> 00:12:12,320
And then all of the numbers I'm
going to be showing today

303
00:12:12,320 --> 00:12:14,480
for these compression processes
actually include the

304
00:12:14,480 --> 00:12:16,860
DDS headers, which is
about 128 bytes.

305
00:12:16,860 --> 00:12:20,050
So this is going to skew
the results slightly.

306
00:12:20,050 --> 00:12:22,770
But if you take that into
account, you have to have the

307
00:12:22,770 --> 00:12:25,240
header of the texture, how big
it is, how many MIPs it is,

308
00:12:25,240 --> 00:12:27,510
what the format is, all
these other things.

309
00:12:27,510 --> 00:12:28,690
So take that in.

310
00:12:28,690 --> 00:12:32,070
And then, any percentage values
that I produce on my

311
00:12:32,070 --> 00:12:35,770
slides are actually in
amount of reduction.

312
00:12:35,770 --> 00:12:39,100
This is basically saying, how
much has been reduced from the

313
00:12:39,100 --> 00:12:43,020
original format so that you're
seeing this large delta to see

314
00:12:43,020 --> 00:12:45,340
what we're saving as all of
these things come together.

315
00:12:45,340 --> 00:12:48,580

316
00:12:48,580 --> 00:12:50,190
So before we move on,
let's take a look at

317
00:12:50,190 --> 00:12:51,410
a typical DXT block.

318
00:12:51,410 --> 00:12:54,100
And for the sake of this talk,
I'm going to focus on DXT1

319
00:12:54,100 --> 00:12:56,240
because, I think, understanding
it from here

320
00:12:56,240 --> 00:12:59,640
allows us to launch off the
different algorithms, DXT3, 5,

321
00:12:59,640 --> 00:13:01,590
DXTN, all these other
sorts of things.

322
00:13:01,590 --> 00:13:03,930
So a DXT block is constructed
likewise.

323
00:13:03,930 --> 00:13:06,720
First off, it starts by storing
a high color and a low

324
00:13:06,720 --> 00:13:08,560
color in a 5-6-5 bit poly.

325
00:13:08,560 --> 00:13:11,660
That's red, green, blue
5-bit, 6-bits, 5 bits.

326
00:13:11,660 --> 00:13:14,160
What these two colors represent
in space is

327
00:13:14,160 --> 00:13:15,780
effectively the endpoints
of a line.

328
00:13:15,780 --> 00:13:20,260
You can see that, these two
large circles here.

329
00:13:20,260 --> 00:13:22,520
Then what you end up with is,
with these two high- and

330
00:13:22,520 --> 00:13:25,710
low-colors, you end up with
a 4x4 block of pixels.

331
00:13:25,710 --> 00:13:28,030
These 4x4 block effectively
only contains

332
00:13:28,030 --> 00:13:30,290
two bits per cell.

333
00:13:30,290 --> 00:13:33,930
And the two bits actually
represent a discrete step

334
00:13:33,930 --> 00:13:37,580
between these two endpoints
in color space, right?

335
00:13:37,580 --> 00:13:40,190
So you can see on the image
here, we've got some random

336
00:13:40,190 --> 00:13:42,460
assortment of colors that
are off the line.

337
00:13:42,460 --> 00:13:44,440
And effectively, they
get snapped to one

338
00:13:44,440 --> 00:13:46,040
of these four values.

339
00:13:46,040 --> 00:13:48,450
This is the heart of
DXT compression.

340
00:13:48,450 --> 00:13:52,060
It allows for a really fast
load, really fast filtering.

341
00:13:52,060 --> 00:13:54,485
It allows you to keep a lot of
blocks in a mem cache line on

342
00:13:54,485 --> 00:13:56,840
the GPU, which is really
important.

343
00:13:56,840 --> 00:13:58,590
And in general, it doesn't
actually reduce

344
00:13:58,590 --> 00:13:59,480
the quality too much.

345
00:13:59,480 --> 00:14:03,610
But it is lossy, so it does
introduce some error.

346
00:14:03,610 --> 00:14:06,940
So let's look at the initial
data set for DXT.

347
00:14:06,940 --> 00:14:10,040
The uncompressed images
are around 37 megs.

348
00:14:10,040 --> 00:14:12,540
This is just the source image,
non-compressed, right?

349
00:14:12,540 --> 00:14:15,820
And I could have chosen a
bigger data set size.

350
00:14:15,820 --> 00:14:19,190
But every time I would run
this set, it takes 15, 20

351
00:14:19,190 --> 00:14:20,700
minutes to actually run
and get the results.

352
00:14:20,700 --> 00:14:22,870
And I've got stuff to do today,
so I can't have a

353
00:14:22,870 --> 00:14:24,470
200-megabyte set.

354
00:14:24,470 --> 00:14:26,680
DXT1, if I just take the source
data and compress it

355
00:14:26,680 --> 00:14:30,800
into DXT1, it gets it down to
about 7.6 megs, which is good.

356
00:14:30,800 --> 00:14:31,970
That's fantastic.

357
00:14:31,970 --> 00:14:34,170
That's exactly the type of
compression size you want to

358
00:14:34,170 --> 00:14:35,720
see out of DXT.

359
00:14:35,720 --> 00:14:38,480
If we zip that data, so we do
whatever everyone in the room

360
00:14:38,480 --> 00:14:39,240
is already doing.

361
00:14:39,240 --> 00:14:40,750
We just take that DXT
data, throw into an

362
00:14:40,750 --> 00:14:41,900
archive, and zip it.

363
00:14:41,900 --> 00:14:46,490
That ends up at about 4.82
megs, or about 36%, 37%

364
00:14:46,490 --> 00:14:48,080
savings, which is good.

365
00:14:48,080 --> 00:14:49,460
It means we're getting
that in addition

366
00:14:49,460 --> 00:14:51,430
to what we're getting.

367
00:14:51,430 --> 00:14:54,540
Now as a form of comparison,
just to look at things a

368
00:14:54,540 --> 00:14:58,050
different way, if we actually
compressed each of the DXT

369
00:14:58,050 --> 00:15:02,590
images individually by zipping
them, we get about 5.1 megs.

370
00:15:02,590 --> 00:15:04,230
Now notice this is higher.

371
00:15:04,230 --> 00:15:07,120
The reason for this is because
we're adding all of the

372
00:15:07,120 --> 00:15:08,650
additional zip data overhead.

373
00:15:08,650 --> 00:15:12,090
So we're adding the headers,
the span chunks, all these

374
00:15:12,090 --> 00:15:13,610
other fun things.

375
00:15:13,610 --> 00:15:16,130
And there's some other
interesting nuances of why

376
00:15:16,130 --> 00:15:18,780
that footprint is higher that
I'll get to a little bit later

377
00:15:18,780 --> 00:15:19,770
in the talk.

378
00:15:19,770 --> 00:15:22,540
The goal for this talk, though,
is to beat this.

379
00:15:22,540 --> 00:15:26,900
We want our compression
algorithm to match what zip is

380
00:15:26,900 --> 00:15:30,130
creating, so that our in memory
version is the same

381
00:15:30,130 --> 00:15:32,160
footprint size as what
we would expect

382
00:15:32,160 --> 00:15:33,190
our of our zip file.

383
00:15:33,190 --> 00:15:34,740
So the goal is can
we get to that?

384
00:15:34,740 --> 00:15:38,680
Can we beat 36.83%
for our hyper

385
00:15:38,680 --> 00:15:40,750
compressed format in memory?

386
00:15:40,750 --> 00:15:42,640
To do this, I'm going to present
you a bag of tricks.

387
00:15:42,640 --> 00:15:45,830
And these are all separate
items. I'm going to talk about

388
00:15:45,830 --> 00:15:49,020
how each one of these items
changes the footprint size,

389
00:15:49,020 --> 00:15:51,260
the memory size, and then
present them individually so

390
00:15:51,260 --> 00:15:53,910
that you can mix and match them,
depending on your input.

391
00:15:53,910 --> 00:15:56,230
So for lossless algorithms, so
these are algorithms that do

392
00:15:56,230 --> 00:15:59,100
not introduce any new errors
into the image.

393
00:15:59,100 --> 00:16:01,190
The first one we're going to
cover is de-interleaving.

394
00:16:01,190 --> 00:16:02,320
I'll talk about that
in a minute.

395
00:16:02,320 --> 00:16:04,120
The next one, of course,
is Huffman compression.

396
00:16:04,120 --> 00:16:06,150
Everyone in here, if you have
a basic computer science

397
00:16:06,150 --> 00:16:08,280
degree, you should have written
a Huffman before.

398
00:16:08,280 --> 00:16:09,970
The next one is delta
encoding.

399
00:16:09,970 --> 00:16:12,210
This is sort of a tricky
thing that not a lot of

400
00:16:12,210 --> 00:16:13,230
people are aware of.

401
00:16:13,230 --> 00:16:14,890
And then, finally, we're going
to talk about codebooks and

402
00:16:14,890 --> 00:16:17,000
what codebooks are and
how you can use them.

403
00:16:17,000 --> 00:16:20,060
Now to really get some amazing
results, we're going to also

404
00:16:20,060 --> 00:16:23,000
talk about one specific
lossy technique.

405
00:16:23,000 --> 00:16:25,840
And this is actually going to
be an expanding blocks, aka

406
00:16:25,840 --> 00:16:28,490
region of interest-based
finding pattern.

407
00:16:28,490 --> 00:16:30,450
And I think you'll love what
we come out of that.

408
00:16:30,450 --> 00:16:31,755
But we're going to save
until the end because

409
00:16:31,755 --> 00:16:33,480
it's the big review.

410
00:16:33,480 --> 00:16:35,310
So let's start the top
with de-interleaving.

411
00:16:35,310 --> 00:16:38,120
Again, as mentioned before, here
is a standard DXT block.

412
00:16:38,120 --> 00:16:40,720
You've got your high-color and
your low-color and your 4x4

413
00:16:40,720 --> 00:16:42,070
set of selector bits.

414
00:16:42,070 --> 00:16:46,150
Now in memory, in GPU memory,
CPU memory and on-disk in a

415
00:16:46,150 --> 00:16:49,540
DDS format, effectively,
you concatenate

416
00:16:49,540 --> 00:16:50,810
these blocks in memory.

417
00:16:50,810 --> 00:16:53,330
So you've got your high-color,
low-color selector bits,

418
00:16:53,330 --> 00:16:55,540
high-color, low-color selector
bits, high-color, low-color

419
00:16:55,540 --> 00:16:56,540
selector bits.

420
00:16:56,540 --> 00:16:59,010
The ideal with de-interleaving
is what if we actually

421
00:16:59,010 --> 00:17:00,130
separated those out?

422
00:17:00,130 --> 00:17:03,160
What if we put all of our high
and low colors together, and

423
00:17:03,160 --> 00:17:05,990
then put all of the selector
bits together?

424
00:17:05,990 --> 00:17:10,099
The intent here is that, for
zip compression, which is

425
00:17:10,099 --> 00:17:15,140
basically a modified version
of LZW, what they create is

426
00:17:15,140 --> 00:17:16,400
actually a window.

427
00:17:16,400 --> 00:17:18,280
And they say, we're
going to analyze.

428
00:17:18,280 --> 00:17:20,660
And we're going to deflate--

429
00:17:20,660 --> 00:17:22,160
it's another compression
algorithm-- we're going to

430
00:17:22,160 --> 00:17:24,750
deflate everything within this
window, and then move on to

431
00:17:24,750 --> 00:17:27,119
the next window and deflate
everything in that window.

432
00:17:27,119 --> 00:17:29,590
So by de-interleaving our
data, putting all of our

433
00:17:29,590 --> 00:17:31,720
colors and then all of our
selector bits in, effectively,

434
00:17:31,720 --> 00:17:35,700
two separate bins, what we're
trying to do here is optimize

435
00:17:35,700 --> 00:17:38,240
for the zip compression
format.

436
00:17:38,240 --> 00:17:41,160
So we're trying to pack as much
duplicate data into each

437
00:17:41,160 --> 00:17:44,320
one of those sliding windows
as we can to, hopefully,

438
00:17:44,320 --> 00:17:46,570
increase the amount of
compression we get.

439
00:17:46,570 --> 00:17:48,840
So let's check out what
that looks like.

440
00:17:48,840 --> 00:17:51,990
So with DXTi, I've got each
one of these techniques

441
00:17:51,990 --> 00:17:55,050
segmented out with a different
enunciation at the top.

442
00:17:55,050 --> 00:17:58,440
So de-interleaving, so again
the DXT1 original footprint

443
00:17:58,440 --> 00:18:00,350
was 7.63 megs.

444
00:18:00,350 --> 00:18:03,500
The interleaved version
is 7.63 megs.

445
00:18:03,500 --> 00:18:05,470
Now again, no compression
has occurred here.

446
00:18:05,470 --> 00:18:09,260
All we've done here is just
moved around the selector bits

447
00:18:09,260 --> 00:18:12,380
and color bits so that they're
homogeneous in

448
00:18:12,380 --> 00:18:13,800
their sorting here.

449
00:18:13,800 --> 00:18:15,910
Now the zip here is fantastic.

450
00:18:15,910 --> 00:18:18,250
We've actually went
to 43% savings.

451
00:18:18,250 --> 00:18:21,070
Now if you remember, this is
basically about a 10%, I

452
00:18:21,070 --> 00:18:21,850
think, savings here.

453
00:18:21,850 --> 00:18:22,640
This is fantastic.

454
00:18:22,640 --> 00:18:23,780
This is the end result.

455
00:18:23,780 --> 00:18:28,110
So simply by de-interleaving
your data, you get about 10%

456
00:18:28,110 --> 00:18:31,040
savings there, which is good.

457
00:18:31,040 --> 00:18:33,760
A 10%, 15% savings there,
which is fantastic.

458
00:18:33,760 --> 00:18:36,010
And this is optimized, so
this is exactly what

459
00:18:36,010 --> 00:18:37,400
we wanted to see.

460
00:18:37,400 --> 00:18:38,080
So that's good.

461
00:18:38,080 --> 00:18:40,480
The next question is, can we
reduce the amount of unique

462
00:18:40,480 --> 00:18:43,010
data in the sliding window
even further?

463
00:18:43,010 --> 00:18:46,040
Such that zip can compress
it even further?

464
00:18:46,040 --> 00:18:48,140
To do that, we're going to look
at a common compression

465
00:18:48,140 --> 00:18:49,850
technique called Huffman.

466
00:18:49,850 --> 00:18:52,830
Now Huffman is effectively a
dictionary creation system.

467
00:18:52,830 --> 00:18:56,530
So dictionary systems all work
in a very specific way.

468
00:18:56,530 --> 00:19:00,260
Effectively, they create a
dictionary symbol of unique

469
00:19:00,260 --> 00:19:02,200
values that exist
in the stream.

470
00:19:02,200 --> 00:19:05,210
And then they replace the
instance of that symbol in the

471
00:19:05,210 --> 00:19:08,430
data stream with a
minimal bit-code

472
00:19:08,430 --> 00:19:09,740
representation of that.

473
00:19:09,740 --> 00:19:12,410
So for instance, Morse code is
a perfect example of that.

474
00:19:12,410 --> 00:19:14,570
They analyzed the English
language.

475
00:19:14,570 --> 00:19:16,600
And they said, E is the most
common character in the

476
00:19:16,600 --> 00:19:17,570
English language.

477
00:19:17,570 --> 00:19:19,140
Therefore, we're going
to represent E

478
00:19:19,140 --> 00:19:20,390
with a single beep.

479
00:19:20,390 --> 00:19:23,190
So then, any time you're doing
scans and whatnot, E is

480
00:19:23,190 --> 00:19:25,740
represented in the least
value possible.

481
00:19:25,740 --> 00:19:29,070
For an example of this, let's
say we've got this string,

482
00:19:29,070 --> 00:19:32,840
four A's, two B's and a C, which
is, with 8-bit ASCII is

483
00:19:32,840 --> 00:19:35,970
about 56 bits, right?

484
00:19:35,970 --> 00:19:39,870
In Huffman, if you just looked
at the compression stream, A

485
00:19:39,870 --> 00:19:41,110
is going to be the most
common symbol.

486
00:19:41,110 --> 00:19:44,480
So it's only going to
get the zero bit.

487
00:19:44,480 --> 00:19:46,020
B is the second most symbol,
so it's going

488
00:19:46,020 --> 00:19:46,720
to get the one bits.

489
00:19:46,720 --> 00:19:48,480
And then C is the third
symbol here, so it's

490
00:19:48,480 --> 00:19:49,460
going to get two bits.

491
00:19:49,460 --> 00:19:51,240
So we go from 56
bits to 8 bits.

492
00:19:51,240 --> 00:19:53,010
And I there's some of you in the
audience who are saying,

493
00:19:53,010 --> 00:19:54,670
actually, that should be
a little bit different,

494
00:19:54,670 --> 00:19:56,960
depending on which Huffman
version you use.

495
00:19:56,960 --> 00:19:58,310
We're going to use this
is an illustration.

496
00:19:58,310 --> 00:20:01,980
Please consult Wikipedia for
all the different versions

497
00:20:01,980 --> 00:20:03,010
that you can find
about Huffman.

498
00:20:03,010 --> 00:20:05,750
It's actually staggering how
many different versions of

499
00:20:05,750 --> 00:20:06,470
Huffman are out there.

500
00:20:06,470 --> 00:20:09,970
So make sure you're tuning
that as you see fit.

501
00:20:09,970 --> 00:20:11,230
So let's look at what
this did to us.

502
00:20:11,230 --> 00:20:12,830
So we've got DXTih.

503
00:20:12,830 --> 00:20:16,210
So this is de-interleaving
plus Huffman.

504
00:20:16,210 --> 00:20:18,640
And also, I want to point out
at the top here, I'm noting

505
00:20:18,640 --> 00:20:22,970
that Huffman and the usage of
Huffman requires you to define

506
00:20:22,970 --> 00:20:24,170
a symbol size.

507
00:20:24,170 --> 00:20:29,150
Now for this test here, I've
actually chosen 16-bit symbols

508
00:20:29,150 --> 00:20:32,800
for my colors and 8-bit symbols
for my selectors.

509
00:20:32,800 --> 00:20:35,520
And this has a big bearing on
how things work, right?

510
00:20:35,520 --> 00:20:38,650
If I were to use 16-bit for
selectors, I get less value.

511
00:20:38,650 --> 00:20:41,450
If I use 8-bits for colors,
I get less compression, et

512
00:20:41,450 --> 00:20:41,960
cetera, et cetera.

513
00:20:41,960 --> 00:20:43,280
So let's take a look at this.

514
00:20:43,280 --> 00:20:46,330
So the standard again,
7.63 megs for DXT.

515
00:20:46,330 --> 00:20:50,140
DXTdih, so de-interleaved
with Huffman

516
00:20:50,140 --> 00:20:51,500
encoding, is about 40%.

517
00:20:51,500 --> 00:20:53,680
So this is not as good
as we saw before.

518
00:20:53,680 --> 00:20:57,490
We're introducing a little bit
of noise into the stream here.

519
00:20:57,490 --> 00:20:59,640
When you add in the
zip, though,

520
00:20:59,640 --> 00:21:02,690
you get back to about--

521
00:21:02,690 --> 00:21:05,290
we only get about 1% savings
by adding the Huffman.

522
00:21:05,290 --> 00:21:07,670
So we didn't get much benefit.

523
00:21:07,670 --> 00:21:10,340
One important note here is
though, that again, this is

524
00:21:10,340 --> 00:21:12,760
optimized for your
colors up top.

525
00:21:12,760 --> 00:21:16,110
And basically, what
I was doing was

526
00:21:16,110 --> 00:21:17,640
optimizing for my set.

527
00:21:17,640 --> 00:21:19,840
Now if you take that into
account, what you should

528
00:21:19,840 --> 00:21:25,090
really be doing is finding
the optimal bit encoding.

529
00:21:25,090 --> 00:21:26,980
For what I did this test,
I just used a

530
00:21:26,980 --> 00:21:28,225
8-bit row, 4x4 block.

531
00:21:28,225 --> 00:21:32,680
So I just took the 4x4 and just
made that one symbol.

532
00:21:32,680 --> 00:21:34,930
In reality, you should scan your
data input to determine

533
00:21:34,930 --> 00:21:37,130
what the optimal combination
provides.

534
00:21:37,130 --> 00:21:38,230
So the blog, [? Seibey ?]

535
00:21:38,230 --> 00:21:39,020
Live--

536
00:21:39,020 --> 00:21:41,160
you can see the link at the
bottom of the slide there--

537
00:21:41,160 --> 00:21:44,720
did a similar test where they
basically figured out what the

538
00:21:44,720 --> 00:21:46,760
multiple pairings were to
determine the best symbol

539
00:21:46,760 --> 00:21:49,880
orientation for this exact
type of compression.

540
00:21:49,880 --> 00:21:51,730
In their particular data set
that they were using, they

541
00:21:51,730 --> 00:21:54,650
found that 2x2 blocks of
selector bits produced the

542
00:21:54,650 --> 00:21:55,670
highest number of duplicate
symbols.

543
00:21:55,670 --> 00:22:00,010
You can see that as the high
value there in the frequency.

544
00:22:00,010 --> 00:22:01,780
In reality, before you actually
compress your data,

545
00:22:01,780 --> 00:22:02,930
you should be doing this.

546
00:22:02,930 --> 00:22:05,060
You should be going through, per
texture, finding out what

547
00:22:05,060 --> 00:22:07,590
the optimal solution is
and compressing that.

548
00:22:07,590 --> 00:22:10,430
And in reality, you should all
have build farms in here.

549
00:22:10,430 --> 00:22:12,340
This is the modern gaming age.

550
00:22:12,340 --> 00:22:14,390
You should have some ability to
kick off a nightly build to

551
00:22:14,390 --> 00:22:16,280
a farm of servers that
bake all your data.

552
00:22:16,280 --> 00:22:19,300
So the processing involved
with generating this data

553
00:22:19,300 --> 00:22:22,490
should not be that
important to you.

554
00:22:22,490 --> 00:22:24,550
So Huffman provided great
reduction for the base stream,

555
00:22:24,550 --> 00:22:26,850
but didn't provide too much
help with our zip

556
00:22:26,850 --> 00:22:27,540
compression, right?

557
00:22:27,540 --> 00:22:29,980
So we got our base data
compressed very nicely.

558
00:22:29,980 --> 00:22:32,730
But when you put zip on top
of it, we didn't see

559
00:22:32,730 --> 00:22:33,410
too much of a help.

560
00:22:33,410 --> 00:22:34,260
Now this is OK.

561
00:22:34,260 --> 00:22:35,580
This is what we want, right?

562
00:22:35,580 --> 00:22:38,790
We want that base, non-zipped,
compressed data to start

563
00:22:38,790 --> 00:22:41,960
getting lower so
it matches zip.

564
00:22:41,960 --> 00:22:44,140
So delta encoding, let's
talk about this.

565
00:22:44,140 --> 00:22:46,830
Delta encoding will effectively
encode a stream of

566
00:22:46,830 --> 00:22:50,010
data by replacing each element
in the stream with a value

567
00:22:50,010 --> 00:22:52,720
that represents the data
from the previous sum.

568
00:22:52,720 --> 00:22:56,590
The goal here is to reduce the
dynamic range of symbols that

569
00:22:56,590 --> 00:22:58,650
could exist in the
stream, right?

570
00:22:58,650 --> 00:23:01,850
And so Huffman encoding will
basically replace the symbols

571
00:23:01,850 --> 00:23:04,780
in the stream, based upon
frequency, with minimal bit

572
00:23:04,780 --> 00:23:05,900
representations.

573
00:23:05,900 --> 00:23:09,210
Delta encoding will scan through
the string before that

574
00:23:09,210 --> 00:23:13,640
and, basically, truncate,
concatenate and quantize to

575
00:23:13,640 --> 00:23:17,340
create more of the low-band,
unique signals.

576
00:23:17,340 --> 00:23:18,680
A perfect example
of this this.

577
00:23:18,680 --> 00:23:22,520
So we've got this string here,
or this set of numbers, 155,

578
00:23:22,520 --> 00:23:24,050
156, yada, yada, yada.

579
00:23:24,050 --> 00:23:27,420
Now if we just threw this at
Huffman, the only symbols that

580
00:23:27,420 --> 00:23:29,250
we would find here as duplicates
are basically, the

581
00:23:29,250 --> 00:23:31,630
157's right there.

582
00:23:31,630 --> 00:23:33,870
Huffman would say that's the
most frequent symbol, that

583
00:23:33,870 --> 00:23:35,310
would get the lowest bit-set.

584
00:23:35,310 --> 00:23:38,140
Now if we run this through delta
encoding first, what

585
00:23:38,140 --> 00:23:43,180
happens is we create 155,
1, 1, 0, 0, 64, 1, 3.

586
00:23:43,180 --> 00:23:45,840
So basically, what you're seeing
here is that what we do

587
00:23:45,840 --> 00:23:47,610
is we say, the first
symbol is 155.

588
00:23:47,610 --> 00:23:51,010
The next symbol is add one.

589
00:23:51,010 --> 00:23:53,610
The next symbol to that is
add one to that value.

590
00:23:53,610 --> 00:23:56,660
The next symbol to that is add
zero, add zero, add 64, add

591
00:23:56,660 --> 00:23:57,490
one, add three.

592
00:23:57,490 --> 00:24:00,470
What we've done here,
effectively, is increased the

593
00:24:00,470 --> 00:24:05,500
number of duplicate symbols in
the stream by encoding the

594
00:24:05,500 --> 00:24:08,230
delta between the values, as
opposed to the actual values

595
00:24:08,230 --> 00:24:08,560
themselves.

596
00:24:08,560 --> 00:24:12,470
So this decreases the dynamic
range of the stream.

597
00:24:12,470 --> 00:24:14,150
So let's see how this
did for us.

598
00:24:14,150 --> 00:24:18,090
So DXTihd with delta encoding.

599
00:24:18,090 --> 00:24:22,660
Base, again, 7.63 megs,
non-zipped.

600
00:24:22,660 --> 00:24:25,410
So just taking the DXT data
itself about 41%.

601
00:24:25,410 --> 00:24:27,720
That's good, right?

602
00:24:27,720 --> 00:24:29,940
And about 45% reduction for
the zipped version.

603
00:24:29,940 --> 00:24:31,590
And that's great, right?

604
00:24:31,590 --> 00:24:33,790
The problem here is is we're
starting to see this plateau.

605
00:24:33,790 --> 00:24:35,710
We're not getting
huge results by

606
00:24:35,710 --> 00:24:38,930
squeezing this stream itself.

607
00:24:38,930 --> 00:24:41,220
It might be worth looking at
a different direction.

608
00:24:41,220 --> 00:24:45,070
And with that, we take
a look at codebooks.

609
00:24:45,070 --> 00:24:48,380
Effectively, we create a
codebook by scanning the DXT

610
00:24:48,380 --> 00:24:51,370
image and create a codebook
that lists all the unique

611
00:24:51,370 --> 00:24:53,440
symbols, sort of like the
dictionary method.

612
00:24:53,440 --> 00:24:57,540
And then we delta encode
those unique values.

613
00:24:57,540 --> 00:24:59,370
Then in the DST block stream--

614
00:24:59,370 --> 00:25:00,750
so basically, we create
all the this

615
00:25:00,750 --> 00:25:01,640
unique stream of colors.

616
00:25:01,640 --> 00:25:02,440
We delta encode that.

617
00:25:02,440 --> 00:25:05,680
And then, in the block stream,
we go back and we list a

618
00:25:05,680 --> 00:25:09,780
256-bit index into
this codebook.

619
00:25:09,780 --> 00:25:12,620
Now, of course, some of you in
the audience should say, hey,

620
00:25:12,620 --> 00:25:13,020
wait a minute.

621
00:25:13,020 --> 00:25:14,820
What if you've got more
than 256 colors?

622
00:25:14,820 --> 00:25:15,760
That's OK.

623
00:25:15,760 --> 00:25:19,990
What we actually do is we
create a sliding window

624
00:25:19,990 --> 00:25:22,870
approach to ensure that you'll
always have a 256-bit index.

625
00:25:22,870 --> 00:25:26,090
So basically, what we do is, for
the entire list of colors

626
00:25:26,090 --> 00:25:28,290
in your codebook, you basically
say, I'm going to

627
00:25:28,290 --> 00:25:31,570
create a window of the
first 256 values.

628
00:25:31,570 --> 00:25:34,995
Those will be numbered zero to
255, then the next set of 256,

629
00:25:34,995 --> 00:25:38,050
and then the next set of 256,
and the next set of 256.

630
00:25:38,050 --> 00:25:41,880
And then, in your actual index
data stream, you're going to

631
00:25:41,880 --> 00:25:45,050
make a reference into the proper
bin of 256 values.

632
00:25:45,050 --> 00:25:46,680
And there's some interesting
ways to make

633
00:25:46,680 --> 00:25:47,960
sure that this works.

634
00:25:47,960 --> 00:25:49,240
There's lots of stuff
on Wikipedia.

635
00:25:49,240 --> 00:25:51,490
I would highly recommend you go
take a look at that because

636
00:25:51,490 --> 00:25:55,660
I don't have time today to
explain the specific nuances.

637
00:25:55,660 --> 00:25:59,690
So with that, we actually got
some different results.

638
00:25:59,690 --> 00:26:00,560
It gave some really good ones.

639
00:26:00,560 --> 00:26:04,220
So first off, we're at 46%
without zip compression--

640
00:26:04,220 --> 00:26:05,750
that's pretty amazing--

641
00:26:05,750 --> 00:26:08,010
and 49% with zip compression.

642
00:26:08,010 --> 00:26:11,520
So we're still sort of chasing
the dragon of zip compression

643
00:26:11,520 --> 00:26:13,570
and trying to track that down.

644
00:26:13,570 --> 00:26:16,280
But again, we're not seeing the
huge jumps in savings that

645
00:26:16,280 --> 00:26:16,880
we're expecting.

646
00:26:16,880 --> 00:26:18,360
Or we haven't broken that 50%.

647
00:26:18,360 --> 00:26:22,110
And there's got to be a
better way to do that.

648
00:26:22,110 --> 00:26:24,350
And with that, we're going to
talk about expanding blocks.

649
00:26:24,350 --> 00:26:27,010
Now this is a lossy technique,
but I think you're going to

650
00:26:27,010 --> 00:26:27,720
like the results.

651
00:26:27,720 --> 00:26:29,470
And so that's why I want
to bring it up today.

652
00:26:29,470 --> 00:26:33,620
So the point here is that
adjacent DXT cells often share

653
00:26:33,620 --> 00:26:35,060
color profiles.

654
00:26:35,060 --> 00:26:38,840
So 4x4 and the block next to it
and the block below it and

655
00:26:38,840 --> 00:26:41,660
block in an adjacent direction
to it, they're--

656
00:26:41,660 --> 00:26:44,980
the way that artists and
pictures work, the number of

657
00:26:44,980 --> 00:26:47,260
pixels we have on a screen,
chances are, those colors are

658
00:26:47,260 --> 00:26:48,130
all correlated.

659
00:26:48,130 --> 00:26:51,210
So what if we used an 8x8
cell instead of a 4x4?

660
00:26:51,210 --> 00:26:52,210
Just hypothetically?

661
00:26:52,210 --> 00:26:54,440
We stored one high- and
low-color per 8x8.

662
00:26:54,440 --> 00:26:58,610
And then we stored six
64 two-bit selectors.

663
00:26:58,610 --> 00:27:00,050
What would this do
to our image?

664
00:27:00,050 --> 00:27:02,190
What would the results be?

665
00:27:02,190 --> 00:27:04,010
Well, let's take
a look at that.

666
00:27:04,010 --> 00:27:06,260
What you're seeing on the screen
here is two images.

667
00:27:06,260 --> 00:27:08,270
The left is the original.

668
00:27:08,270 --> 00:27:13,920
The right has been compressed
with 8x8 blocks instead of 4x4

669
00:27:13,920 --> 00:27:14,710
DXT blocks.

670
00:27:14,710 --> 00:27:17,090
From this distance, you
really shouldn't

671
00:27:17,090 --> 00:27:18,980
see much of a problem.

672
00:27:18,980 --> 00:27:22,240
You can see a little bit of
banding, a little bit

673
00:27:22,240 --> 00:27:25,270
quantization, maybe some blocks
for the full image.

674
00:27:25,270 --> 00:27:27,620
But in general, you shouldn't
see anything wrong.

675
00:27:27,620 --> 00:27:29,770
It's not until we zoom in that
you actually start seeing

676
00:27:29,770 --> 00:27:31,530
problems. And even here, it's
pretty hard to notice.

677
00:27:31,530 --> 00:27:33,200
I want to point out where
the errors are here.

678
00:27:33,200 --> 00:27:35,300
If you look at the top of the
bird's beak, you can see some

679
00:27:35,300 --> 00:27:36,920
blocking artifacts there.

680
00:27:36,920 --> 00:27:40,250
You can see some graininess at
the bottom of the background.

681
00:27:40,250 --> 00:27:42,630
You see the quantization of
the steps of colors there.

682
00:27:42,630 --> 00:27:44,480
But in general, this
image performs

683
00:27:44,480 --> 00:27:47,000
very well at 8x8 blocks.

684
00:27:47,000 --> 00:27:48,710
Here's another example.

685
00:27:48,710 --> 00:27:50,980
This is from the Kodak
suite as well.

686
00:27:50,980 --> 00:27:52,670
At this distance, you
really shouldn't

687
00:27:52,670 --> 00:27:54,520
see much of a problem.

688
00:27:54,520 --> 00:27:57,700
Again, this is 8x8 block
compressed, right?

689
00:27:57,700 --> 00:28:00,010
We actually see less problems
than the parrot because

690
00:28:00,010 --> 00:28:02,170
there's a lot less smooth
gradients in this image.

691
00:28:02,170 --> 00:28:05,715
And there's more general
sub-noise, which helps hide

692
00:28:05,715 --> 00:28:07,150
the DXT artifacts.

693
00:28:07,150 --> 00:28:09,230
Now when we zoom in on her
chin, you actually start

694
00:28:09,230 --> 00:28:11,850
seeing the problems
pretty badly.

695
00:28:11,850 --> 00:28:14,700
Here, you actually see the block
and the color truncation

696
00:28:14,700 --> 00:28:16,430
on her necklace as well as some

697
00:28:16,430 --> 00:28:18,430
clipping on her chin there.

698
00:28:18,430 --> 00:28:20,250
You can actually see these
problems. But again, you have

699
00:28:20,250 --> 00:28:21,860
to zoom in all the way
to look at these.

700
00:28:21,860 --> 00:28:24,210
Now if someone's chasing you
around with a rocket launcher

701
00:28:24,210 --> 00:28:26,960
at 60 frames a second, you're
probably not going to care.

702
00:28:26,960 --> 00:28:29,070
Here's a trick question.

703
00:28:29,070 --> 00:28:31,390
Which one's compressed?

704
00:28:31,390 --> 00:28:33,160
Now this is an in-game
texture.

705
00:28:33,160 --> 00:28:34,290
So this is something
you would see.

706
00:28:34,290 --> 00:28:36,200
This is something that artists
would generate to put in your

707
00:28:36,200 --> 00:28:36,830
environment.

708
00:28:36,830 --> 00:28:39,220
Which one of these
is compressed?

709
00:28:39,220 --> 00:28:41,430
Well, of course, with this
normal trend, the one on the

710
00:28:41,430 --> 00:28:43,850
left is the original and the
one on the right is the

711
00:28:43,850 --> 00:28:45,290
compressed version.

712
00:28:45,290 --> 00:28:47,330
But could you really tell,
especially with

713
00:28:47,330 --> 00:28:48,360
this type of image?

714
00:28:48,360 --> 00:28:51,790
Even when we zoom in, can you
tell which one of these has

715
00:28:51,790 --> 00:28:55,390
been quantized 8x8 blocks
and which hasn't?

716
00:28:55,390 --> 00:28:56,850
So let's look at the
results here.

717
00:28:56,850 --> 00:28:58,800
This is awesome.

718
00:28:58,800 --> 00:29:01,730
With 8x8 blocks explicitly--
so we're using 8x8 blocks

719
00:29:01,730 --> 00:29:02,340
everywhere--

720
00:29:02,340 --> 00:29:05,600
we actually end up with our
zipped and our non-zipped

721
00:29:05,600 --> 00:29:11,100
memory footprint at exactly the
same size, 67.7% savings,

722
00:29:11,100 --> 00:29:14,050
taking the original 37
megs down to 2.46.

723
00:29:14,050 --> 00:29:16,430
This is exactly what we were
looking for in this research.

724
00:29:16,430 --> 00:29:18,990
We were trying to find a way
to make the uncompressed

725
00:29:18,990 --> 00:29:21,700
version in memory match what
we get out of a zip.

726
00:29:21,700 --> 00:29:24,050
And we did it by combining
Huffman encoding,

727
00:29:24,050 --> 00:29:27,230
deinterlacing, and
doing 8x8 blocks.

728
00:29:27,230 --> 00:29:28,270
It's fantastic.

729
00:29:28,270 --> 00:29:30,420
Oh, as well as codebooks.

730
00:29:30,420 --> 00:29:32,280
So now let's talk about
everyone else in here.

731
00:29:32,280 --> 00:29:33,340
So you remember that
triangle, right?

732
00:29:33,340 --> 00:29:34,360
So everyone else in
here is say well,

733
00:29:34,360 --> 00:29:34,910
what about the triangle?

734
00:29:34,910 --> 00:29:38,230
We've gotten speed, and
we've gotten quality--

735
00:29:38,230 --> 00:29:40,130
or we've gotten compression size
and we've gotten quality.

736
00:29:40,130 --> 00:29:41,850
Now let's talk about speed.

737
00:29:41,850 --> 00:29:45,310
So for this style I just
showed you, which is

738
00:29:45,310 --> 00:29:47,750
de-interleaving, Huffman
and [? styles ?]

739
00:29:47,750 --> 00:29:53,230
delta encoding of codebooks with
8x8 blocks, using CS101

740
00:29:53,230 --> 00:29:54,150
Huffman and delta encoding.

741
00:29:54,150 --> 00:29:55,690
So I didn't do anything
special here.

742
00:29:55,690 --> 00:29:57,620
I didn't spend any time
optimizing my Huffman

743
00:29:57,620 --> 00:29:59,430
decompression or my
delta encoding.

744
00:29:59,430 --> 00:30:01,210
I just threw it in there.

745
00:30:01,210 --> 00:30:06,530
With about 67.8% savings,
I was getting about 73

746
00:30:06,530 --> 00:30:09,020
megapixels a second
on a single core.

747
00:30:09,020 --> 00:30:11,150
So this was one core doing this
compression, which is

748
00:30:11,150 --> 00:30:12,040
fantastic, right?

749
00:30:12,040 --> 00:30:14,400
If you go to two cores,
that's 114.

750
00:30:14,400 --> 00:30:16,610
Three cores, you start getting
into some of the numbers that

751
00:30:16,610 --> 00:30:18,220
you start seeing here.

752
00:30:18,220 --> 00:30:22,000
The cool thing here is that
that's 1.32 bits per pixel.

753
00:30:22,000 --> 00:30:22,670
That's fantastic.

754
00:30:22,670 --> 00:30:24,950
DXT is four bits per pixel.

755
00:30:24,950 --> 00:30:27,430
And with this technique, we
get it down to about 1.32.

756
00:30:27,430 --> 00:30:29,220
That's awesome.

757
00:30:29,220 --> 00:30:31,520
So here's the big
reveal, right?

758
00:30:31,520 --> 00:30:33,340
This is the entire thing.

759
00:30:33,340 --> 00:30:35,020
Why stop at 8x8 blocks?

760
00:30:35,020 --> 00:30:39,590
What if we actually modified our
image heuristics to scan,

761
00:30:39,590 --> 00:30:42,200
based upon the frequency of the
image, and use somewhere

762
00:30:42,200 --> 00:30:46,040
between four and 16x16 blocks?

763
00:30:46,040 --> 00:30:47,020
We do the same thing.

764
00:30:47,020 --> 00:30:49,220
We de-interleave, delta encode
and Huffman encode them.

765
00:30:49,220 --> 00:30:52,930
With this, we get about 80%
reduction at 93 megapixels a

766
00:30:52,930 --> 00:30:55,150
second for diffuse
textures only.

767
00:30:55,150 --> 00:30:58,760
And that gets us down to
0.8 bits per pixel.

768
00:30:58,760 --> 00:31:00,170
That is amazing.

769
00:31:00,170 --> 00:31:01,970
For one core, you're telling
me I can get

770
00:31:01,970 --> 00:31:03,210
0.8 bits per pixel.

771
00:31:03,210 --> 00:31:05,700
And I can decompress 93
megatexles a second.

772
00:31:05,700 --> 00:31:06,420
That's awesome.

773
00:31:06,420 --> 00:31:07,805
This is fantastic, right?

774
00:31:07,805 --> 00:31:10,620
Of course, you have to do all
this stuff offline and

775
00:31:10,620 --> 00:31:11,560
compress it properly.

776
00:31:11,560 --> 00:31:13,300
But that's a separate thing.

777
00:31:13,300 --> 00:31:15,850
So the bigger reveal is that,
once I came to all this

778
00:31:15,850 --> 00:31:17,560
research and got all this stuff
done a couple of days

779
00:31:17,560 --> 00:31:20,840
later, a very young, friendly
gentleman named Rich

780
00:31:20,840 --> 00:31:23,230
Geldreich, who I've worked with
at my previous companies,

781
00:31:23,230 --> 00:31:26,110
decided to put his
CRUNCH codec--

782
00:31:26,110 --> 00:31:28,360
which is his version of texture
compression-- online.

783
00:31:28,360 --> 00:31:30,990
It's licensed very generally.

784
00:31:30,990 --> 00:31:32,630
You should go check it out.

785
00:31:32,630 --> 00:31:34,800
Click that link there
and go check it out.

786
00:31:34,800 --> 00:31:38,760
He does about the same, about
between 0.8 and 1.25 bits per

787
00:31:38,760 --> 00:31:39,740
pixel for his texture

788
00:31:39,740 --> 00:31:42,580
compression for diffuse textures.

789
00:31:42,580 --> 00:31:45,320
For normal textures, he gets
about 1.75 to 2 bits per

790
00:31:45,320 --> 00:31:47,200
pixel, which is still
really amazing.

791
00:31:47,200 --> 00:31:49,490
This is cutting edge for game
textures, without having to go

792
00:31:49,490 --> 00:31:53,230
through a full JPEG
2000 stack.

793
00:31:53,230 --> 00:31:55,110
The thing here that is different
is that he gets

794
00:31:55,110 --> 00:31:59,480
about 256 megatexles a second
decompression, single core.

795
00:31:59,480 --> 00:32:00,820
I was only able to get 93.

796
00:32:00,820 --> 00:32:03,440
So he's got some dark
voodoo in there that

797
00:32:03,440 --> 00:32:04,930
really changes the game.

798
00:32:04,930 --> 00:32:08,280
Again, his stuff is up
on code.google.com.

799
00:32:08,280 --> 00:32:08,920
You can get it.

800
00:32:08,920 --> 00:32:10,520
You can use it in your game.

801
00:32:10,520 --> 00:32:11,990
I forget what the
licenses are.

802
00:32:11,990 --> 00:32:14,120
But you should definitely
go check that out.

803
00:32:14,120 --> 00:32:17,510
So take aways, it's actually
easy to get really simple

804
00:32:17,510 --> 00:32:19,740
savings with really simple
algorithms, if you just

805
00:32:19,740 --> 00:32:21,290
concatenate them in
the right way.

806
00:32:21,290 --> 00:32:24,030
Most importantly, your mileage
may vary for texture types.

807
00:32:24,030 --> 00:32:26,510
What works for an ambient
occlusion texture really well

808
00:32:26,510 --> 00:32:28,300
may not work for
a DXT texture.

809
00:32:28,300 --> 00:32:31,810
So it's important to optimize
and combine each of these

810
00:32:31,810 --> 00:32:33,870
things to put them together
to figure out what texture

811
00:32:33,870 --> 00:32:35,190
profile works best for you.

812
00:32:35,190 --> 00:32:37,700
And it's best to spend offline
doing your compression.

813
00:32:37,700 --> 00:32:40,170
You want to spend all of your
time online, while the game is

814
00:32:40,170 --> 00:32:43,360
running, while you're fighting
for that 16 millisecond frame

815
00:32:43,360 --> 00:32:46,230
to get you 60 Hertz, you want
all of that time to go towards

816
00:32:46,230 --> 00:32:46,710
what you need.

817
00:32:46,710 --> 00:32:48,910
You don't want to be spending
a lot of time decompressing

818
00:32:48,910 --> 00:32:49,460
and compressing.

819
00:32:49,460 --> 00:32:51,940
So do all of your compression
offline.

820
00:32:51,940 --> 00:32:53,680
And do your decompression
online.

821
00:32:53,680 --> 00:32:54,760
So that's it for me.

822
00:32:54,760 --> 00:32:57,190
Big thanks to Rich Geldreich,
John Brooks and Ken Adams for

823
00:32:57,190 --> 00:32:58,250
their help in this talk.

824
00:32:58,250 --> 00:32:59,500
You can see our final
table here.

825
00:32:59,500 --> 00:33:00,940
And feel free to contact me
any time you nee need

826
00:33:00,940 --> 00:33:03,180
questions, colton@google.com.

827
00:33:03,180 --> 00:33:03,950
Thank you all, very much.

828
00:33:03,950 --> 00:33:05,200
Have a good day.

829
00:33:05,200 --> 00:33:05,500

