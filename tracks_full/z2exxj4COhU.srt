1
00:00:00,000 --> 00:00:02,860

2
00:00:02,860 --> 00:00:03,230
ALEX FAABORG: All right.

3
00:00:03,230 --> 00:00:03,950
Welcome and Good Morning.

4
00:00:03,950 --> 00:00:04,540
Thanks for coming.

5
00:00:04,540 --> 00:00:05,610
I'm Alex Faaborg.

6
00:00:05,610 --> 00:00:07,130
I'm a designer at Google.

7
00:00:07,130 --> 00:00:09,280
My background's in cognitive
science and machine learning.

8
00:00:09,280 --> 00:00:12,000
And at Google, I've worked on
Google Now and on Glass and on

9
00:00:12,000 --> 00:00:13,440
the Android platform.

10
00:00:13,440 --> 00:00:16,149
And giving design talks at
venues like I/O is really one

11
00:00:16,149 --> 00:00:18,080
of the favorite parts
of my job.

12
00:00:18,080 --> 00:00:21,320
I really enjoy doing it,
especially being able to

13
00:00:21,320 --> 00:00:22,610
talked audience of engineers.

14
00:00:22,610 --> 00:00:24,740
You guys are obviously smarter
than most people.

15
00:00:24,740 --> 00:00:26,360
It means I can move
very quickly.

16
00:00:26,360 --> 00:00:27,410
There's a lot of slides
in this talk.

17
00:00:27,410 --> 00:00:29,150
So we're going to
have to dive in.

18
00:00:29,150 --> 00:00:30,750
So how many of you
are engineers?

19
00:00:30,750 --> 00:00:31,240
Quick hands.

20
00:00:31,240 --> 00:00:31,780
Yep.

21
00:00:31,780 --> 00:00:33,400
All of you.

22
00:00:33,400 --> 00:00:34,580
Awesome.

23
00:00:34,580 --> 00:00:34,990
Makes sense.

24
00:00:34,990 --> 00:00:35,800
It's a tech conference.

25
00:00:35,800 --> 00:00:38,480
So you write great software.

26
00:00:38,480 --> 00:00:41,290
You make sure that your software
executes well or is

27
00:00:41,290 --> 00:00:45,120
interpreted well on a silicon
based processor.

28
00:00:45,120 --> 00:00:46,560
But that's really the
first time that your

29
00:00:46,560 --> 00:00:47,210
code is going to run.

30
00:00:47,210 --> 00:00:49,730
It's going to run it again
on another machine.

31
00:00:49,730 --> 00:00:51,780
So you start with the mechanical
computation, which

32
00:00:51,780 --> 00:00:52,890
is of this entire conference is

33
00:00:52,890 --> 00:00:54,270
about and is very important.

34
00:00:54,270 --> 00:00:55,420
But then there's
another phase.

35
00:00:55,420 --> 00:00:57,260
And that's the biological
computation that's happening

36
00:00:57,260 --> 00:00:58,370
right after.

37
00:00:58,370 --> 00:01:00,940
And this is really where your
code is actually being

38
00:01:00,940 --> 00:01:03,440
interpreted, is on
the user's mind.

39
00:01:03,440 --> 00:01:06,190
So the more you can understand
about both of these machines,

40
00:01:06,190 --> 00:01:08,520
the better software that
you're going to write.

41
00:01:08,520 --> 00:01:10,970
And it's important to focus on
some of the attributes of what

42
00:01:10,970 --> 00:01:13,220
we know about biological
computation.

43
00:01:13,220 --> 00:01:14,620
So a very quick overview
of the talk.

44
00:01:14,620 --> 00:01:16,520
Here's my very rough diagram.

45
00:01:16,520 --> 00:01:18,560
We're going to spend about
half the talk on vision.

46
00:01:18,560 --> 00:01:20,460
And then the second half the
talk we'll be focusing on

47
00:01:20,460 --> 00:01:23,620
attention emotion memory.

48
00:01:23,620 --> 00:01:25,150
Diving into vision--

49
00:01:25,150 --> 00:01:26,460
During the opening sequence
we went through a

50
00:01:26,460 --> 00:01:27,930
lot of optical illusions.

51
00:01:27,930 --> 00:01:29,730
Of course, no cognitive science
talk is complete

52
00:01:29,730 --> 00:01:31,520
without a few optical
illusions.

53
00:01:31,520 --> 00:01:33,240
This one is physiological.

54
00:01:33,240 --> 00:01:36,670
We're basically overloading your
rods, where the little

55
00:01:36,670 --> 00:01:38,785
white dots are popping
and becoming black.

56
00:01:38,785 --> 00:01:41,440
So this is basically just pure
hacking of hardware.

57
00:01:41,440 --> 00:01:43,000
But what's more interesting
is this one.

58
00:01:43,000 --> 00:01:44,420
This one is not physiological.

59
00:01:44,420 --> 00:01:47,190
This one's running on software,
where the lines

60
00:01:47,190 --> 00:01:48,490
appear to be slanted.

61
00:01:48,490 --> 00:01:48,910
They're not.

62
00:01:48,910 --> 00:01:50,010
They're horizontal.

63
00:01:50,010 --> 00:01:53,060
But nonetheless, your mind is
interpreting it because the

64
00:01:53,060 --> 00:01:55,640
very powerful video codec that
you have that's grabbing all

65
00:01:55,640 --> 00:01:58,480
this raw data is trying to do
certain optimizations based

66
00:01:58,480 --> 00:02:00,720
off of expected input in the
types of things you see in the

67
00:02:00,720 --> 00:02:01,760
real world.

68
00:02:01,760 --> 00:02:04,900
Another example is this one,
where it really looks like

69
00:02:04,900 --> 00:02:06,720
there's a gradient on the bar.

70
00:02:06,720 --> 00:02:07,690
The bar's a flat color.

71
00:02:07,690 --> 00:02:09,539
The gradient's only
in the background.

72
00:02:09,539 --> 00:02:12,310
And the reason for this effect
is what we're used to natural

73
00:02:12,310 --> 00:02:13,570
lighting environments.

74
00:02:13,570 --> 00:02:16,840
We're not used to this thing
being able to happen, where

75
00:02:16,840 --> 00:02:19,510
you can have a flat color
and a gradient into the

76
00:02:19,510 --> 00:02:20,450
background.

77
00:02:20,450 --> 00:02:22,970
So all of our evolution has been
optimized for the types

78
00:02:22,970 --> 00:02:25,400
of things that we're
likely to see.

79
00:02:25,400 --> 00:02:27,510
So it's taking short cuts.

80
00:02:27,510 --> 00:02:31,960
It's increasing the amount of
difference that you can very

81
00:02:31,960 --> 00:02:34,640
quickly see the edges
and changes in shape

82
00:02:34,640 --> 00:02:36,170
and changes in shape.

83
00:02:36,170 --> 00:02:38,710
So one of things we're great
at is edge detection.

84
00:02:38,710 --> 00:02:40,270
This is actually my favorite
optical illusions.

85
00:02:40,270 --> 00:02:41,910
It's called the Kansai
Triangle.

86
00:02:41,910 --> 00:02:43,980
And what's cool about this
is there's isn't actually

87
00:02:43,980 --> 00:02:45,540
triangle in the shot.

88
00:02:45,540 --> 00:02:48,760
But you perceive a right
triangle in the foreground.

89
00:02:48,760 --> 00:02:50,900
It's being created
out of nothing.

90
00:02:50,900 --> 00:02:53,250
And when you look at the edges,
you almost feel like

91
00:02:53,250 --> 00:02:56,800
you can see the shadow
on the edge, even

92
00:02:56,800 --> 00:02:58,280
though it's not there.

93
00:02:58,280 --> 00:03:00,710
Your mind is making assumptions
on the triangle

94
00:03:00,710 --> 00:03:02,410
being in the foreground.

95
00:03:02,410 --> 00:03:05,440
And that edge doesn't exist but
nonetheless you feel like

96
00:03:05,440 --> 00:03:07,670
you can see the shadow,
which is very strange.

97
00:03:07,670 --> 00:03:10,770
So a lot of other cognitive
scientists and designers, many

98
00:03:10,770 --> 00:03:13,500
years ago, came up with the
Gestalt Laws of Grouping.

99
00:03:13,500 --> 00:03:14,850
And this is something
that all designers

100
00:03:14,850 --> 00:03:16,520
will learn in school.

101
00:03:16,520 --> 00:03:18,940
And there's eight of
them for grouping.

102
00:03:18,940 --> 00:03:21,110
We only a little bit of time,
so we'll go through three of

103
00:03:21,110 --> 00:03:22,830
them specifically.

104
00:03:22,830 --> 00:03:24,570
First one's the Law
of Proximity.

105
00:03:24,570 --> 00:03:27,670
By introducing white space, you
perceive columns, instead

106
00:03:27,670 --> 00:03:29,980
of just the same number
of shapes.

107
00:03:29,980 --> 00:03:31,980
When it's uniform grade, you
just perceive all the shapes.

108
00:03:31,980 --> 00:03:33,430
But as soon as you have a little
bit of white space--

109
00:03:33,430 --> 00:03:35,620
your mind is very good at
finding patterns and assigning

110
00:03:35,620 --> 00:03:36,510
order to things.

111
00:03:36,510 --> 00:03:39,270
So you very quickly assign
that into three columns.

112
00:03:39,270 --> 00:03:41,220
So you don't have to draw boxes
around everything to

113
00:03:41,220 --> 00:03:41,940
create grouping.

114
00:03:41,940 --> 00:03:44,030
You can just use a little
bit of white space.

115
00:03:44,030 --> 00:03:45,550
So an obvious example
of this--

116
00:03:45,550 --> 00:03:47,470
Google search results.

117
00:03:47,470 --> 00:03:49,160
By introducing just a little bit
of white space in between

118
00:03:49,160 --> 00:03:51,410
the search results, it's very
easy for people to group those

119
00:03:51,410 --> 00:03:53,390
and chunk them into individual
results.

120
00:03:53,390 --> 00:03:55,535
Whereas if that white space
didn't exist, even though you

121
00:03:55,535 --> 00:03:58,070
have all the formatting cues--

122
00:03:58,070 --> 00:04:00,540
bold titles and the
blue underlines.

123
00:04:00,540 --> 00:04:01,260
Those things still exist.

124
00:04:01,260 --> 00:04:03,820
But it's still very hard to be
able to see that structure

125
00:04:03,820 --> 00:04:05,340
without the white space.

126
00:04:05,340 --> 00:04:08,080
Obvious analogy to writing
software is trying to look at

127
00:04:08,080 --> 00:04:09,890
code without white
space is hard.

128
00:04:09,890 --> 00:04:12,200
I mean, a computer can interpret
it just fine.

129
00:04:12,200 --> 00:04:13,852
In fact, white space is probably
wasting a little bit

130
00:04:13,852 --> 00:04:14,510
of bandwidth.

131
00:04:14,510 --> 00:04:17,860
But nonetheless for you as a
developer, you need that to

132
00:04:17,860 --> 00:04:19,110
see the structure.

133
00:04:19,110 --> 00:04:21,070

134
00:04:21,070 --> 00:04:23,820
Another Gestalt Law of Grouping
is the Law of

135
00:04:23,820 --> 00:04:28,030
Closure, where we very quickly
complete forms.

136
00:04:28,030 --> 00:04:30,720
It's not actually full circle or
full square, but we quickly

137
00:04:30,720 --> 00:04:31,840
make it one.

138
00:04:31,840 --> 00:04:35,220
So again, you don't have to draw
boxes around everything.

139
00:04:35,220 --> 00:04:37,550
In the case of Android, just
having the little edges on the

140
00:04:37,550 --> 00:04:39,660
text field is enough for
your mind to mentally

141
00:04:39,660 --> 00:04:40,970
complete the box.

142
00:04:40,970 --> 00:04:42,130
You begin to see the box.

143
00:04:42,130 --> 00:04:43,680
And the box exists in your mind,
even though it wasn't

144
00:04:43,680 --> 00:04:44,930
literally drawn on the screen.

145
00:04:44,930 --> 00:04:47,520

146
00:04:47,520 --> 00:04:48,700
Law of Similarity--

147
00:04:48,700 --> 00:04:50,760
here, instead of columns,
we're seeing row.

148
00:04:50,760 --> 00:04:52,870
And the only thing we changed
was level of contrast of some

149
00:04:52,870 --> 00:04:53,860
of the objects.

150
00:04:53,860 --> 00:04:55,270
But the rows pop out.

151
00:04:55,270 --> 00:04:56,380
And it doesn't have
to contrast.

152
00:04:56,380 --> 00:04:59,930
It could also be shape,
where seeing stars

153
00:04:59,930 --> 00:05:02,030
versus circles, where--

154
00:05:02,030 --> 00:05:03,780
the rows really pop
out at you.

155
00:05:03,780 --> 00:05:06,940
And we this in Gmail, where--
in this case, columns.

156
00:05:06,940 --> 00:05:08,940
You have a column of check boxes
and column of starring

157
00:05:08,940 --> 00:05:11,600
messages and a column of
priority, where it's very easy

158
00:05:11,600 --> 00:05:14,020
to then visually scan down that
column, even though the

159
00:05:14,020 --> 00:05:15,430
spacing is all uniform.

160
00:05:15,430 --> 00:05:17,940
It's otherwise just a grid.

161
00:05:17,940 --> 00:05:19,990
So those are all things
that are innate,

162
00:05:19,990 --> 00:05:21,640
that apply to everyone.

163
00:05:21,640 --> 00:05:24,830
There's also some aspects of
layout that are cultural, that

164
00:05:24,830 --> 00:05:26,250
you pick up over time.

165
00:05:26,250 --> 00:05:29,670
One of the most basic one of
these is reading right to left

166
00:05:29,670 --> 00:05:31,100
versus left to right.

167
00:05:31,100 --> 00:05:33,150
This is very important for the
structure of your application.

168
00:05:33,150 --> 00:05:35,190
It's not just about the text,
but it's about the logical

169
00:05:35,190 --> 00:05:38,660
flow of the application, where
the meaning of a switch being

170
00:05:38,660 --> 00:05:41,970
on versus off should actually
be inverted, if it's

171
00:05:41,970 --> 00:05:43,460
horizontal switch.

172
00:05:43,460 --> 00:05:46,540
I should note-- the back
button is wrong.

173
00:05:46,540 --> 00:05:48,140
As I was a taking the screen
shot, I realize this.

174
00:05:48,140 --> 00:05:50,060
We'll fix that.

175
00:05:50,060 --> 00:05:53,190
But the notion of back is
opposite, if you're used to

176
00:05:53,190 --> 00:05:55,880
reading right to left, instead
of left to right.

177
00:05:55,880 --> 00:05:57,740
And this applies to really know
the whole structure of

178
00:05:57,740 --> 00:05:59,200
your application.

179
00:05:59,200 --> 00:06:00,730
And things like going
up in the hierarchy.

180
00:06:00,730 --> 00:06:02,090
In the action bar,
we see that.

181
00:06:02,090 --> 00:06:03,190
We didn't do that correctly.

182
00:06:03,190 --> 00:06:06,960
Or even things like the Settings
button being sort of

183
00:06:06,960 --> 00:06:09,250
peripheral to activating a
keyboard in this sense--

184
00:06:09,250 --> 00:06:10,470
there's a certain flow to it.

185
00:06:10,470 --> 00:06:13,130
And that flow should
be reversed.

186
00:06:13,130 --> 00:06:16,340
So a whole new topic related to
vision-- peripheral vision.

187
00:06:16,340 --> 00:06:19,920
Let's say we have an underground
testing facility,

188
00:06:19,920 --> 00:06:23,000
where we do studies on people,
of course for science.

189
00:06:23,000 --> 00:06:25,090
And we keep people isolated.

190
00:06:25,090 --> 00:06:25,990
So we don't about
the studies are.

191
00:06:25,990 --> 00:06:28,430
So they don't know any idea what
the study is going in.

192
00:06:28,430 --> 00:06:30,800
But we give you this
test chamber.

193
00:06:30,800 --> 00:06:32,770
And we ask you to go
into the room.

194
00:06:32,770 --> 00:06:33,920
So you go in.

195
00:06:33,920 --> 00:06:36,800
And bam-- tiger.

196
00:06:36,800 --> 00:06:37,950
That's the only test.

197
00:06:37,950 --> 00:06:40,190
Basically, we're just monitoring
how long it takes

198
00:06:40,190 --> 00:06:42,330
you to realize that there's
a tiger in front of you.

199
00:06:42,330 --> 00:06:45,730
And it's going to be about 190
milliseconds to recognize that

200
00:06:45,730 --> 00:06:48,310
tiger that we very carefully
set on the podium.

201
00:06:48,310 --> 00:06:49,850
It's probably hard to get
him to stay there.

202
00:06:49,850 --> 00:06:50,890
And of course, he
then kills you.

203
00:06:50,890 --> 00:06:52,680
But that's not part
of the test.

204
00:06:52,680 --> 00:06:53,375
That part's arbitrary.

205
00:06:53,375 --> 00:06:55,890
And it's up to the tiger.

206
00:06:55,890 --> 00:06:58,820
So let's run another study.

207
00:06:58,820 --> 00:07:01,320
At this point, subjects are
kind of talking, say these

208
00:07:01,320 --> 00:07:02,470
chamber's are no good.

209
00:07:02,470 --> 00:07:03,615
So you go in.

210
00:07:03,615 --> 00:07:04,790
There's nothing.

211
00:07:04,790 --> 00:07:07,330
Once again, bam-- tiger.

212
00:07:07,330 --> 00:07:10,305
And what's interesting about
this is you'll see this tiger

213
00:07:10,305 --> 00:07:12,020
in 80 milliseconds.

214
00:07:12,020 --> 00:07:15,380
We've actually evolved to have
faster peripheral vision for

215
00:07:15,380 --> 00:07:18,240
detecting motion in objects
because of tigers.

216
00:07:18,240 --> 00:07:22,800
There's a very legitimate for
this is all the people who had

217
00:07:22,800 --> 00:07:25,720
vision that was uniform,
between central and

218
00:07:25,720 --> 00:07:27,180
peripheral, they'll get
killed by tigers and

219
00:07:27,180 --> 00:07:28,340
the rest of us survived.

220
00:07:28,340 --> 00:07:31,540
And we're very good at seeing
movement and change in our

221
00:07:31,540 --> 00:07:32,410
peripheral vision.

222
00:07:32,410 --> 00:07:34,000
It's highly tuned for that.

223
00:07:34,000 --> 00:07:35,510
So what's this have to do
with interface design?

224
00:07:35,510 --> 00:07:38,320
Obviously, we don't exist in
environments where we have a

225
00:07:38,320 --> 00:07:39,390
lot of predators anymore.

226
00:07:39,390 --> 00:07:41,390
We're all information workers.

227
00:07:41,390 --> 00:07:43,890
But nonetheless, you can use
this in your design.

228
00:07:43,890 --> 00:07:45,990
So let's say that I'm
modifying this

229
00:07:45,990 --> 00:07:47,180
product-to-plan document.

230
00:07:47,180 --> 00:07:49,170
And then my PM shows up.

231
00:07:49,170 --> 00:07:50,640
And this is his actual
at avatar

232
00:07:50,640 --> 00:07:52,530
picture up in the corner--

233
00:07:52,530 --> 00:07:55,290
of the tiger, tying
it all together.

234
00:07:55,290 --> 00:07:57,110
And what's interesting about
this is it's actually easier

235
00:07:57,110 --> 00:07:59,290
for me to see that notification
of him arriving

236
00:07:59,290 --> 00:08:01,830
in the document, when that's
being cued off of my

237
00:08:01,830 --> 00:08:04,080
peripheral vision, verses if it
happened right in front of

238
00:08:04,080 --> 00:08:05,800
me as an editing text
and things.

239
00:08:05,800 --> 00:08:09,330
So that same effect of grabbing
the user's attention

240
00:08:09,330 --> 00:08:10,090
of notification--

241
00:08:10,090 --> 00:08:12,640
it's actually easier
in their periphery.

242
00:08:12,640 --> 00:08:14,900
So another new topic
related to vision--

243
00:08:14,900 --> 00:08:16,860
geons and object recognition.

244
00:08:16,860 --> 00:08:19,670
So we recognize a lot of
objects in the world.

245
00:08:19,670 --> 00:08:21,920
We're constantly encountering
new objects.

246
00:08:21,920 --> 00:08:24,040
There's millions, potentially
billions of them.

247
00:08:24,040 --> 00:08:27,150
So the question is how do we
actually store all of this in

248
00:08:27,150 --> 00:08:28,590
our own software?

249
00:08:28,590 --> 00:08:30,820
And this is some of the
most recent research.

250
00:08:30,820 --> 00:08:32,860
The idea is that we have a core

251
00:08:32,860 --> 00:08:34,980
vocabulary geometric objects.

252
00:08:34,980 --> 00:08:36,679
And people argue about
the exact number

253
00:08:36,679 --> 00:08:38,230
of how many it is.

254
00:08:38,230 --> 00:08:40,260
But people are gravitating
towards there

255
00:08:40,260 --> 00:08:42,070
maybe being 24 of them.

256
00:08:42,070 --> 00:08:43,309
And even just 24--

257
00:08:43,309 --> 00:08:45,210
any three of them, you could
create greater than 306

258
00:08:45,210 --> 00:08:47,210
billion combinations.

259
00:08:47,210 --> 00:08:49,340
So that's robust enough that
it could be working.

260
00:08:49,340 --> 00:08:51,340
But basically the idea is when
you look at an object, you

261
00:08:51,340 --> 00:08:54,080
immediately break it down into
the geons that it contains.

262
00:08:54,080 --> 00:08:55,930
And then you use that
to do the look up of

263
00:08:55,930 --> 00:08:57,090
which object it is.

264
00:08:57,090 --> 00:08:59,800
So here you got cylinder
and the curved candle.

265
00:08:59,800 --> 00:09:01,730
You do that look up and
say, oh, it's a cup.

266
00:09:01,730 --> 00:09:04,510
And this applies to even more
complicated objects, where, as

267
00:09:04,510 --> 00:09:06,320
you're perceiving it, you're
constantly just breaking it

268
00:09:06,320 --> 00:09:08,750
down into the various core
geometric objects and then

269
00:09:08,750 --> 00:09:10,360
using that for recognition.

270
00:09:10,360 --> 00:09:12,040
So what's interesting
about this--

271
00:09:12,040 --> 00:09:12,970
not exactly tigers.

272
00:09:12,970 --> 00:09:17,330
But you will recognize the
silhouette, the basic geometry

273
00:09:17,330 --> 00:09:20,420
of the shape, faster than you'll
recognize an icon that

274
00:09:20,420 --> 00:09:23,230
is more photo-realistic
or rendered.

275
00:09:23,230 --> 00:09:25,630
Now it's a very small
difference.

276
00:09:25,630 --> 00:09:27,240
So you might have been able
to perceive it as

277
00:09:27,240 --> 00:09:28,890
you're watching yourself.

278
00:09:28,890 --> 00:09:31,120
But nonetheless it's kind of
interesting that, if you

279
00:09:31,120 --> 00:09:34,150
really care about speed and
subtlety and having a truly

280
00:09:34,150 --> 00:09:37,810
optimized design, these sort of
silhouette, basic geometry

281
00:09:37,810 --> 00:09:40,580
shapes actually do recognize
faster for the user.

282
00:09:40,580 --> 00:09:43,120
And this is also--

283
00:09:43,120 --> 00:09:45,310
there's a lot of aesthetic
reasons why you might want to

284
00:09:45,310 --> 00:09:46,640
use one versus the other.

285
00:09:46,640 --> 00:09:48,350
For instance, on the Android
home screen, where we can't

286
00:09:48,350 --> 00:09:51,410
control the background, we opt
for using the more rendered

287
00:09:51,410 --> 00:09:53,760
icons because they have better
contrast on variety of

288
00:09:53,760 --> 00:09:54,890
backgrounds.

289
00:09:54,890 --> 00:09:57,970
But also on Android, the hollow
icon set is entirely

290
00:09:57,970 --> 00:10:01,420
this flat icon style, in part
due to aesthetics and also in

291
00:10:01,420 --> 00:10:03,310
part due to this recognition.

292
00:10:03,310 --> 00:10:05,280
So you can very quickly
spot things in here--

293
00:10:05,280 --> 00:10:08,110
camera or joystick or battery.

294
00:10:08,110 --> 00:10:10,030
As soon as you see these things,
you very immediately

295
00:10:10,030 --> 00:10:12,220
know what they are.

296
00:10:12,220 --> 00:10:14,170
So another type of object
recognition we do is facial

297
00:10:14,170 --> 00:10:14,860
recognition.

298
00:10:14,860 --> 00:10:16,380
And this is actually done
on a completely

299
00:10:16,380 --> 00:10:18,110
separate hardware path.

300
00:10:18,110 --> 00:10:19,890
You're not looking at someone's
face and breaking it

301
00:10:19,890 --> 00:10:21,270
down into various geometric
objects.

302
00:10:21,270 --> 00:10:24,320
You have an entirely separate
set of hardware in your mind

303
00:10:24,320 --> 00:10:27,720
that's purely dedicated
recognizing faces.

304
00:10:27,720 --> 00:10:30,850
So let's take you into
another test chamber.

305
00:10:30,850 --> 00:10:32,330
At this point, you're
really worried.

306
00:10:32,330 --> 00:10:35,540
And you go in, and you think,
oh my god, it's

307
00:10:35,540 --> 00:10:37,200
full stock-art people.

308
00:10:37,200 --> 00:10:38,970
And they're all walking
towards me--

309
00:10:38,970 --> 00:10:40,140
even worse than tigers.

310
00:10:40,140 --> 00:10:43,220
And in this test, it is
all stock-art people.

311
00:10:43,220 --> 00:10:46,980
And they're kind of
creepy perfection.

312
00:10:46,980 --> 00:10:49,000
But there's one person and
who's actually a friend.

313
00:10:49,000 --> 00:10:51,420
And the test is how quickly do
you see your friend as all

314
00:10:51,420 --> 00:10:52,920
these people walk towards you.

315
00:10:52,920 --> 00:10:54,260
And you'll see them
very quickly.

316
00:10:54,260 --> 00:10:56,270
You've probably have already
about perceived this at the

317
00:10:56,270 --> 00:10:58,050
conference, where various people
conference-- you're

318
00:10:58,050 --> 00:10:59,810
seeing a lot of cases,
obviously.

319
00:10:59,810 --> 00:11:01,740
But you'll quickly spot these
people that you know.

320
00:11:01,740 --> 00:11:03,440
You might not remember their
name, which is a whole

321
00:11:03,440 --> 00:11:04,350
different thing.

322
00:11:04,350 --> 00:11:07,130
But you'll definitely know that
you recognize them as

323
00:11:07,130 --> 00:11:09,190
someone that you've
met before.

324
00:11:09,190 --> 00:11:11,390
And you're able to do this by
taking in all the faces

325
00:11:11,390 --> 00:11:13,650
simultaneously, which is
also very interesting.

326
00:11:13,650 --> 00:11:16,830
You're not having to do a very
linear scan between each face

327
00:11:16,830 --> 00:11:17,950
to process it.

328
00:11:17,950 --> 00:11:19,830
This is all just happening in
the background, on the full

329
00:11:19,830 --> 00:11:21,050
visual field.

330
00:11:21,050 --> 00:11:24,490
So in terms of implications
for interface design--

331
00:11:24,490 --> 00:11:27,270
if you're looking to see a
particular email came in from

332
00:11:27,270 --> 00:11:29,970
say James or Ruth, with this
type of design you're going to

333
00:11:29,970 --> 00:11:32,080
have to do when your scan
down the list and

334
00:11:32,080 --> 00:11:33,360
read each of the names.

335
00:11:33,360 --> 00:11:36,430
But if we start using faces,
then you can get that effect

336
00:11:36,430 --> 00:11:38,130
of all the hardware that
we had built for facial

337
00:11:38,130 --> 00:11:39,130
recognition.

338
00:11:39,130 --> 00:11:41,290
So for instance, the
Glass Timeline--

339
00:11:41,290 --> 00:11:43,410
as you're browsing through it,
you can very quickly find

340
00:11:43,410 --> 00:11:45,250
particular pieces of information
because of those

341
00:11:45,250 --> 00:11:46,070
pictures of faces.

342
00:11:46,070 --> 00:11:47,980
So if you're looking for a
lunch appointment, you'll

343
00:11:47,980 --> 00:11:50,610
recognize Richard there and
you'll see a very quickly.

344
00:11:50,610 --> 00:11:54,060
And you don't have to do that
scan as you would for text.

345
00:11:54,060 --> 00:11:55,600
So any new topic related
to vision--

346
00:11:55,600 --> 00:11:57,480
perceived affordances.

347
00:11:57,480 --> 00:11:59,620
Every single time people talk
about perceived affordances,

348
00:11:59,620 --> 00:12:01,360
they talk about doorknobs.

349
00:12:01,360 --> 00:12:03,340
I'm starting to think that maybe
people think it's only

350
00:12:03,340 --> 00:12:03,950
about doorknobs.

351
00:12:03,950 --> 00:12:07,210
So we'll do a whole
new example.

352
00:12:07,210 --> 00:12:08,170
So again, test chamber.

353
00:12:08,170 --> 00:12:09,220
And yeah, there's
not a doorknob.

354
00:12:09,220 --> 00:12:10,440
That's not the test.

355
00:12:10,440 --> 00:12:12,030
You have to go through door.

356
00:12:12,030 --> 00:12:13,990
And the way you figure out how
to go through the door is a

357
00:12:13,990 --> 00:12:16,410
perceived affordance because you
see the physics of how the

358
00:12:16,410 --> 00:12:17,520
thing's going to slide.

359
00:12:17,520 --> 00:12:19,160
But you go into the
test chamber.

360
00:12:19,160 --> 00:12:21,990
And there's pieces of trash
on the ground floor or

361
00:12:21,990 --> 00:12:22,780
some type of object.

362
00:12:22,780 --> 00:12:24,090
And what we're trying
to figure is,

363
00:12:24,090 --> 00:12:26,210
do you recycle correctly?

364
00:12:26,210 --> 00:12:27,900
And the answer is, usually no.

365
00:12:27,900 --> 00:12:29,770
People are actually pretty
lazy about this.

366
00:12:29,770 --> 00:12:30,270
They're busy.

367
00:12:30,270 --> 00:12:31,860
They're thinking about
other things.

368
00:12:31,860 --> 00:12:33,000
They just look at all
the trash cans and

369
00:12:33,000 --> 00:12:34,380
just throw it in one.

370
00:12:34,380 --> 00:12:37,430
What's interesting, though, is
if you change the trash cans

371
00:12:37,430 --> 00:12:40,350
to have slots that match roughly
the types of objects

372
00:12:40,350 --> 00:12:42,840
that should go in them,
recycling actually goes up

373
00:12:42,840 --> 00:12:45,730
34%, or correct recycling.

374
00:12:45,730 --> 00:12:48,520
Because, as you're holding a
piece of paper, you just sort

375
00:12:48,520 --> 00:12:51,150
of think, yes that is where
I should put this.

376
00:12:51,150 --> 00:12:53,230
And you slide it in.

377
00:12:53,230 --> 00:12:55,090
And you're doing this processing
very quickly, just

378
00:12:55,090 --> 00:12:57,250
as you look at a doorknob, you
make immediate inferences

379
00:12:57,250 --> 00:12:58,600
about its physics, on if
you're supposed to

380
00:12:58,600 --> 00:13:00,310
push or pull it.

381
00:13:00,310 --> 00:13:02,140
So an image that, of course,
had been circling around

382
00:13:02,140 --> 00:13:04,370
inside of Google--

383
00:13:04,370 --> 00:13:06,960
Sometimes hiring the smartest
people isn't enough, where,

384
00:13:06,960 --> 00:13:10,410
no, the milk jug is
not compostable.

385
00:13:10,410 --> 00:13:12,290
But I would argue, first of
all, they were probably

386
00:13:12,290 --> 00:13:14,180
thinking about a very hard
computer science problem as

387
00:13:14,180 --> 00:13:15,180
they did this.

388
00:13:15,180 --> 00:13:17,100
And secondly, based on perceived
affordances, that

389
00:13:17,100 --> 00:13:19,940
was the correct slot for
something milk jug sized.

390
00:13:19,940 --> 00:13:21,540
They were doing all
that processing.

391
00:13:21,540 --> 00:13:22,920
They just weren't reading
any signs or

392
00:13:22,920 --> 00:13:24,230
really paying attention.

393
00:13:24,230 --> 00:13:26,350
And these types of perceived
affordances obviously have a

394
00:13:26,350 --> 00:13:27,770
lot of impact in the
physical world for

395
00:13:27,770 --> 00:13:29,270
doors and trash cans.

396
00:13:29,270 --> 00:13:31,180
But they also have an impact
in the virtual world, with

397
00:13:31,180 --> 00:13:34,670
things like buttons, sliders,
where can see the physics of

398
00:13:34,670 --> 00:13:36,630
the object as you
just take it in.

399
00:13:36,630 --> 00:13:39,360
And it immediately visually
conveys to you how you

400
00:13:39,360 --> 00:13:40,790
interact with it--

401
00:13:40,790 --> 00:13:44,360
on and off switches, or even a
check box that can or cannot

402
00:13:44,360 --> 00:13:47,030
contain a check inside of it.

403
00:13:47,030 --> 00:13:50,700
So another topic about vision--
color deficiency.

404
00:13:50,700 --> 00:13:52,630
So most of us in the room
have three cones--

405
00:13:52,630 --> 00:13:54,630
tri-chromatic vision.

406
00:13:54,630 --> 00:13:56,600
And some of us in the room will
have two cones, where you

407
00:13:56,600 --> 00:14:00,080
could-- most commonly, you'll
lose either the perception of

408
00:14:00,080 --> 00:14:01,220
red or perception of green.

409
00:14:01,220 --> 00:14:04,310
It's also possible to
lose perception of

410
00:14:04,310 --> 00:14:06,780
blue, although that's--

411
00:14:06,780 --> 00:14:07,430
its more rare--

412
00:14:07,430 --> 00:14:08,600
Tritanopia.

413
00:14:08,600 --> 00:14:13,630
So there's a general the
misconception that when you're

414
00:14:13,630 --> 00:14:15,420
colorblind, you actually
perceive the world in

415
00:14:15,420 --> 00:14:17,430
monochrome, which is not true.

416
00:14:17,430 --> 00:14:19,726
It's much more common to only
lose one of your cones through

417
00:14:19,726 --> 00:14:21,090
a mutation.

418
00:14:21,090 --> 00:14:23,130
And there's also a general
perception that you shouldn't

419
00:14:23,130 --> 00:14:26,130
use red-green differences
in your interface.

420
00:14:26,130 --> 00:14:29,580
So let's look at an interface it
uses red-green differences.

421
00:14:29,580 --> 00:14:31,920
And we can very easily simulate
what this appears for

422
00:14:31,920 --> 00:14:35,540
someone of Deuteranopia or
Protanopia or Tritanopia.

423
00:14:35,540 --> 00:14:39,280
And the reason it works is
there's more than just color

424
00:14:39,280 --> 00:14:39,830
in the values.

425
00:14:39,830 --> 00:14:41,690
You also have the level
of contrast.

426
00:14:41,690 --> 00:14:43,720
So even though you're losing
that red-green difference,

427
00:14:43,720 --> 00:14:46,500
you're having very clear
contrast differences.

428
00:14:46,500 --> 00:14:48,420
So instead of just making
assumptions on which covers

429
00:14:48,420 --> 00:14:49,430
you should or shouldn't use.

430
00:14:49,430 --> 00:14:51,640
It's better to just run them
through filters, your

431
00:14:51,640 --> 00:14:53,810
interface, see what it actually
looks like for people

432
00:14:53,810 --> 00:14:55,650
in that population set,
make sure things

433
00:14:55,650 --> 00:14:57,380
are working out OK.

434
00:14:57,380 --> 00:15:00,610
And there's a lot of-- if go
online and just search for it.

435
00:15:00,610 --> 00:15:03,740
There's a lot SVG filters that
you can apply to an image to

436
00:15:03,740 --> 00:15:04,510
see the change.

437
00:15:04,510 --> 00:15:05,870
There's a java application
that you can

438
00:15:05,870 --> 00:15:07,410
download to do it locally.

439
00:15:07,410 --> 00:15:08,400
Or if you have Photoshop--

440
00:15:08,400 --> 00:15:09,430
amazingly enough, it's actually

441
00:15:09,430 --> 00:15:11,470
in the menu of Photoshop.

442
00:15:11,470 --> 00:15:13,530
As I was researching this,
I was kind of surprised.

443
00:15:13,530 --> 00:15:14,850
I had never seen this before.

444
00:15:14,850 --> 00:15:17,220
You can just go to View, Proof
Set Up, and actually--

445
00:15:17,220 --> 00:15:18,220
unfortunately for
the people with

446
00:15:18,220 --> 00:15:20,320
Tritanopia, they aren't included.

447
00:15:20,320 --> 00:15:23,180
But [INAUDIBLE] very easy if you
have Photoshop to quickly

448
00:15:23,180 --> 00:15:24,540
check out an interface.

449
00:15:24,540 --> 00:15:26,980
And what's sort of interesting
is a mutation causes you to

450
00:15:26,980 --> 00:15:29,940
have two cones versus three.

451
00:15:29,940 --> 00:15:31,280
But you can also have
a mutation it

452
00:15:31,280 --> 00:15:33,150
gives you four cones--

453
00:15:33,150 --> 00:15:37,070
tetrachromats that have
superhuman color vision, which

454
00:15:37,070 --> 00:15:39,930
is really cool because each
cone gives you about 100

455
00:15:39,930 --> 00:15:40,770
ranges of color.

456
00:15:40,770 --> 00:15:43,620
So we all have about a million
colors that we can perceive.

457
00:15:43,620 --> 00:15:46,760
Tetrachromats can perceive 100
million colors because of

458
00:15:46,760 --> 00:15:48,320
their fourth cone.

459
00:15:48,320 --> 00:15:51,250
And this is also extremely
common in other species.

460
00:15:51,250 --> 00:15:53,890
Birds, for instance, all
have four cones.

461
00:15:53,890 --> 00:15:55,120
The fact that we have
three primary

462
00:15:55,120 --> 00:15:56,750
colors is kind of arbitrary.

463
00:15:56,750 --> 00:15:58,480
If we were a society of
birds, we would have

464
00:15:58,480 --> 00:15:59,820
four primary colors.

465
00:15:59,820 --> 00:16:02,190
Birds can perceive ultraviolet
light.

466
00:16:02,190 --> 00:16:03,830
There's no design implication
here.

467
00:16:03,830 --> 00:16:05,120
It's just cool.

468
00:16:05,120 --> 00:16:06,470
So moving on--

469
00:16:06,470 --> 00:16:09,570
colors and culture.

470
00:16:09,570 --> 00:16:10,450
This is not innate.

471
00:16:10,450 --> 00:16:13,610
It's something that's
learned over time.

472
00:16:13,610 --> 00:16:15,940
Concepts like to red, at least
in the West, are usually

473
00:16:15,940 --> 00:16:17,330
pretty negative.

474
00:16:17,330 --> 00:16:21,420
Red tape, in the red, these
types of phrases.

475
00:16:21,420 --> 00:16:22,640
Whereas in Japan,
red is actually

476
00:16:22,640 --> 00:16:24,160
very celebratory color.

477
00:16:24,160 --> 00:16:26,450
It has connotations
of lanterns and

478
00:16:26,450 --> 00:16:28,140
festivals and happiness.

479
00:16:28,140 --> 00:16:30,120
Of course, their stop signs are
still red for consistency.

480
00:16:30,120 --> 00:16:31,870
I guess those are more
celebratory stop signs.

481
00:16:31,870 --> 00:16:34,630
But they've a very
advanced culture.

482
00:16:34,630 --> 00:16:37,610
But it's important to sort of
think about the various

483
00:16:37,610 --> 00:16:39,770
implications of the colors
that you're choosing.

484
00:16:39,770 --> 00:16:41,120
But that's not true
for all colors.

485
00:16:41,120 --> 00:16:42,880
Other colors are
more universal.

486
00:16:42,880 --> 00:16:45,510
For instance, gold
is universal and

487
00:16:45,510 --> 00:16:46,210
not just as a color.

488
00:16:46,210 --> 00:16:48,670
But it's sort of universal
in the universe.

489
00:16:48,670 --> 00:16:51,090
And this has implications on
Android design, where we use

490
00:16:51,090 --> 00:16:54,590
gold to mean the
monetary value.

491
00:16:54,590 --> 00:16:59,000
This of course localizes very
well to pirates as well.

492
00:16:59,000 --> 00:17:00,940
So let's go on Selective
Visual Variables--

493
00:17:00,940 --> 00:17:02,550
another test chamber.

494
00:17:02,550 --> 00:17:03,870
Now this is a great
test chamber.

495
00:17:03,870 --> 00:17:06,609
This is a test chamber that you
actually get to go into.

496
00:17:06,609 --> 00:17:09,670
And the winners will get
Android plushies.

497
00:17:09,670 --> 00:17:13,450
So hopefully you'll win.

498
00:17:13,450 --> 00:17:14,430
The task is--

499
00:17:14,430 --> 00:17:17,730
I'm going to show you a range
of letters and numbers.

500
00:17:17,730 --> 00:17:20,540
And the first person to
correctly shout out how many

501
00:17:20,540 --> 00:17:23,150
red letters or numbers
there are, just red

502
00:17:23,150 --> 00:17:25,060
shapes total, wins.

503
00:17:25,060 --> 00:17:26,290
And I'll time you.

504
00:17:26,290 --> 00:17:28,870
So--

505
00:17:28,870 --> 00:17:30,462
we'll see how long
this takes you.

506
00:17:30,462 --> 00:17:30,920
All right.

507
00:17:30,920 --> 00:17:32,170
Begin.

508
00:17:32,170 --> 00:17:35,776

509
00:17:35,776 --> 00:17:37,930
[AUDIENCE SHOUTING ANSWERS]

510
00:17:37,930 --> 00:17:39,720
21 over here.

511
00:17:39,720 --> 00:17:41,140
Who was the person
who said 21?

512
00:17:41,140 --> 00:17:41,952
You?

513
00:17:41,952 --> 00:17:42,360
All right.

514
00:17:42,360 --> 00:17:45,885
There you go.

515
00:17:45,885 --> 00:17:48,150
Sorry, Angela's going to
help me hand it out.

516
00:17:48,150 --> 00:17:49,130
OK.

517
00:17:49,130 --> 00:17:49,990
Let's try again.

518
00:17:49,990 --> 00:17:52,960
And that took about
15 seconds.

519
00:17:52,960 --> 00:17:53,380
All right.

520
00:17:53,380 --> 00:17:58,178
Your next task is how
many G's are there.

521
00:17:58,178 --> 00:18:09,002
[AUDIENCE SHOUTING ANSWERS]

522
00:18:09,002 --> 00:18:09,500
All right.

523
00:18:09,500 --> 00:18:10,995
16.

524
00:18:10,995 --> 00:18:11,695
Who is--

525
00:18:11,695 --> 00:18:13,300
Who's the person who
said 16 over here?

526
00:18:13,300 --> 00:18:13,630
Yep.

527
00:18:13,630 --> 00:18:15,720
You're right.

528
00:18:15,720 --> 00:18:17,920
OK.

529
00:18:17,920 --> 00:18:22,070
That took you about 20 seconds
and there were less of them.

530
00:18:22,070 --> 00:18:24,530
And you were all shouting out
completely random numbers were

531
00:18:24,530 --> 00:18:27,222
not accurate at all.

532
00:18:27,222 --> 00:18:30,760
So the implication for this is
that color is a selective

533
00:18:30,760 --> 00:18:32,980
visual variable, while
shape is not.

534
00:18:32,980 --> 00:18:34,660
In computer science
terms, you can per

535
00:18:34,660 --> 00:18:37,190
color in constant time.

536
00:18:37,190 --> 00:18:39,010
Going back to the spread.

537
00:18:39,010 --> 00:18:41,740
As you look at it, the color's
all coming in, just as you can

538
00:18:41,740 --> 00:18:43,830
do face recognition
constant time.

539
00:18:43,830 --> 00:18:47,550
Whereas for shape, you're having
to do a very clear

540
00:18:47,550 --> 00:18:48,990
linear scan-- look this shape.

541
00:18:48,990 --> 00:18:49,315
Is it a G?

542
00:18:49,315 --> 00:18:49,680
No.

543
00:18:49,680 --> 00:18:50,400
Move onto the next.

544
00:18:50,400 --> 00:18:50,885
Is it a G?

545
00:18:50,885 --> 00:18:52,120
No.

546
00:18:52,120 --> 00:18:55,310
So this is an important effect
for your interfaces, this type

547
00:18:55,310 --> 00:18:57,950
of visual processing, that if
someone's looking for shapes

548
00:18:57,950 --> 00:19:00,320
they're going to have to
do that linear scan.

549
00:19:00,320 --> 00:19:02,790
So the implication for
Google search--

550
00:19:02,790 --> 00:19:08,880
by using color for title and
then also for the domain, the

551
00:19:08,880 --> 00:19:12,980
full URL, you're able to then
very quickly in your mind to

552
00:19:12,980 --> 00:19:15,320
do a selective--

553
00:19:15,320 --> 00:19:16,860
it's sort of filtering
on that color and

554
00:19:16,860 --> 00:19:18,020
then scan just those.

555
00:19:18,020 --> 00:19:20,260
And you can pop to them, just
as you're popping tall with

556
00:19:20,260 --> 00:19:21,730
the red letters,
counting them.

557
00:19:21,730 --> 00:19:25,720
So if you're looking, for
instance, for berkeley.edu,

558
00:19:25,720 --> 00:19:29,040
you can say, not it, it, without
having to read any of

559
00:19:29,040 --> 00:19:30,700
the other information.

560
00:19:30,700 --> 00:19:32,110
That covers vision.

561
00:19:32,110 --> 00:19:34,956
Let's dive into the attention,
emotion, and memory.

562
00:19:34,956 --> 00:19:37,250
First, we'll talk
about attention.

563
00:19:37,250 --> 00:19:38,740
This is an fMRI machine.

564
00:19:38,740 --> 00:19:40,380
And what's really amazing about
this machine is we can

565
00:19:40,380 --> 00:19:43,390
actually look at the minds and
see how it's operating on a

566
00:19:43,390 --> 00:19:44,740
physiological level.

567
00:19:44,740 --> 00:19:45,990
And this is how we
know everything

568
00:19:45,990 --> 00:19:47,380
about how mind works.

569
00:19:47,380 --> 00:19:51,850
So very, very, rough, high level
overview of the mind--

570
00:19:51,850 --> 00:19:54,320
you have working memory and
you have long term memory.

571
00:19:54,320 --> 00:19:57,780
And that's a super
simplification.

572
00:19:57,780 --> 00:20:00,930
And now to make a bad analogy on
top of the simplification--

573
00:20:00,930 --> 00:20:04,440
working memory is kind of like
a clipboard on the computer.

574
00:20:04,440 --> 00:20:05,920
You store things there
for a moment.

575
00:20:05,920 --> 00:20:07,470
And then you're done
with them.

576
00:20:07,470 --> 00:20:09,830
Whereas long term memory is a
little bit more like storage.

577
00:20:09,830 --> 00:20:11,870
Now there's a lot of ways in
that it's not like storage.

578
00:20:11,870 --> 00:20:15,120
In fact, when you access your
memories you also modify them.

579
00:20:15,120 --> 00:20:18,170
So read and write are tied
together, which is one of the

580
00:20:18,170 --> 00:20:21,240
reasons that memory is so
unreliable, unlike a computer,

581
00:20:21,240 --> 00:20:24,070
where you're able to read
without writing to the file.

582
00:20:24,070 --> 00:20:26,875
Also sleeping is kind of like
defragmenting your hard drive.

583
00:20:26,875 --> 00:20:28,940
But this is rough analogy.

584
00:20:28,940 --> 00:20:32,700
So what's important about
working memory is the user is

585
00:20:32,700 --> 00:20:34,440
going to be strong things their
working memory as they

586
00:20:34,440 --> 00:20:35,830
go throughout their day--

587
00:20:35,830 --> 00:20:37,510
various things that they're
working on, software they're

588
00:20:37,510 --> 00:20:39,110
writing, variables--

589
00:20:39,110 --> 00:20:40,340
any number of things.

590
00:20:40,340 --> 00:20:43,700
And then if they receive an
interruption what happens is

591
00:20:43,700 --> 00:20:45,800
the interruption captures
their attention and the

592
00:20:45,800 --> 00:20:47,840
working memory fades away.

593
00:20:47,840 --> 00:20:50,790
So you wouldn't write software
that was actively deleting the

594
00:20:50,790 --> 00:20:52,380
contents of the clipboard,
right?

595
00:20:52,380 --> 00:20:54,120
You obviously care
about data loss.

596
00:20:54,120 --> 00:20:56,150
But when you create interruptive
notifications,

597
00:20:56,150 --> 00:20:56,900
that's what you're doing.

598
00:20:56,900 --> 00:20:59,070
You're deleting the clipboard
of the user's minds.

599
00:20:59,070 --> 00:21:02,390
And this is why interruptions
can be so dangerous and cause

600
00:21:02,390 --> 00:21:04,610
people to be very ineffective
throughout their day, if

601
00:21:04,610 --> 00:21:06,510
they're constantly getting
email toasts and they're

602
00:21:06,510 --> 00:21:08,500
running around putting
out fires.

603
00:21:08,500 --> 00:21:09,890
You're doing a lot of thrashing

604
00:21:09,890 --> 00:21:10,700
on the user's thread.

605
00:21:10,700 --> 00:21:13,680
And they're losing a lot of
data, as the thing that they

606
00:21:13,680 --> 00:21:14,930
were storing was lost.

607
00:21:14,930 --> 00:21:17,530

608
00:21:17,530 --> 00:21:20,000
So another aspect related to
working memory is chunking.

609
00:21:20,000 --> 00:21:21,950
Basically what we're doing here
is because you're working

610
00:21:21,950 --> 00:21:24,200
memory so limited in size,
you're putting things on it

611
00:21:24,200 --> 00:21:25,745
and then taking them
off quickly.

612
00:21:25,745 --> 00:21:28,150
So I was trying to find an
example in Google products--

613
00:21:28,150 --> 00:21:29,880
a little bit hard because we
don't have a lot of serial

614
00:21:29,880 --> 00:21:30,600
numbers or anything.

615
00:21:30,600 --> 00:21:33,340
But for two factor
authentication, when you have

616
00:21:33,340 --> 00:21:36,400
a recovery password, we
chunk it, based off

617
00:21:36,400 --> 00:21:37,930
of four letter sequences.

618
00:21:37,930 --> 00:21:39,860
So it's easier for you
to then write it in.

619
00:21:39,860 --> 00:21:40,930
If we didn't have
these spaces, it

620
00:21:40,930 --> 00:21:41,740
would be very difficult.

621
00:21:41,740 --> 00:21:44,020
You'd be grabbing as much as you
could possibly store and

622
00:21:44,020 --> 00:21:46,340
then going back and trying to
find where you left off.

623
00:21:46,340 --> 00:21:48,780
More commonly you see this
of credit card numbers.

624
00:21:48,780 --> 00:21:51,540
If you ever create an
application that doesn't let

625
00:21:51,540 --> 00:21:53,980
the user enter spaces on a
credit card number, you're a

626
00:21:53,980 --> 00:21:56,190
very bad developer.

627
00:21:56,190 --> 00:21:59,165
Also, if you actually generate
an error because they entered

628
00:21:59,165 --> 00:22:01,310
spaces, you're just a
horrifically bad developer.

629
00:22:01,310 --> 00:22:03,690
So please, please,
don't do that.

630
00:22:03,690 --> 00:22:05,300
It doesn't matter how
smart the user is.

631
00:22:05,300 --> 00:22:06,410
They could be brilliant.

632
00:22:06,410 --> 00:22:09,010
They still have a limited
working memory, as we all do.

633
00:22:09,010 --> 00:22:13,490
And we rely on, in this case,
Gestalt Proximity with the

634
00:22:13,490 --> 00:22:16,010
white space, for being able to
grab the various chunks and

635
00:22:16,010 --> 00:22:17,650
move them over.

636
00:22:17,650 --> 00:22:21,590
In other aspects related to
memory is interruptions and

637
00:22:21,590 --> 00:22:23,560
the concept of flow.

638
00:22:23,560 --> 00:22:25,645
So as the user is holding
their working memory and

639
00:22:25,645 --> 00:22:28,500
they're doing these creative
tasks, if they're really

640
00:22:28,500 --> 00:22:30,660
focused they can get into this
particular psychological state

641
00:22:30,660 --> 00:22:32,390
called flow.

642
00:22:32,390 --> 00:22:33,390
It has a few attributes.

643
00:22:33,390 --> 00:22:34,860
Usually have a very clear
goal when you're

644
00:22:34,860 --> 00:22:36,130
doing creative work.

645
00:22:36,130 --> 00:22:37,250
You're very focused.

646
00:22:37,250 --> 00:22:39,200
You're receiving constant
feedback.

647
00:22:39,200 --> 00:22:41,240
And this also affects your
perception of time, when some

648
00:22:41,240 --> 00:22:43,450
people report that time almost
seems to stand still when

649
00:22:43,450 --> 00:22:45,280
they're in this state, whereas
other people report that time

650
00:22:45,280 --> 00:22:46,890
goes extremely quickly.

651
00:22:46,890 --> 00:22:49,200
So, for instance, when you're
playing a musical instrument,

652
00:22:49,200 --> 00:22:51,130
if you're very good instrument,
you're not

653
00:22:51,130 --> 00:22:53,100
actually thinking about the
sort of physicality of the

654
00:22:53,100 --> 00:22:53,900
instrument itself.

655
00:22:53,900 --> 00:22:55,540
You're just creating music.

656
00:22:55,540 --> 00:22:58,250
Or perhaps a more common
example-- if you're playing a

657
00:22:58,250 --> 00:23:01,080
video game, you don't think
about the controller.

658
00:23:01,080 --> 00:23:03,720
You get lost in the narrative
of the game and you start to

659
00:23:03,720 --> 00:23:05,770
have the world kind
of fade away.

660
00:23:05,770 --> 00:23:06,700
Same with reading a book.

661
00:23:06,700 --> 00:23:08,950
You to think about turning the
pages, just sort of lost in

662
00:23:08,950 --> 00:23:09,920
the narrative of the book.

663
00:23:09,920 --> 00:23:11,640
And, of course, same with
writing software, where you're

664
00:23:11,640 --> 00:23:14,310
not really thinking about all
the tools you're using.

665
00:23:14,310 --> 00:23:16,440
You're thinking about the
creative endeavor of writing

666
00:23:16,440 --> 00:23:17,550
the software.

667
00:23:17,550 --> 00:23:21,220
And what's important is when
you create interactive

668
00:23:21,220 --> 00:23:24,160
notification, you're not just
deleting the working memory,

669
00:23:24,160 --> 00:23:25,610
you're also pulling
them out of flow.

670
00:23:25,610 --> 00:23:27,750
And it can actually take quite
some time for them to get back

671
00:23:27,750 --> 00:23:30,580
into the psychological state.

672
00:23:30,580 --> 00:23:32,930
So another topic related to
attention is line length and

673
00:23:32,930 --> 00:23:33,980
reading speed.

674
00:23:33,980 --> 00:23:35,360
There've been a lot of
studies on this.

675
00:23:35,360 --> 00:23:37,800
So if you have very lines of
text, people can consume

676
00:23:37,800 --> 00:23:39,650
information faster.

677
00:23:39,650 --> 00:23:41,510
So for instance, Google News
here, as you're reading these

678
00:23:41,510 --> 00:23:43,940
headlines you can fly through
them because you don't have to

679
00:23:43,940 --> 00:23:45,570
do a break onto the next one.

680
00:23:45,570 --> 00:23:49,110
However, if you ask people what
they prefer, they say

681
00:23:49,110 --> 00:23:50,960
they prefer shorter lines.

682
00:23:50,960 --> 00:23:53,530
So it's this, are we force
feeding them information or

683
00:23:53,530 --> 00:23:55,990
not argument, where, yes,
they're are more efficient and

684
00:23:55,990 --> 00:23:57,440
optimal with long lines.

685
00:23:57,440 --> 00:23:59,060
But it's not as comfortable.

686
00:23:59,060 --> 00:24:02,420
So you see an application like
Currents on Android, which is

687
00:24:02,420 --> 00:24:03,740
a new news reading app.

688
00:24:03,740 --> 00:24:06,325
By having very short lines of
text, it becomes a more casual

689
00:24:06,325 --> 00:24:08,320
and browsable interface.

690
00:24:08,320 --> 00:24:12,920
And it's not as intense as
reading these very long ones.

691
00:24:12,920 --> 00:24:14,745
So moving over into emotion--

692
00:24:14,745 --> 00:24:17,050
one of the first aspects
of emotion is trust.

693
00:24:17,050 --> 00:24:22,980
And just as we're designed to
recognize tigers very quickly,

694
00:24:22,980 --> 00:24:26,510
we're also designed to make
trust decisions very quickly

695
00:24:26,510 --> 00:24:29,590
because we evolved in a state
where we had to decide, is

696
00:24:29,590 --> 00:24:30,750
this thing going to kill me?

697
00:24:30,750 --> 00:24:32,360
Can I eat this thing?

698
00:24:32,360 --> 00:24:33,990
These were very basic
decisions.

699
00:24:33,990 --> 00:24:36,080
We had to them very quickly.

700
00:24:36,080 --> 00:24:37,350
We don't have to make them
quickly anymore, but

701
00:24:37,350 --> 00:24:40,220
nonetheless these first
impressions start to stick,

702
00:24:40,220 --> 00:24:42,190
where you're choosing a bank.

703
00:24:42,190 --> 00:24:44,880
And you look at this bank and
you think, well they clearly

704
00:24:44,880 --> 00:24:47,060
been around for a while, perhaps
a few centuries, based

705
00:24:47,060 --> 00:24:49,450
off of that column.

706
00:24:49,450 --> 00:24:52,150
And if it gets really windy,
like my money's not going to

707
00:24:52,150 --> 00:24:54,610
blow over, because it's
very, very sturdy.

708
00:24:54,610 --> 00:24:55,730
So this is probably good bank.

709
00:24:55,730 --> 00:24:58,180
And maybe it's a horrendously
corrupt bank.

710
00:24:58,180 --> 00:25:01,230
But you form of these immediate
trust decisions.

711
00:25:01,230 --> 00:25:03,630
This comes into play in the
virtual world as well.

712
00:25:03,630 --> 00:25:05,290
Let's say you're writing
financial software.

713
00:25:05,290 --> 00:25:06,880
And the user has to decide
which of these two

714
00:25:06,880 --> 00:25:08,130
applications they trust.

715
00:25:08,130 --> 00:25:10,520

716
00:25:10,520 --> 00:25:12,870
And they don't know anything
else, like the developer could

717
00:25:12,870 --> 00:25:16,220
be brilliant and also indie
and not corrupt at all and

718
00:25:16,220 --> 00:25:18,770
writing really good stuff
with Money Pro

719
00:25:18,770 --> 00:25:21,150
using Comic Sans and--

720
00:25:21,150 --> 00:25:23,580
versus this one, where it's a
3D render and has little bit

721
00:25:23,580 --> 00:25:24,000
of texture.

722
00:25:24,000 --> 00:25:25,120
And it's just a great icon.

723
00:25:25,120 --> 00:25:27,710
And what's interesting is once
you formed that decision on

724
00:25:27,710 --> 00:25:30,510
which one you trust, then as
we give you additional

725
00:25:30,510 --> 00:25:34,030
information, you start to tell
yourself these made up stories

726
00:25:34,030 --> 00:25:36,580
to try to justify your
original assumption.

727
00:25:36,580 --> 00:25:39,740
So we say, well look Money Pro's
got four stars, whereas

728
00:25:39,740 --> 00:25:41,390
this one only one.

729
00:25:41,390 --> 00:25:43,850
You say, well yeah but maybe
this one didn't work on all

730
00:25:43,850 --> 00:25:44,940
the devices.

731
00:25:44,940 --> 00:25:46,200
Those people are just
mad because they

732
00:25:46,200 --> 00:25:47,600
couldn't have its greatness.

733
00:25:47,600 --> 00:25:48,940
Or maybe it's really
expensive.

734
00:25:48,940 --> 00:25:49,690
I haven't looked at
the price yet.

735
00:25:49,690 --> 00:25:50,500
And they're just mad.

736
00:25:50,500 --> 00:25:52,530
So you start to create these
theories, even though we've

737
00:25:52,530 --> 00:25:54,140
given you evidence that,
not, that one's better.

738
00:25:54,140 --> 00:25:56,220
You've already made your
initial trust decision.

739
00:25:56,220 --> 00:25:59,590
So the obvious implication
here is first impressions

740
00:25:59,590 --> 00:26:02,400
matter a lot, especially for
interfaces like an app store,

741
00:26:02,400 --> 00:26:05,070
where people are very quickly
choosing one amongst others.

742
00:26:05,070 --> 00:26:09,040
It's very worthwhile to invest
in your application icon and

743
00:26:09,040 --> 00:26:14,140
create that very first
impression, to be very solid.

744
00:26:14,140 --> 00:26:16,130
So text and culture--

745
00:26:16,130 --> 00:26:19,040
not innate but learned
over time.

746
00:26:19,040 --> 00:26:22,840
Obviously, we think about
threads and exceptions and all

747
00:26:22,840 --> 00:26:25,870
this terminology and jargon
of software development.

748
00:26:25,870 --> 00:26:28,380
It's important to think about
the environment the user's in.

749
00:26:28,380 --> 00:26:31,140
If you create a dialogue box
that says, this application

750
00:26:31,140 --> 00:26:32,960
has performed an illegal
operation.

751
00:26:32,960 --> 00:26:35,520
You don't actually know the
geopolitical environment that

752
00:26:35,520 --> 00:26:36,200
the user's in.

753
00:26:36,200 --> 00:26:37,910
This might actually scare
them quite a bit.

754
00:26:37,910 --> 00:26:39,760
So please don't do that.

755
00:26:39,760 --> 00:26:41,420
Another great one is,

756
00:26:41,420 --> 00:26:44,860
unfortunately, earth has crashed.

757
00:26:44,860 --> 00:26:47,030
I was trying to crash Google
Earth to get that one.

758
00:26:47,030 --> 00:26:48,860
And I couldn't crash it.

759
00:26:48,860 --> 00:26:51,890
But, it does exist,
that dialog.

760
00:26:51,890 --> 00:26:54,820
Jumping over into memory,
off of attention--

761
00:26:54,820 --> 00:26:55,850
talking a little bit
about learning.

762
00:26:55,850 --> 00:26:59,300
So again, we have working memory
and long term memory.

763
00:26:59,300 --> 00:27:02,360
And the task here is to transfer
something that's just

764
00:27:02,360 --> 00:27:05,440
in the very transitory,
ephemeral working memory into

765
00:27:05,440 --> 00:27:06,180
your long term memory.

766
00:27:06,180 --> 00:27:07,470
And once we've done that
transfer, you've

767
00:27:07,470 --> 00:27:08,670
learned this new thing.

768
00:27:08,670 --> 00:27:09,910
And there's two ways
to do that.

769
00:27:09,910 --> 00:27:11,450
The first is to associate
it with something

770
00:27:11,450 --> 00:27:12,560
they already know.

771
00:27:12,560 --> 00:27:15,540
If you can form that linkage,
it'll transfer very quickly.

772
00:27:15,540 --> 00:27:17,720
And then if they don't have that
linkage yet, the second

773
00:27:17,720 --> 00:27:19,710
way to do it is through
repetition.

774
00:27:19,710 --> 00:27:23,100
So let's look at an example
in interface design.

775
00:27:23,100 --> 00:27:25,280
One of the things the users
immediately have to get used

776
00:27:25,280 --> 00:27:27,350
to on an Android is that all
their apps are in a separate

777
00:27:27,350 --> 00:27:28,850
place, whereas the
home screen is a

778
00:27:28,850 --> 00:27:30,830
customizable home for them.

779
00:27:30,830 --> 00:27:32,810
So we need to tell them
your apps are here.

780
00:27:32,810 --> 00:27:35,440
If you to get to one of your
apps, hit this button.

781
00:27:35,440 --> 00:27:36,877
This was a screen that
we rolled out

782
00:27:36,877 --> 00:27:37,660
in Ice Cream Sandwich.

783
00:27:37,660 --> 00:27:38,700
It actually doesn't
work very well.

784
00:27:38,700 --> 00:27:40,990
And reason for that is they
realized that they're blocked,

785
00:27:40,990 --> 00:27:42,270
so they don't read anything
and they just

786
00:27:42,270 --> 00:27:44,570
hit OK really quickly.

787
00:27:44,570 --> 00:27:45,430
There's no repetition.

788
00:27:45,430 --> 00:27:46,250
We only told them once.

789
00:27:46,250 --> 00:27:48,610
It wasn't linked to anything
there ready know.

790
00:27:48,610 --> 00:27:49,780
So they start to play
with the phone.

791
00:27:49,780 --> 00:27:52,160
And then they say, well
where are my apps?

792
00:27:52,160 --> 00:27:54,310
Then they proceed to hit
everything until they

793
00:27:54,310 --> 00:27:55,760
finally find it.

794
00:27:55,760 --> 00:27:58,450
So an alternative design
could be--

795
00:27:58,450 --> 00:28:00,950
we can't really to something
they know, so just repetition.

796
00:28:00,950 --> 00:28:04,420
If we just had this refrain for
the first number of times

797
00:28:04,420 --> 00:28:07,430
saying, all your apps are here,
that would start to set

798
00:28:07,430 --> 00:28:09,400
in after just a couple
of times.

799
00:28:09,400 --> 00:28:10,810
Or after they'd hit the Control,
then we'd stop

800
00:28:10,810 --> 00:28:11,270
showing it.

801
00:28:11,270 --> 00:28:12,450
By doing this--

802
00:28:12,450 --> 00:28:16,740
it's not as loud or as broad
as that intrusive,

803
00:28:16,740 --> 00:28:19,455
interruptive flow that
we initially had.

804
00:28:19,455 --> 00:28:21,660
But this would actually
perform better.

805
00:28:21,660 --> 00:28:24,860
It performs better because
of the repetition.

806
00:28:24,860 --> 00:28:26,570
Now if you're linking to
something that the user are

807
00:28:26,570 --> 00:28:30,840
already knows, it's OK to just
have a single reference that

808
00:28:30,840 --> 00:28:32,100
they might ignore.

809
00:28:32,100 --> 00:28:33,920
So for instance, let's say
you've already use the

810
00:28:33,920 --> 00:28:35,110
commenting system a lot.

811
00:28:35,110 --> 00:28:38,350
And now we say, you can also
chat with document

812
00:28:38,350 --> 00:28:38,920
collaborators.

813
00:28:38,920 --> 00:28:41,230
It's an adaptation of
something you've

814
00:28:41,230 --> 00:28:42,270
already been using.

815
00:28:42,270 --> 00:28:45,230
Here, because it links to that
existing knowledge, this'll

816
00:28:45,230 --> 00:28:46,960
perform pretty well, even
though it's just

817
00:28:46,960 --> 00:28:50,320
that one time interface.

818
00:28:50,320 --> 00:28:55,140
So another aspect of learning is
recognition versus recall.

819
00:28:55,140 --> 00:28:58,740
Now the classic example is the
command line versus graphical

820
00:28:58,740 --> 00:29:01,290
interface, where the command
line's very difficult because

821
00:29:01,290 --> 00:29:03,600
you have to, in your own memory,
remember all the

822
00:29:03,600 --> 00:29:06,320
commands are and access
them over time.

823
00:29:06,320 --> 00:29:06,850
And--

824
00:29:06,850 --> 00:29:08,580
it's actually a little bit more
complicated than that.

825
00:29:08,580 --> 00:29:08,950
That's true.

826
00:29:08,950 --> 00:29:09,860
But there's more going on.

827
00:29:09,860 --> 00:29:11,720
And essentially what we're
doing is cognitive load

828
00:29:11,720 --> 00:29:14,500
balancing, where in order
of difficulty,

829
00:29:14,500 --> 00:29:16,490
recall is very difficult.

830
00:29:16,490 --> 00:29:18,950
Visual targeting is moderately
difficult.

831
00:29:18,950 --> 00:29:20,970
And then movement
is very easy.

832
00:29:20,970 --> 00:29:24,200
So if we look at a variety of
interfaces, a command line

833
00:29:24,200 --> 00:29:26,330
versus the graphical interface,
the graphical

834
00:29:26,330 --> 00:29:28,370
interface is going to perform
better because it's mostly the

835
00:29:28,370 --> 00:29:30,200
visual targeting and movement.

836
00:29:30,200 --> 00:29:32,020
I can remember that there
are alignment tools.

837
00:29:32,020 --> 00:29:33,720
I can't remember what
they all are.

838
00:29:33,720 --> 00:29:38,040
But I'm able to look there and
say, oh OK, a line left, and

839
00:29:38,040 --> 00:29:39,090
hit that button.

840
00:29:39,090 --> 00:29:41,500
So it's a interplay between me
and the interface, where I'm

841
00:29:41,500 --> 00:29:42,460
remembering some things.

842
00:29:42,460 --> 00:29:44,340
It's remembering other
things for me.

843
00:29:44,340 --> 00:29:46,860
And it's able to augment my
memory because it's being

844
00:29:46,860 --> 00:29:48,140
shown graphically.

845
00:29:48,140 --> 00:29:50,610
And then the visual targeting
is a little bit of work.

846
00:29:50,610 --> 00:29:54,000
Interestingly, the voice
interfaces are, at least in

847
00:29:54,000 --> 00:29:56,970
terms of cognitive load, more
similar to a command line.

848
00:29:56,970 --> 00:29:59,870
The pulsing mic is kind of the
new blinking cursor because

849
00:29:59,870 --> 00:30:01,620
you have to remember, like for
instance what all the Android

850
00:30:01,620 --> 00:30:03,640
Voice Actions are.

851
00:30:03,640 --> 00:30:04,500
And--

852
00:30:04,500 --> 00:30:06,896
can anyone name all Android
Voice Actions?

853
00:30:06,896 --> 00:30:07,750
Yeah.

854
00:30:07,750 --> 00:30:12,330
So that's still, in the recall,
very high cognitive

855
00:30:12,330 --> 00:30:14,240
load category.

856
00:30:14,240 --> 00:30:16,740
And then also interestingly is
we're seeing interfaces that

857
00:30:16,740 --> 00:30:18,690
are in some ways just
about movement.

858
00:30:18,690 --> 00:30:20,690
So like invoking Google Now,
once you've learned that

859
00:30:20,690 --> 00:30:24,340
behavior, you don't even have
to do any visual targeting.

860
00:30:24,340 --> 00:30:26,380
You're just swiping directly
up off the bottom of the

861
00:30:26,380 --> 00:30:27,150
screen to do it.

862
00:30:27,150 --> 00:30:29,260
So it's in many ways one of the
easiest interfaces because

863
00:30:29,260 --> 00:30:32,480
it's only on that third track,
which was the easiest.

864
00:30:32,480 --> 00:30:35,000
Also swiping through the
timeline on Glass-- you're not

865
00:30:35,000 --> 00:30:36,900
doing very much visual targeting
because it's such

866
00:30:36,900 --> 00:30:38,450
low information density.

867
00:30:38,450 --> 00:30:40,860
And you just have the single
cards in the timeline.

868
00:30:40,860 --> 00:30:45,360
And you're just doing the gross
motor skills of Swipe or

869
00:30:45,360 --> 00:30:46,560
Accelerometer looking.

870
00:30:46,560 --> 00:30:49,200
So those are very easy
interfaces, at least in terms

871
00:30:49,200 --> 00:30:51,150
of cognitive load.

872
00:30:51,150 --> 00:30:53,640
Another topic I want to talk
about with learning is the

873
00:30:53,640 --> 00:30:55,710
notion of learning
with examples.

874
00:30:55,710 --> 00:30:58,360
Now, often people say, well,
we need to create this

875
00:30:58,360 --> 00:31:00,270
interface in this particular
way because it's similar to

876
00:31:00,270 --> 00:31:01,340
this other product
that exists.

877
00:31:01,340 --> 00:31:04,460
And users aren't going
to use it otherwise.

878
00:31:04,460 --> 00:31:06,660
That's not entirely true.

879
00:31:06,660 --> 00:31:08,130
You can show the user
an example of

880
00:31:08,130 --> 00:31:09,330
how something works.

881
00:31:09,330 --> 00:31:10,870
And now it's consistent.

882
00:31:10,870 --> 00:31:14,210
Their futures experiences are
consistent with that example.

883
00:31:14,210 --> 00:31:15,470
So they've effectively
learned it.

884
00:31:15,470 --> 00:31:17,090
People are pretty smart.

885
00:31:17,090 --> 00:31:19,380
So you're able to actually
create innovative software

886
00:31:19,380 --> 00:31:21,980
that changes things from the
existing marketplace and

887
00:31:21,980 --> 00:31:22,780
playing field.

888
00:31:22,780 --> 00:31:25,220
You just have to bring people
up to speed on why you made

889
00:31:25,220 --> 00:31:26,280
this changes.

890
00:31:26,280 --> 00:31:28,320
So a couple examples of this--

891
00:31:28,320 --> 00:31:31,300
in 2006, the notion of
the end of files.

892
00:31:31,300 --> 00:31:33,080
Obviously, people are very
used to the file system.

893
00:31:33,080 --> 00:31:35,380
They're used to attaching
things, sending them around.

894
00:31:35,380 --> 00:31:36,350
That's kind of broken.

895
00:31:36,350 --> 00:31:39,170
So how do you get this huge
marketplace that is very sort

896
00:31:39,170 --> 00:31:40,820
of set in the consistency
of how files

897
00:31:40,820 --> 00:31:42,490
operate into a new model?

898
00:31:42,490 --> 00:31:44,810
And the answers is you just
lay out a very quick

899
00:31:44,810 --> 00:31:47,320
argument for it.

900
00:31:47,320 --> 00:31:48,540
MALE SPEAKER: Meet Sam.

901
00:31:48,540 --> 00:31:50,850
Sam is the editor of a
neighborhood newsletter called

902
00:31:50,850 --> 00:31:52,020
The Oak Tree View.

903
00:31:52,020 --> 00:31:54,390
She works with local writers who
like to publish articles

904
00:31:54,390 --> 00:31:55,240
in the newsletter.

905
00:31:55,240 --> 00:31:58,350
Sam loves her job but often
feels frustrated when time is

906
00:31:58,350 --> 00:32:00,290
wasted managing all
the 'articles.

907
00:32:00,290 --> 00:32:01,690
It's a familiar problem.

908
00:32:01,690 --> 00:32:04,190
Each month, writers send her
draft articles as email

909
00:32:04,190 --> 00:32:05,040
attachments.

910
00:32:05,040 --> 00:32:07,510
She reviews them and sends
them back with comments.

911
00:32:07,510 --> 00:32:10,050
One article might create six
different versions of the same

912
00:32:10,050 --> 00:32:12,510
file, not to mention the
countless emails.

913
00:32:12,510 --> 00:32:15,230
Sam often feels buried by all
the email attachments.

914
00:32:15,230 --> 00:32:17,794
She finds it hard to track all
the versions being sent to her

915
00:32:17,794 --> 00:32:18,650
from the writers.

916
00:32:18,650 --> 00:32:21,260
As the deadline looms,
frustration rises.

917
00:32:21,260 --> 00:32:22,860
Something has to give.

918
00:32:22,860 --> 00:32:25,860
Sam decides to try something
new, Google Docs.

919
00:32:25,860 --> 00:32:27,110
Here's what happens.

920
00:32:27,110 --> 00:32:30,110
First, she visits the Google
Docs page and creates a free

921
00:32:30,110 --> 00:32:31,110
Google account.

922
00:32:31,110 --> 00:32:32,140
She logs in.

923
00:32:32,140 --> 00:32:34,570
And because some articles were
already written, she uploads

924
00:32:34,570 --> 00:32:36,600
the current drafts right
from her computer.

925
00:32:36,600 --> 00:32:38,930
With a snap, Google
Docs turns offline

926
00:32:38,930 --> 00:32:41,190
articles into online versions.

927
00:32:41,190 --> 00:32:43,310
Now all she needs to do is
invite the writers to

928
00:32:43,310 --> 00:32:45,650
collaborate on the documents.

929
00:32:45,650 --> 00:32:47,710
ALEX FAABORG: So there, you lay
out a very simple problem.

930
00:32:47,710 --> 00:32:49,060
You show the solution.

931
00:32:49,060 --> 00:32:49,870
Sam is sad.

932
00:32:49,870 --> 00:32:51,190
Sam has too many files.

933
00:32:51,190 --> 00:32:53,260
And people are able to very
quickly then adapt to that new

934
00:32:53,260 --> 00:32:55,130
model because they
have been able to

935
00:32:55,130 --> 00:32:56,570
learn from that example.

936
00:32:56,570 --> 00:32:58,720
It's not the case that you
have to be consistent.

937
00:32:58,720 --> 00:33:01,920
Looking at another example--

938
00:33:01,920 --> 00:33:03,120
2012, 2013--

939
00:33:03,120 --> 00:33:04,830
The notion of predictive
assistance.

940
00:33:04,830 --> 00:33:06,890
When we were working on Google
Now, we were worried about--

941
00:33:06,890 --> 00:33:08,800
it's going to be really hard
to explain this to people

942
00:33:08,800 --> 00:33:10,130
because people are so
used to search,

943
00:33:10,130 --> 00:33:11,020
especially with Google.

944
00:33:11,020 --> 00:33:12,650
People are extremely used to
search because Google such a

945
00:33:12,650 --> 00:33:13,800
search company.

946
00:33:13,800 --> 00:33:16,020
How do people wrap their head
around the notion of a product

947
00:33:16,020 --> 00:33:17,740
that's going to give them the
information that they need

948
00:33:17,740 --> 00:33:18,500
right when they need it?

949
00:33:18,500 --> 00:33:21,320
And the answer is you show them
a couple of very simple

950
00:33:21,320 --> 00:33:23,600
examples to lay out the
nature of the product.

951
00:33:23,600 --> 00:33:25,740
And from just those examples
they can start to extrapolate

952
00:33:25,740 --> 00:33:27,650
new use cases in
their own mind.

953
00:33:27,650 --> 00:33:29,770
So here's a quick spot we
created for Google Now.

954
00:33:29,770 --> 00:33:47,798
[MUSIC PLAYING]

955
00:33:47,798 --> 00:33:48,784
FEMALE SPEAKER 1: Bonjour.

956
00:33:48,784 --> 00:33:50,263
FEMALE SPEAKER 2: A fruit or
a vegetable, do you think?

957
00:33:50,263 --> 00:33:50,756
FEMALE SPEAKER 1: I don't know.

958
00:33:50,756 --> 00:33:53,714
I can ask.

959
00:33:53,714 --> 00:33:55,686
[SPEAKING IN FRENCH]

960
00:33:55,686 --> 00:33:58,644
MALE SPEAKER 2: It's a fish.

961
00:33:58,644 --> 00:33:59,670
FEMALE SPEAKER 2: Oh.

962
00:33:59,670 --> 00:34:02,530
ALEX FAABORG: So three
very short examples--

963
00:34:02,530 --> 00:34:03,560
and I'm only showing
a clip of it.

964
00:34:03,560 --> 00:34:04,890
But from that, you can
understand the

965
00:34:04,890 --> 00:34:05,620
nature of the product.

966
00:34:05,620 --> 00:34:08,610
And we're finding when we told
people, so this thing is going

967
00:34:08,610 --> 00:34:09,969
to predict the information
you need.

968
00:34:09,969 --> 00:34:11,960
They would look back
and say, OK what?

969
00:34:11,960 --> 00:34:13,070
That makes no sense.

970
00:34:13,070 --> 00:34:16,260
But if we said, you're standing
on a train platform.

971
00:34:16,260 --> 00:34:17,790
You want to know when the next
train's going to come.

972
00:34:17,790 --> 00:34:19,679
So you go in it, and the
schedule's already there.

973
00:34:19,679 --> 00:34:21,199
They say, oh that's
pretty cool.

974
00:34:21,199 --> 00:34:24,050
From the example they're able
to then grapple onto

975
00:34:24,050 --> 00:34:24,560
[INAUDIBLE]

976
00:34:24,560 --> 00:34:26,179
and then imagine
other examples.

977
00:34:26,179 --> 00:34:29,280
That was act one of laying
out this notion of

978
00:34:29,280 --> 00:34:30,350
this augmented reality.

979
00:34:30,350 --> 00:34:32,320
Of course, act two is then
saying to people.

980
00:34:32,320 --> 00:34:33,558
And it's going to augment
your vision.

981
00:34:33,558 --> 00:34:35,510
[MUSIC PLAYING]

982
00:34:35,510 --> 00:34:36,974
MALE SPEAKER 3: After this
bridge, first exit.

983
00:34:36,974 --> 00:34:38,224
MALE SPEAKER 4: Woo hoo.

984
00:34:38,224 --> 00:34:41,366

985
00:34:41,366 --> 00:34:41,854
MALE SPEAKER 5: Hurry up.

986
00:34:41,854 --> 00:34:43,820
A12, right there.

987
00:34:43,820 --> 00:34:45,540
ALEX FAABORG: So this product
is great for when you're

988
00:34:45,540 --> 00:34:50,300
running down escalators is
the consistent notion.

989
00:34:50,300 --> 00:34:52,190
Because we're showing people
being so busy that's also

990
00:34:52,190 --> 00:34:54,400
important for the
predictiveness, where they

991
00:34:54,400 --> 00:34:55,989
don't even have time
to quickly look up

992
00:34:55,989 --> 00:34:56,690
their flight details.

993
00:34:56,690 --> 00:34:57,655
They need to get the gate.

994
00:34:57,655 --> 00:34:58,820
And it's right there.

995
00:34:58,820 --> 00:35:02,665
So what's important here is
that consistency is--

996
00:35:02,665 --> 00:35:03,215
[MIC FEEDBACK]

997
00:35:03,215 --> 00:35:03,893
Sorry.

998
00:35:03,893 --> 00:35:08,530
It's making noise against
my phone.

999
00:35:08,530 --> 00:35:10,690
Consistency is not critical.

1000
00:35:10,690 --> 00:35:12,780
It really bothers me that
that's one of usability

1001
00:35:12,780 --> 00:35:14,590
heuristics that people
focus on.

1002
00:35:14,590 --> 00:35:16,700
You do not need to build
products that are identical to

1003
00:35:16,700 --> 00:35:18,190
what exists in the
marketplace.

1004
00:35:18,190 --> 00:35:21,120
What you can do is you can have
innovation and teaching.

1005
00:35:21,120 --> 00:35:23,630
And people are smart, like you
don't have to design for the

1006
00:35:23,630 --> 00:35:25,020
first-run interaction.

1007
00:35:25,020 --> 00:35:26,930
You can bring people up
to speed over time.

1008
00:35:26,930 --> 00:35:28,220
And they're going to understand
your product.

1009
00:35:28,220 --> 00:35:30,210
And you can create interesting
products that aren't just

1010
00:35:30,210 --> 00:35:31,850
leveraging what's already in
the marketplace for the

1011
00:35:31,850 --> 00:35:33,750
purpose of usability.

1012
00:35:33,750 --> 00:35:35,730
So the final topic I want
to focus on is the

1013
00:35:35,730 --> 00:35:37,950
perception of time.

1014
00:35:37,950 --> 00:35:40,590
So interesting example
here-- just looking

1015
00:35:40,590 --> 00:35:43,201
at a progress bar.

1016
00:35:43,201 --> 00:35:45,755
In this case, it's a pretty
simple progress bar.

1017
00:35:45,755 --> 00:35:48,570

1018
00:35:48,570 --> 00:35:50,090
We look at another one.

1019
00:35:50,090 --> 00:35:51,080
This one it slows down.

1020
00:35:51,080 --> 00:35:54,940
You're like, come on, come
on-- and then finishes.

1021
00:35:54,940 --> 00:35:59,280
Versus the final one, where
it starts out OK.

1022
00:35:59,280 --> 00:36:01,200
And then it speeds up
towards the end.

1023
00:36:01,200 --> 00:36:04,320
And of course, all these
progress bars are taking the

1024
00:36:04,320 --> 00:36:05,395
same amount of time
to complete.

1025
00:36:05,395 --> 00:36:07,710
But what's interesting is we're
actually really bad at

1026
00:36:07,710 --> 00:36:09,090
gauging how long
something took.

1027
00:36:09,090 --> 00:36:11,740
We don't really have an
internal chronometer.

1028
00:36:11,740 --> 00:36:14,490
What we're doing is we're
perceiving the process of time

1029
00:36:14,490 --> 00:36:15,870
based off of change.

1030
00:36:15,870 --> 00:36:18,170
So something that speeds up
towards the end, is actually

1031
00:36:18,170 --> 00:36:20,240
going to appear faster than
something that slows down

1032
00:36:20,240 --> 00:36:21,780
towards the end, even if
the overall amount

1033
00:36:21,780 --> 00:36:22,910
of time is the same.

1034
00:36:22,910 --> 00:36:24,550
And they're been a lot
of studies on this.

1035
00:36:24,550 --> 00:36:27,600
In fact other products, have
had slow progress bars.

1036
00:36:27,600 --> 00:36:30,755
And everyone complained that
file copying got slower, even

1037
00:36:30,755 --> 00:36:33,350
though it was actually faster,
which is crazy.

1038
00:36:33,350 --> 00:36:35,480
So it's important to know
these types of effects.

1039
00:36:35,480 --> 00:36:38,910
They're can actually be a 10%
variance in how long something

1040
00:36:38,910 --> 00:36:40,480
people think something's
going to take.

1041
00:36:40,480 --> 00:36:42,730
Another example of this is if
you're working on something

1042
00:36:42,730 --> 00:36:45,730
like the Web Layout engine,
where obviously there's a lot

1043
00:36:45,730 --> 00:36:47,630
of people working very hard
at making that quickly.

1044
00:36:47,630 --> 00:36:50,420
So your immediate assumption
could be, well we'll just do a

1045
00:36:50,420 --> 00:36:52,270
layout as things come in.

1046
00:36:52,270 --> 00:36:54,770
As soon as we have it, we show
to the screen, quick feedback,

1047
00:36:54,770 --> 00:36:55,990
it's all going to be great.

1048
00:36:55,990 --> 00:36:59,570
What's interesting is because we
we're so bad gauging time.

1049
00:36:59,570 --> 00:37:03,280
And we only gauge time based off
of the individual changes

1050
00:37:03,280 --> 00:37:05,560
and how many changes
there were.

1051
00:37:05,560 --> 00:37:07,890
That will actually appear to
take longer than if you were

1052
00:37:07,890 --> 00:37:10,520
just pause and then bring
everything in

1053
00:37:10,520 --> 00:37:11,880
once you have it.

1054
00:37:11,880 --> 00:37:15,205
Because we mentally chunk that
white space of before.

1055
00:37:15,205 --> 00:37:17,620
And we don't really know
how long before was.

1056
00:37:17,620 --> 00:37:19,550
We just know that, wow I
have my whole page now.

1057
00:37:19,550 --> 00:37:21,580
And it came in very quickly.

1058
00:37:21,580 --> 00:37:23,730
So there are all these aspects
of how we're perceiving time

1059
00:37:23,730 --> 00:37:25,250
that come to play with the
performance of your

1060
00:37:25,250 --> 00:37:27,920
applications and how you
can optimize for that.

1061
00:37:27,920 --> 00:37:31,010
And our perception of time it's
kind of like an optical

1062
00:37:31,010 --> 00:37:33,630
illusion, where we're not
perceiving reality.

1063
00:37:33,630 --> 00:37:36,440
We're filtering everything
through our own hardware.

1064
00:37:36,440 --> 00:37:38,940
And if you learn about our
hardware, and if you learn

1065
00:37:38,940 --> 00:37:41,430
about the biological
computational machines that we

1066
00:37:41,430 --> 00:37:44,110
are and that your software
actually runs on, you can

1067
00:37:44,110 --> 00:37:45,190
write better software.

1068
00:37:45,190 --> 00:37:47,020
And you'll be a better
engineer.

1069
00:37:47,020 --> 00:37:49,610
So some quick recommended
reading, if you want to check

1070
00:37:49,610 --> 00:37:50,190
out some books--

1071
00:37:50,190 --> 00:37:53,050
Don Norman, famous cognitive
psychologist.

1072
00:37:53,050 --> 00:37:55,810
Susan Weinschenk has a very
book came out recently.

1073
00:37:55,810 --> 00:37:57,210
Also of course, all the
writing from the

1074
00:37:57,210 --> 00:37:58,460
injury design team.

1075
00:37:58,460 --> 00:38:00,970
And before you read those books,
you can spend your

1076
00:38:00,970 --> 00:38:02,650
entire day learning
about design.

1077
00:38:02,650 --> 00:38:04,570
This was the first session.

1078
00:38:04,570 --> 00:38:06,520
In the same room, we'll
have three more

1079
00:38:06,520 --> 00:38:08,180
sessions about design.

1080
00:38:08,180 --> 00:38:09,700
And then if you head
over to Room 12.

1081
00:38:09,700 --> 00:38:12,450
There's Android design
for UI developers.

1082
00:38:12,450 --> 00:38:14,450
So now we can take some
quick questions.

1083
00:38:14,450 --> 00:38:15,700
And thank you.

1084
00:38:15,700 --> 00:38:18,528

1085
00:38:18,528 --> 00:38:23,786
[APPLAUSE]

1086
00:38:23,786 --> 00:38:26,280
And I think very quick questions
because we have

1087
00:38:26,280 --> 00:38:28,140
about a minute and a half--

1088
00:38:28,140 --> 00:38:29,122
one question.

1089
00:38:29,122 --> 00:38:30,086
[VOICES FROM THE AUDIENCE]

1090
00:38:30,086 --> 00:38:31,050
Ah yeah.

1091
00:38:31,050 --> 00:38:32,500
Sure.

1092
00:38:32,500 --> 00:38:33,700
Thank you.

1093
00:38:33,700 --> 00:38:34,160
All right.

1094
00:38:34,160 --> 00:38:35,410
Thank you for coming.

1095
00:38:35,410 --> 00:38:35,935

